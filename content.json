{"meta":{"title":"呀嘿","subtitle":"让生命重一些","description":"","author":"Edison.Chen","url":"https://YaHeii.github.io","root":"/"},"pages":[{"title":"留言板","date":"2024-02-22T14:47:24.000Z","updated":"2024-02-22T14:48:09.800Z","comments":true,"path":"comment/index.html","permalink":"https://yaheii.github.io/comment/index.html","excerpt":"","text":""}],"posts":[{"title":"回调函数","slug":"回调函数","date":"2025-07-30T16:00:00.000Z","updated":"2025-08-02T06:13:49.145Z","comments":true,"path":"2025/07/31/回调函数/","permalink":"https://yaheii.github.io/2025/07/31/%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/","excerpt":"","text":"回调本身有多种形式，下面一一介绍 封装式&#x2F;延迟回调12345678910111213141516171819202122class Task&#123; public: typedef void (*TaskCallback)(void*);//为 ‘一个指向“返回值为void，参数为void*”的函数’ 的指针类型，创建一个新的类型别名，名为 TaskCallback。” Task():mTaskCallback(NULL),mArg(NULL) &#123;&#125;;//当你创建一个 Task 对象时，它内部的函数指针 mTaskCallback 和参数指针 mArg 都被初始化为 NULL。 void setTaskCallback(TaskCallback cb, void* arg) &#123; mTaskCallback = cb; mArg = arg; &#125; void handle() &#123; if(mTaskCallback) mTaskCallback(mArg); &#125; bool operator=(const Task&amp; task) &#123; this-&gt;mTaskCallback = task.mTaskCallback; this-&gt;mArg = task.mArg; &#125; private: TaskCallback mTaskCallback; void* mArg;&#125;; 在上面的回调函数定义中，首先定义别名，具体见上，其中void*是一个通用指针，可以指向任何类型数据。为了充分说明，我们定义一个处理函数。 12345678910111213141516171819// 假设有一个连接信息的结构体struct ConnectionInfo &#123; int socket_fd; std::string client_ip;&#125;;// 定义一个符合 TaskCallback 签名的函数void handle_connection(void* arg) &#123; // 将 void* 转换回原始类型 ConnectionInfo* info = static_cast&lt;ConnectionInfo*&gt;(arg); printf(&quot;Handling connection from IP: %s on socket %d\\n&quot;, info-&gt;client_ip.c_str(), info-&gt;socket_fd); // ... 在这里执行具体的读写操作 ... // 清理资源 delete info;&#125; 接下来通过set方法装载，myTask.setTaskCallback(handle_connection, connectionData);,执行完这步后，myTask 对象内部的 mTaskCallback 指向了 handle_connection 函数，mArg 指向了 connectionData 对象的内存地址。 需要的时候，系统的其他部分（比如一个线程池或者事件循环）只需要调用 Task 对象的 handle() 方法，就可以执行被封装的任务，而无需关心任务具体是什么。 这种模式非常适用于：1.线程池&#x2F;任务队列：主线程创建一堆 Task 对象（通过 set 配置好），然后把这些对象扔进一个队列。工作线程不断从队列中取出 Task 对象，并调用它们的 handle() 方法。 2.事件驱动编程：当某个事件发生时，事件循环调用对应事件处理程序（一个 Task 对象）的 handle() 方法。 3.实现解耦：调用 handle() 的代码完全不知道它到底在执行哪个具体函数，它只知道如何执行一个 Task。 普通回调这种方式就是网上非常常见的，这里不做赘述，贴上gpt代码 123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;// 这是一个回调函数，告诉qsort如何比较两个整数int compare_integers(const void* a, const void* b) &#123; int val1 = *(int*)a; int val2 = *(int*)b; return (val1 - val2);&#125;int main() &#123; int numbers[] = &#123;5, 2, 8, 1, 9&#125;; int n = sizeof(numbers) / sizeof(numbers[0]); // 调用qsort，直接把回调函数 compare_integers 作为参数传进去 qsort(numbers, n, sizeof(int), compare_integers); for(int i = 0; i &lt; n; i++) &#123; printf(&quot;%d &quot;, numbers[i]); &#125; printf(&quot;\\n&quot;); return 0;&#125; 现代C++中的lamda与function std::function: 一个通用的、多态的函数封装器。它可以存储、复制和调用任何“可调用对象”（普通函数、Lambda表达式、成员函数、函数对象等）。它完美替代了 void (TaskCallback)(void)。 Lambda表达式: 一种在源代码中定义匿名函数的方式。它可以方便地“捕获”上下文中的变量，从而完全避免使用不安全的 void*。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;functional&gt; // 需要包含这个头文件#include &lt;string&gt;class ModernTask &#123;public: // 使用 std::function，它可以持有任何无参数、无返回值的可调用对象 // 这比C风格的函数指针更强大、更安全 using TaskCallback = std::function&lt;void()&gt;; ModernTask() = default; // 使用默认构造函数 // set函数现在接受一个 std::function 对象 void setTask(TaskCallback cb) &#123; mTaskCallback = cb; &#125; // handle函数保持不变 void handle() &#123; if (mTaskCallback) &#123; // 检查是否有效 mTaskCallback(); // 直接调用 &#125; &#125;private: TaskCallback mTaskCallback;&#125;;// 使用示例struct ConnectionInfo &#123; int socket_fd; std::string client_ip;&#125;;int main() &#123; ModernTask myTask; ConnectionInfo connectionData = &#123;10, &quot;192.168.1.101&quot;&#125;; // 使用Lambda表达式来创建回调 // [=] 表示按值捕获所有外部变量 (这里是connectionData) myTask.setTask([=]() &#123; // 在Lambda内部，可以直接访问 connectionData，无需 void* 转换 std::cout &lt;&lt; &quot;Handling connection from IP: &quot; &lt;&lt; connectionData.client_ip &lt;&lt; &quot; on socket &quot; &lt;&lt; connectionData.socket_fd &lt;&lt; std::endl; &#125;); myTask.handle(); return 0;&#125;","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"回调函数","slug":"回调函数","permalink":"https://yaheii.github.io/tags/%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/"}],"author":null},{"title":"高性能RTSP服务器","slug":"高性能RTSP服务器","date":"2025-07-24T16:00:00.000Z","updated":"2025-08-02T06:13:19.803Z","comments":true,"path":"2025/07/25/高性能RTSP服务器/","permalink":"https://yaheii.github.io/2025/07/25/%E9%AB%98%E6%80%A7%E8%83%BDRTSP%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"基本RTSP服务器对于一个基本的RTSP服务器而言，只需要定义好封装的包装格式，建立socket链接，定义RTSP询问、交互格式（SDP）即可。在socket链接部分，有两种方法，一种是基于TCP一种是基于UDP，在阻塞情况下，只需要在服务器端做轮询操作即可。实例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859WSADATA wsaData; //启动socket /* Param1：MAKEWORD 请求的winsock版本 Param2：指向WSADATA的指针 */ if (WSAStartup(MAKEWORD(2, 2), &amp;wsaData) != 0) &#123; printf(&quot;PC Server Socket Start Up Error \\n&quot;); return -1; &#125; int serverSockfd; /* 创建socket，这里由于我做的是RTSP服务器，所以做了封装， 但基本函数就是 SOCKET sock = socket(AF_INET, SOCK_STREAM, 0); */ serverSockfd = createTcpSocket(); if (serverSockfd &lt; 0) &#123; WSACleanup(); printf(&quot;failed to create tcp socket\\n&quot;); return -1; &#125; //bind if (bindSocketAddr(serverSockfd, &quot;0.0.0.0&quot;, SERVER_PORT) &lt; 0) &#123; printf(&quot;failed to bind addr\\n&quot;); return -1; &#125; //listen if (listen(serverSockfd, 10) &lt; 0) &#123; printf(&quot;failed to listen\\n&quot;); return -1; &#125; printf(&quot;%s rtsp://127.0.0.1:%d\\n&quot;,__FILE__, SERVER_PORT); //循环接收 while (true) &#123; int clientSockfd; char clientIp[40]; int clientPort; clientSockfd = acceptClient(serverSockfd, clientIp, &amp;clientPort); if (clientSockfd &lt; 0) &#123; printf(&quot;failed to accept client\\n&quot;); return -1; &#125; printf(&quot;accept client;client ip:%s,client port:%d\\n&quot;, clientIp, clientPort); doClient(clientSockfd, clientIp, clientPort); &#125; //注意释放顺序 closesocket(serverSockfd); WSACleanup(); return 0; 在上面的基本RTSP服务器例子中，核心是doclient，也就是如何处理接收到的RTSP命令。核心是拆分行，并与协议做比对这里不再赘述。 高性能RTSP服务器何所谓“高性能” 在读取文件方面就需要定义一个线程池 实现快速重启，结束后快速释放端口 定义IO时间，创建只读的回调函数 阻塞很少，绝大部分都是回调函数 可以支持epoll poller select 设置了定时器，所有的调度都是基于定时器实现的。对于不同的线程，定时器都是独立的，需要设置一个定时器的定时器，保证同步，在WIN系统中，定时器回调由子线程托管，非win系统通过select网络模型。 main函数流程123456789101112131415161718192021222324252627282930313233343536373839404142int main() &#123; srand(time(NULL));//时间初始化 /* createNew-&gt;EventScheduler 进行socket初始化 开启select网络模型 创建时间管理器，并开启下面调用 TimerManager-&gt;TimerManager::readCallback-&gt;TimerManager::handleRead 在handle内部对定时器是否超时以及超时后是否停止，做出处理。使用timer.handleEvent()来判断是否停止 Timer::handleEvent()-&gt; TimerEvent::handleEvent()-&gt; TimerEvent::mTimeoutCallback(mArg); 在其中的 */ EventScheduler* scheduler = EventScheduler::createNew(EventScheduler::POLLER_SELECT);// 创建事件调度器，使用select作为IO多路复用模型 ThreadPool* threadPool = ThreadPool::createNew(1);// MediaSessionManager* sessMgr = MediaSessionManager::createNew();// 创建会话管理器 UsageEnvironment* env = UsageEnvironment::createNew(scheduler, threadPool);//将线程池和时间调度器打包成环境 Ipv4Address rtspAddr(&quot;127.0.0.1&quot;, 8554); RtspServer* rtspServer = RtspServer::createNew(env, sessMgr, rtspAddr);//环境、会话管理器、rtsp地址创建，rtsp服务器 LOGI(&quot;----------session init start------&quot;); &#123; MediaSession* session = MediaSession::createNew(&quot;test&quot;); MediaSource* source = H264FileMediaSource::createNew(env, &quot;D:/code/C++/BXC_RtspServer_study-master/RTSP_server/data/video4.h264&quot;); Sink* sink = H264FileSink::createNew(env, source); session-&gt;addSink(MediaSession::TrackId0, sink); source = AACFileMeidaSource::createNew(env, &quot;D:/code/C++/BXC_RtspServer_study-master/RTSP_server/data/video5.aac&quot;); sink = AACFileSink::createNew(env, source); session-&gt;addSink(MediaSession::TrackId1, sink); sessMgr-&gt;addSession(session); &#125; LOGI(&quot;----------session init end------&quot;); rtspServer-&gt;start(); env-&gt;scheduler()-&gt;loop(); return 0;","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"RTSP服务器","slug":"RTSP服务器","permalink":"https://yaheii.github.io/tags/RTSP%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"高性能","slug":"高性能","permalink":"https://yaheii.github.io/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"}],"author":null},{"title":"socket编程","slug":"socket编程","date":"2025-07-22T16:00:00.000Z","updated":"2025-07-25T14:31:02.574Z","comments":true,"path":"2025/07/23/socket编程/","permalink":"https://yaheii.github.io/2025/07/23/socket%E7%BC%96%E7%A8%8B/","excerpt":"","text":"以下部分内容参考文章Socket本质上就是对TCP&#x2F;IP协议组的封装。是应用层与传输层通信的中间层。从设计模式角度而言，Socket实际上是一组接口，。用户通过这些接口去组织数据，以符合指定的协议。Socket有三种类型， SOCK_STREAM 面向稳定通信，底层是TCP SOCK_DGRAM 无连接的通信，底层是UDP，需要上层协议来保证可靠性。 SOCK_RAW 更加灵活的数据控制，可以指定IP头部。 常用Socket编程接口 socket()：创建socket bind()：绑定socket到本地地址和端口，通常由服务端调用 listen()：TCP专用，开启监听模式 accept()：TCP专用，服务器等待客户端连接，一般是阻塞态 connect()：TCP专用，客户端主动连接服务器 send()：TCP专用，发送数据 recv()：TCP专用，接收数据 sendto()：UDP专用，发送数据到指定的IP地址和端口 recvfrom()：UDP专用，接收数据，返回数据远端的IP地址和端口 closesocket()：关闭socket Socket通信流程TCP流程 UDP流程需要注意，TCP和UDP的端口互不干扰，所以说系统可以同时开启TCP80端口和UDP80端口 网络编程模型 同步阻塞迭代模型123456789bind(srvfd);listen(srvfd);for(;;)&#123; clifd = accept(srvfd,...); //开始接受客户端来的连接 read(clifd,buf,...); //从客户端读取数据 dosomthingonbuf(buf); write(clifd,buf)//发送数据到客户端&#125; 上述程序弊端： 如果没有客户端的连接请求，进程会阻塞在accept系统调用处，程序不能执行其他任何操作。(系统调用使得程序从用户态陷入内核态) 在与客户端建立好一条链路后，通过read系统调用从客户端接受数据，而客户端发送数据过来是不可控的。如果客户端迟迟不发生数据过来，则程序同样会阻塞在read调用，此时，如果另外的客户端来尝试连接时，都会失败。 同样的道理，write系统调用也会使得程序出现阻塞(例如：客户端接受数据异常缓慢，导致写缓冲区满，数据迟迟发送不出)。 多进程并发模型多进程并发模型在同步阻塞迭代模型的基础上进行了一些改进，以避免是程序阻塞在read系统调用上。核心代码如下： 123456789101112131415161718192021222324bind(srvfd);listen(srvfd);for(;;)&#123;clifd = accept(srvfd,...); //开始接受客户端来的连接ret = fork();switch( ret )&#123; case -1 : do_err_handler(); break; case 0: // 子进程 client_handler(clifd); break ; default : // 父进程 close(clifd); continue ;&#125;&#125;void client_handler(clifd)&#123; read(clifd,buf,...); //从客户端读取数据 dosomthingonbuf(buf); write(clifd,buf)//发送数据到客户端&#125; 多线程并发模型在多进程并发模型中，每一个客户端连接开启fork一个进程，若客户端连接较大，则系统依然将不堪负重。通过多线程(或线程池)并发模型，可以在一定程度上改善这一问题。 在服务端的线程模型实现方式一般有三种： 按需生成(来一个连接生成一个线程) 线程池(预先生成很多线程) Leader follower（LF）以第一种为例，其核心代码如下：12345678910111213141516171819void *thread_callback( void *args ) //线程回调函数&#123; int clifd = *(int *)args ; client_handler(clifd);&#125;void client_handler(clifd)&#123; read(clifd,buf,...); //从客户端读取数据 dosomthingonbuf(buf); write(clifd,buf)//发送数据到客户端&#125;bind(srvfd);listen(srvfd);for(;;)&#123; clifd = accept(); pthread_create(...,thread_callback,&amp;clifd);&#125; 在这个模型中，服务端分为了两个线程，一是负责业务逻辑和流的读取。二是accept链接。 IO多路复用多进程模型和多线程(线程池)模型每个进程&#x2F;线程只能处理一路IO，在服务器并发数较高的情况下，过多的进程&#x2F;线程会使得服务器性能下降。而通过多路IO复用，能使得一个进程同时处理多路IO，提升服务器吞吐量。这是一种进程预先告知内核的能力，让内核发现进程指定的一个或多个IO条件就绪了，就通知进程。使得一个进程能在一连串的事件上等待。IO复用的实现方式目前主要有select、poll和epoll。select和poll的原理基本相同： 注册待侦听的fd(这里的fd创建时最好使用非阻塞) 每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回 返回结果中包括已就绪和未就绪的fd 相比select，poll解决了单个进程能够打开的文件描述符数量有限制这个问题：select受限于FD_SIZE的限制，如果修改则需要修改这个宏重新编译内核；而poll通过一个pollfd数组向内核传递需要关注的事件，避开了文件描述符数量限制。此外，select和poll共同具有的一个很大的缺点就是包含大量fd的数组被整体复制于用户态和内核态地址空间之间，开销会随着fd数量增多而线性增大。epoll的出现，解决了select、poll的缺点： 基于事件驱动的方式，避免了每次都要把所有fd都扫描一遍。 epoll_wait只返回就绪的fd。 epoll使用nmap内存映射技术避免了内存复制的开销。 epoll的fd数量上限是操作系统的最大文件句柄数目,这个数目一般和内存有关，通常远大于1024。 select：支持注册 FD_SETSIZE(1024) 个 socket。 poll： poll 作为 select 的替代者，最大的区别就是，poll 不再限制 socket 数量。 epoll：epoll 能直接返回具体的准备好的通道，时间复杂度 O(1)。 C在win平台创建socket的实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859WSADATA wsaData; //启动socket /* Param1：MAKEWORD 请求的winsock版本 Param2：指向WSADATA的指针 */ if (WSAStartup(MAKEWORD(2, 2), &amp;wsaData) != 0) &#123; printf(&quot;PC Server Socket Start Up Error \\n&quot;); return -1; &#125; int serverSockfd; /* 创建socket，这里由于我做的是RTSP服务器，所以做了封装， 但基本函数就是 SOCKET sock = socket(AF_INET, SOCK_STREAM, 0); */ serverSockfd = createTcpSocket(); if (serverSockfd &lt; 0) &#123; WSACleanup(); printf(&quot;failed to create tcp socket\\n&quot;); return -1; &#125; //bind if (bindSocketAddr(serverSockfd, &quot;0.0.0.0&quot;, SERVER_PORT) &lt; 0) &#123; printf(&quot;failed to bind addr\\n&quot;); return -1; &#125; //listen if (listen(serverSockfd, 10) &lt; 0) &#123; printf(&quot;failed to listen\\n&quot;); return -1; &#125; printf(&quot;%s rtsp://127.0.0.1:%d\\n&quot;,__FILE__, SERVER_PORT); //循环接收 while (true) &#123; int clientSockfd; char clientIp[40]; int clientPort; clientSockfd = acceptClient(serverSockfd, clientIp, &amp;clientPort); if (clientSockfd &lt; 0) &#123; printf(&quot;failed to accept client\\n&quot;); return -1; &#125; printf(&quot;accept client;client ip:%s,client port:%d\\n&quot;, clientIp, clientPort); doClient(clientSockfd, clientIp, clientPort); &#125; //注意释放顺序 closesocket(serverSockfd); WSACleanup(); return 0;","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"网络编程，Socket","slug":"网络编程，Socket","permalink":"https://yaheii.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%EF%BC%8CSocket/"}],"author":null},{"title":"音频重采样","slug":"音频重采样","date":"2025-07-21T16:00:00.000Z","updated":"2025-08-02T06:14:32.415Z","comments":true,"path":"2025/07/22/音频重采样/","permalink":"https://yaheii.github.io/2025/07/22/%E9%9F%B3%E9%A2%91%E9%87%8D%E9%87%87%E6%A0%B7/","excerpt":"","text":"采样格式定义：采样格式指的是音频样本数据的数字表示方式。常见的格式包括脉冲编码调制（PCM）格式，它们可以是浮点数（如32位浮点）或整数（如16位或24位）。类型： PCM S16LE&#x2F;S16BE：16位有符号整数，小端&#x2F;大端。这是CD音质的标准格式，提供足够的动态范围，文件大小适中。 PCM S24LE&#x2F;S24BE：24位有符号整数，小端&#x2F;大端。提供更宽的动态范围，常用于专业音频录制。 PCM F32LE&#x2F;F32BE：32位浮点数，小端&#x2F;大端。允许极其宽的动态范围，但文件大小也较大。 选择考量：选择合适的采样格式通常要平衡音质需求、存储空间限制和系统兼容性。 多声道系统常见的多声道配置有 5.1 系统：包括前左、前右、中央、低频效果（LFE）、后左和后右六个声道。 7.1 系统：在 5.1 的基础上增加了两个侧面声道（侧左和侧右），以实现更全面的环绕声效果。 其他配置：如 9.1 系统等，根据不同的应用需求，可能会有更多的声道配置。 在这个部分引入音频帧的计算，方便我们下面进行pts、dts、cts分析 音频帧的数据量 &#x3D; 采样率 * 采样位宽 * 声道数 * 时间 播放时间 &#x3D; 总采样数&#x2F;采样率总采样数 = 音频帧数据量/声道数 各种官方文档都对这一部分有规定，总采样数（samples）指的是一个声道的采样，帧（frame）指的是一个时间点的样本集合，包（packet）指的是多个frame的集合 FFmpeg对于音频重采样的处理首先是配置上下文swr_alloc_set_opts；接下来对于swr_init初始化音频采样格式等基础信息；由swr_convert执行实际的重采样工作；最后由swr_free释放资源。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"SLAM","slug":"technology/SLAM","permalink":"https://yaheii.github.io/categories/technology/SLAM/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://yaheii.github.io/tags/SLAM/"},{"name":"概率","slug":"概率","permalink":"https://yaheii.github.io/tags/%E6%A6%82%E7%8E%87/"}],"author":null},{"title":"流媒体基础","slug":"直播结构","date":"2025-07-17T16:00:00.000Z","updated":"2025-08-03T13:24:42.012Z","comments":true,"path":"2025/07/18/直播结构/","permalink":"https://yaheii.github.io/2025/07/18/%E7%9B%B4%E6%92%AD%E7%BB%93%E6%9E%84/","excerpt":"","text":"最近在研究直播系统的架构，在学习的过程中发现直播系统的结构很复杂，一方面是音视频传输过程中的协议不兼容，例如RTSP在现代播放器的不兼容，需要转码才能进行播放。另一方面则是附属功能的复杂，例如礼物，公屏互动等等。还有一方面则是多个拉流会导致拥塞情况，CDN的分发需要考虑高并发。所以准备一边做这个项目，一边从架构角度梳理一下直播系统的搭建过程中应该考虑的问题。 协议解析RTSP协议栈RTSP协议栈通常与live555 共同出现，在这个协议栈中包括RTP传输，RTCP质量评估，以及RTSP交互。已经有很多文章提到了，这里不再赘述 RTMP 连接：类似握手过程（三次）。 C0+C1： 客户端向服务器发送 C0（1字节，表明 RTMP 版本）和 C1（1536字节，包含时间戳等随机数据）两个数据块。 S0+S1+S2： 服务器收到 C0&#x2F;C1 后，立刻回复 S0（版本号）、S1（服务器版的时间戳和随机数据）和 S2（将客户端发来的 C1 数据原样返回）三个数据块。 C2： 客户端收到 S1 后，回复一个 C2 块（将服务器发来的 S1 数据原样返回）。 数据传输：TCP connect： 客户端向服务器发起connect命令，请求连接到服务器上的一个指定应用（Application）。例如rtmp:&#x2F;&#x2F;server.com&#x2F;live这里的live就是应用名。这个消息里会包含一些参数，如 Flash 版本、编码信息等。 connect response： 服务器验证通过后，会返回一个成功响应，告知客户端可以继续。 createStream： 客户端发送createStream命令，请求服务器创建一个逻辑上的“流通道”，服务器会返回一个唯一的 Stream ID。后续的音视频数据都会在这个 ID 上传输。 publish： 客户端发送publish命令，并携带一个“流名称”（也叫 “stream key” 或 “secret”），告知服务器：“我准备好要往这个流通道上推送数据了，流的名字是 room123。 开始推流: 服务器验证publish命令成功后，客户端就可以正式开始发送音视频数据了。 在传输过程中，RTMP使用了基于chunk的流式结构，为了在一条 TCP 连接上同时传输音频、视频和控制信令而不互相阻塞，RTMP 会把大的数据（如一整个视频帧）拆分成小的“块”（Chunk）。每个 Chunk 都有一个自己的小头部，包含了这个块属于哪个流 （Stream ID）、消息类型、长度等信息。默认的 Chunk 大小是 128 字节。每个完整的音视频帧或元数据，在被拆分成 Chunk 之前，会被打包成一个带有“标签 （Tag）”的数据单元。这个 Tag 的结构（包括 Tag 类型是音频&#x2F;视频&#x2F;脚本数据、时间戳等）和 FLV 文件中的 Tag 结构是一模一样的。 RTMP是过去Flash时代常用的标准，但是由于与现在的浏览器不兼容，所以流的制作方面使用会变少。但是在内容推送方面还是用的最广。延迟相较于HLS低，但是没有WebRTC低。目前点播平台的主流方案就是RTMP推流，HLS负责分发。在这个过程中核心是媒体服务器，诸如SRS\\Nginx-RTMP\\Wpwza。它的职责就是接收来自推流端的唯一一路 RTMP 流，然后将其实时转封装 （transmux） 成包含多个码率的 HLS 切片和清单文件，供海量观众通过 CDN 拉取播放。 另外分发端方案还有Flv。由于flv和RTMP流的数据载体在结构上几乎是一致的，这种设计使得媒体服务器可以极其高效地在 RTMP 和 FLV 之间进行转换。由于没有 HLS 那样的切片和缓冲机制，HTTP-FLV 的延迟可以稳定在 2-5 秒，非常接近 RTMP。 由于它运行在标准的 HTTP 协议之上，可以轻松穿透防火墙，并且不需要像 RTMP 那样使用特殊的 1935 端口。借助flv.js这个强大的库，可以在所有支持 Media Source Extensions （MSE） 的现代浏览器中直接播放 HTTP-FLV 直播，无需 Flash 插件。而HLS兼容CDN，CDN对于HTTP文件下载的支持很完善，因此HLS的兼容性比HTTP-FLV 要好很多；同样HTTP-FLV的延迟比HLS要低很多，基本上可以做到3的5秒左右延迟，而HLS的延迟一般是8到10秒以上。 HLS &amp; MPEG-DASH 信令：HTTP 数据传输：基于TCP的HTTP 媒体容器：MPEG-2 TS（用于 HLS）、ISO BMFF （MP4）（用于 DASH 和更新的 HLS）。 有很多优点，但是流的延迟很大，在网上看到的资料大约都是10s左右。这个协议是将流切片，通过HTTP服务器提供发送服务，分片下载。也是比较适合点播，由于依靠标准的web服务器，和CDN能够轻松的扩展几百万的观众，TCP又可以确保可靠交付。还具备自适应比特率，防火墙遍历。 webrtc基于Webrtc的开发一方面延迟极低，另一方面网络拥塞等控制也会很好。它并不是一个单一的协议，而包括了一系列的API和协议，是一个综合性的框架。 SRT 数据传输：UDP，但是在数据包级别实现了自己的ARQ机制（自动重复请求），保证了可靠性 目前准备替代RTMP的协议，基本上只要在网上搜索RTMP实现直播，那么一定会有人说RTMP的不兼容和服务器端转协议问题。而与之相对应的就是SRT的流行。 SRT使用的封装是TS封装，因此对于新的Codec天然就支持。而SRT基于UDP协议，因此对于延迟和弱网传输，也比RTMP要好不少。 一般RTMP延迟在1到3秒以上，而SRT的延迟在300到500毫秒，而且在弱网下表现也很稳定。在广播电视l领域，由于长距离跨国跨地区 传输，或者户外广播时，网络不稳定，因此SRT比RTMP的优势会更明显。 SRT是SRS的核心协议，SRS早在2020年即支持了SRT协议，并且在2022年实现了SRT协程化，从而大幅提高了SRT和其他核心协议的一致性。 比如回调和API的支持，SRT和RTMP保持了非常高的一致性。 架构分析推流端推流端可以使用OBS，推送到对应的端口。采用的协议可以是RTSP、RTMP、SRT。 服务器用来接收OBS的推流，可以使用Nginx来做架构。如果拉流端不兼容协议。服务器还需要对流本身进行转换。 拉流端如果选用浏览器，内嵌了播放器，需要向媒体服务器请求流并进行播放。采用的协议有HLS,DASH,HTTP-FLV,WebRTC 实现路线SRS (Simple Realtime Server): 支持RTMP, HLS, HTTP-FLV, WebRTC, SRT等多种协议。 Nginx + nginx-rtmp-module: 可以将接收到的RTMP流轻松转换为HLS或作为HTTP-FLV分发。但模块已多年未更新，对DASH、WebRTC等新协议支持不足。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"RTSP","slug":"RTSP","permalink":"https://yaheii.github.io/tags/RTSP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://yaheii.github.io/tags/Nginx/"},{"name":"HLS","slug":"HLS","permalink":"https://yaheii.github.io/tags/HLS/"},{"name":"RTMP","slug":"RTMP","permalink":"https://yaheii.github.io/tags/RTMP/"}],"author":null},{"title":"音视频控制流分析","slug":"音视频控制流分析","date":"2025-07-12T16:00:00.000Z","updated":"2025-07-31T01:47:39.943Z","comments":true,"path":"2025/07/13/音视频控制流分析/","permalink":"https://yaheii.github.io/2025/07/13/%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8E%A7%E5%88%B6%E6%B5%81%E5%88%86%E6%9E%90/","excerpt":"","text":"最近在做一个本地播放器，这个播放器是从ffplay.c改编而来的，对源代码进行重构，并添加QT界面。其中的核心文件是videoctl，这篇文章目的在于细致的分析这个文件，进而理解播放器的工作流程。至于播放器的外围部分这里不做详细讨论，仅在第一节中进行阐述 undefined 播放器线程分配。 播放器组成 播放器前端是基于QT的，核心媒体处理分三个部分，一个是数据包的格式规定，一个是音视频的渲染，另一个则是音视频的控制。用户界面做了解耦处理，将信号和槽做多级触发。项目中的核心结构体是CurVideoState，在对这个结构体进行操作时要加线程锁，保证线程安全，例如执行全屏操作时，一方面是QT计算屏幕大小，一方面是SDL实现视频的缩放，刷新等等。这其中就多次涉及CurVideoState状态的改变，每次改变时都要注意为结构体加入线程锁。 注意关于界面的一些操作设置，例如单击鼠标，双击标题栏，拖动界面。通过注册表来实现软件启动时的默认设置，例如初始文件等。 全屏播放应该有两种方式，包括整体填充屏幕，和show组件填充屏幕 下面是ffmpeg解码的流程,结构体分析参考另一篇文章simplest——ffmpeg_player源码阅读 核心媒体处理 datactl.h : 媒体数据控制器，负责处理媒体数据流 ffplay_renderer.h&#x2F;c: 基于FFmpeg的渲染器，处理音视频解码和渲染 videoctl.h&#x2F;cpp: 视频控制器，管理视频播放、暂停等核心功能 用户界面组件 mainwindow.h&#x2F;cpp: 主窗口，应用程序的入口和主界面 ctlbar.h&#x2F;cpp: 控制栏，提供播放、暂停、音量等控制按钮 title.h&#x2F;cpp: 标题栏组件 show.h&#x2F;cpp: 显示区域，负责视频内容的显示 playlist.h&#x2F;cpp: 播放列表管理 medialist.h&#x2F;cpp**: 媒体列表组件 customslider.h&#x2F;cpp: 自定义滑块控件，可能用于进度条或音量控制 核心结构体 VideoState其中包括，指向解复用器的指针，线程句柄，音视频、外部时钟。强制刷新变量。视频状态变量等等多个变量的存储， 时钟类 MyAVpacketList其中存放packet的链表结构，并且每次拖动进度条会更新其中的serial字段。 PacketQueue定义Packet 队列自身属性，包括serial字段，包数量字段，队列所有元素数据大小字段等等 packet_qeue_start其中有packet_queue_put_private(q,&amp;flush_pkt);这里传入了一个flush_pkt是为了触发PacketQueue中对应的serial。使其+1.并触发解码器清空自身的缓存，重新开始解码 FrameQueue帧队列，其中包含多个接口；frame_peek_writable获取一个可写的frame，可以以阻塞或者非阻塞方式进行。 线程列表 媒体处理分析以下是videoctl.h分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314#ifndef VIDEOCTL_H#define VIDEOCTL_H#include &lt;QObject&gt;#include &lt;QThread&gt;#include &lt;QString&gt;#include &quot;datactl.h&quot;#include &quot;globalhelper.h&quot;class VideoCtl : public QObject&#123; Q_OBJECTpublic: /* 需要注意的是，这里创建了一个单例对象，具体做法就是将构造函数，析构函数，拷贝构造方法和赋值构造函数都放入Private中，禁止外部构造对象 在类里会设计一个获取实例的静态函数，可以全局访问 单例设计模式分为懒汉式和饿汉式，具体见文章,https://juejin.cn/post/6844903928497176584 */ static VideoCtl *GetInstance(); ~VideoCtl(); static int read_thread_wrapper(void *arg);//读取线程包装器 static int audio_thread_wrapper(void *arg);//音频线程包装器 static int video_thread_wrapper(void *arg);//视频线程包装器 static int subtitle_thread_wrapper(void *arg);//字幕线程包装器 /* 首先确保其他播放线程结束，避免资源冲突，使用m_playLoopIndex字段。接下里发送信号给标题栏完成播放的初始化 接下来通过对预置选项的判断，完成标志位初始化，完成SDL初始化。并配置硬件加速，以及渲染。 创建LoopThread线程 */ void StartPlay(QString strFileName, WId widPlayWid);//开始播放 void StopPlay();//停止播放 /* 创建SDL事件， 判断双击事件，显示全屏，通过检测鼠标单击事件的时间差实现。 鼠标隐藏事件 */ void LoopThread(VideoState* CurStream);//循环线程 /* 这个函数是设备初始化的核心，负责配置和打开SDL音频设备。完成音频格式协商，设备参数设置，错误恢复等复杂流程 创建音频结构体，其中包括期望的音频参数，和实际的音频参数。 定义备选声道数和采样率数组，用于降级策略。 成功打开设备后，会返回spec结构体，其中包括 spec.freq: 实际的采样率（可能与请求不同） spec.channels: 实际的声道数 spec.format: 实际的音频格式 spec.samples: 音频缓冲区大小 spec.size: 音频缓冲区的总字节数 */ int audio_open(void *opaque, AVChannelLayout *wanted_channel_layout, int wanted_sample_rate, struct AudioParams *audio_hw_params);//音频打开 /* double pts; // 当前媒体的时间戳 double pts_drift; // 与系统时钟的漂移（pts - gettime） double last_updated; // 最后更新时间（系统时间） double speed; // 播放速率（支持变速） int serial; // 用于检测流是否连续的序列号 int paused; // 暂停状态 int *queue_serial; // 关联队列版本号（跨线程同步） av_gettime_relative()获取自程序启动的相对时间，单位us `set_clock_at`: 设置时钟的基础函数，直接设置时钟的时间戳、序列号和更新时间点 - `pts`: 当前媒体的时间戳(秒) - `serial`: 用于检测流是否连续的序列号 - `time`: 系统当前时间(秒) - `pts_drift`: 计算媒体时间与系统时间的偏移量 `set_clock`: 简化版的时钟设置函数，自动获取当前系统时间并调用`set_clock_at` `get_clock`: 获取时钟当前时间，考虑了暂停状态和播放速度 - 如果序列号不匹配(可能发生了seek操作)，返回NaN - 如果暂停，直接返回保存的pts - 如果正常播放，返回考虑了时间漂移和播放速度的实际时间点 `sync_clock_to_slave`: 将一个时钟同步到另一个时钟(主从同步) - 当两个时钟差距超过阈值(AV_NOSYNC_THRESHOLD)时进行同步 - 通常用于将视频时钟或外部时钟同步到音频时钟 这些函数在音视频同步中的应用： - 音频、视频和外部时钟各有一个Clock实例(audclk, vidclk, extclk) - 通过`get_master_sync_type`决定主时钟(通常是音频时钟) - 其他时钟会通过`sync_clock_to_slave`与主时钟同步 - 视频帧显示时会根据与主时钟的差异决定是加速、减速还是丢帧这种设计确保了即使在网络延迟、解码速度不一致等情况下，音视频仍能保持同步播放。 */ void set_clock_at(Clock *c, double pts, int serial, double time);//设置时钟 void sync_clock_to_slave(Clock *c, Clock *slave);//同步时钟 double get_clock(Clock *c);//获取时钟 void set_clock(Clock *c, double pts, int serial);//设置时钟 /* 维护一个sample_array，实现音频的定长缓存，缓存区为环形 */ void update_sample_display(VideoState *is, short *samples, int samples_size); /* 帧获取与序列检查：从音频帧队列获取下一帧，确保序列号匹配，避免处理过时的帧 音频同步：调用synchronize_audio计算需要的样本数，用于与主时钟同步 动态重采样： 检测音频格式变化（采样率、声道布局、样本格式） 动态创建和配置重采样器 处理音频格式转换，确保输出符合音频设备要求 时钟更新：根据帧的PTS和样本数更新音频时钟，这是音视频同步的关键 内存管理：动态分配和管理音频缓冲区，处理各种边缘情况 */ int audio_decode_frame(VideoState *is);//音频解码 /* 通过动态调整音频样本数量来实现音频与主时钟的同步。 检查是否需要同步（音频不是主时钟时才需要） 计算音频时钟与主时钟的差异 如果差异在合理范围内，累积并计算平均差异 当积累足够样本且平均差异超过阈值时，调整音频样本数 返回调整后的样本数，供重采样器使用 */ int synchronize_audio(VideoState *is, int nb_samples);//同步音频 double get_master_clock(VideoState *is);//获取主时钟 int get_master_sync_type(VideoState *is);//获取主同步类型 /* 创建线程锁 创建AVPacket 创建格式上下文 打开媒体文件并解析格式 获取流信息 seek机制 选择最佳流 处理附加信息 错误处理 */ int read_thread(void *arg);//读取线程 /* 开启解码线程 */ int decoder_start(Decoder *d, int (*fn)(void *), const char *thread_name, void *arg);//解码器启动 /* 用户界面调用OnPlaySeek(double dPercent)函数，该函数内部会计算实际时间戳并调用stream_seek()，并且重启线程 */ void stream_seek(VideoState *is, int64_t pos, int64_t rel, int by_bytes); void step_to_next_frame(VideoState *is);//步进到下一个帧，逐帧前进功能 void stream_toggle_pause(VideoState *is);//处理暂停到播放的状态转换 int stream_has_enough_packets(AVStream *st, int stream_id, PacketQueue *queue);//流是否有足够的包，在读取线程中控制读取数据的速率 /* 解码器初始化， */ int stream_component_open(VideoState *is, int stream_index); /* 进行音频解码，decoder_decode_frame()函数 首先进行初始化，并进入解码循环 解码一帧 检查音频配置是否发生改变 音频过滤处理，实现混音，重采样等操作 帧处理和入队 检查包队列序列号和解码器序列号，判断是否发送seek操作 */ int audio_thread(void *arg); inline int cmp_audio_fmts(enum AVSampleFormat fmt1, int64_t channel_count1, enum AVSampleFormat fmt2, int64_t channel_count2);//比较音频格式。用于配置编码器 int configure_audio_filters(VideoState *is, const char *afilters, int force_output_format);//配置音频过滤器 /* 初始化， 获取视频帧，不仅使用decoder_decode_frame，同时通过检测序列号和时间戳来进行丢帧，进行同步 检测视频格式变化 进行过滤处理 时间戳处理，需要计算过滤器的延时 队列管理 同步控制 */ int video_thread(void *arg);//视频线程 /* 丢帧策略 在两种情况下考虑丢帧： framedrop &gt; 0: 强制丢帧模式 framedrop &amp;&amp; get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER: 开启丢帧且视频不是主时钟 通过判断fabs(diff)决定是否丢弃。 */ int get_video_frame(VideoState *is, AVFrame *frame);//获取视频帧 int queue_picture(VideoState *is, AVFrame *src_frame, double pts, double duration, int64_t pos, int serial);//队列图片 void set_default_window_size(int width, int height, AVRational sar);//设置默认窗口大小 void calculate_display_rect(SDL_Rect *rect, int scr_xleft, int scr_ytop, int scr_width, int scr_height, int pic_width, int pic_height, AVRational pic_sar);//计算显示矩形 /* 将视频帧转换成YUV格式数据 处理视频自动旋转 添加自定义滤镜：宏INSERT_FILT */ int configure_video_filters(AVFilterGraph *graph, VideoState *is, const char *vfilters, AVFrame *frame);//配置视频过滤器 int configure_filtergraph(AVFilterGraph *graph, const char *filtergraph, AVFilterContext *source_ctx, AVFilterContext *sink_ctx);//配置过滤器图 /* 字幕也是当作一个frame帧来进行处理的。 同样利用PTS来进行同步， */ int subtitle_thread(void *arg);//字幕线程 int filter_codec_opts(const AVDictionary *opts, enum AVCodecID codec_id, AVFormatContext *s, AVStream *st, const AVCodec *codec, AVDictionary **dst);//过滤器编码选项 int check_stream_specifier(AVFormatContext *s, AVStream *st, const char *spec);//检查流规范 int create_hwaccel(AVBufferRef **device_ctx);//创建硬件加速 void toggle_pause(VideoState *is);//切换暂停 void do_exit(VideoState *is);//退出 void stream_cycle_channel(int codec_type);//流循环通道 void toggle_audio_display();//切换音频显示 VideoState *m_CurVideoState = nullptr;//当前视频状态 bool m_playLoopIndex;//播放循环索引private: explicit VideoCtl(QObject *parent = nullptr);//构造函数 bool Init();//初始化 bool ConnectSignalSlots();//连接信号与槽 void UpdateVolume(int sign, double step);//更新音量 void refresh_loop_wait_event(VideoState* is, SDL_Event* event);//刷新循环等待事件 VideoState* stream_open(const char *filename, const AVInputFormat *iformat);//流打开 void stream_close(VideoState *is);//流关闭 void stream_component_close(int stream_index);//流组件关闭 int video_open(); void seek_chapter(int incr);//章节跳转 void toggle_full_screen();//切换全屏 void toggle_mute();//切换静音 void init_clock(Clock *c, int *queue_serial);//初始化时钟 // double get_master_clock(); // int get_master_sync_type(); void check_external_clock_speed();//检查外部时钟速度 void set_clock_speed(Clock *c, double speed);//设置时钟速度 void update_volume(int sign, double step);//更新音量 void update_video_pts(double pts, int serial);//更新视频时间戳 double compute_target_delay(double delay);//计算目标延迟 double vp_duration(Frame *vp, Frame *nextvp);//计算视频持续时间 void video_refresh(double *remaining_time);//视频刷新 void video_display();//视频显示 void video_audio_display();//视频音频显示 void video_image_display();//视频图像显示 int upload_texture(SDL_Texture **tex, AVFrame *frame);//上传纹理 int realloc_texture(SDL_Texture **texture, Uint32 new_format, int new_width, int new_height, SDL_BlendMode blendmode, int init_texture);//重新分配纹理 void get_sdl_pix_fmt_and_blendmode(int format, Uint32 *sdl_pix_fmt, SDL_BlendMode *sdl_blendmode);//获取SDL像素格式和混合模式 void set_sdl_yuv_conversion_mode(AVFrame *frame);//设置SDL YUV转换模式 inline void fill_rectangle(int x, int y, int w, int h);//填充矩形 inline int compute_mod(int a, int b);//计算模数 static VideoCtl* m_pInstance;//单例实例 bool m_initIndex;//初始化索引 SDL_Window *window;//窗口 SDL_Renderer *renderer;//渲染器 SDL_RendererInfo renderer_info = &#123; 0 &#125;;//渲染器信息 SDL_AudioDeviceID audio_dev;//音频设备 WId play_wid; // 播放窗口 int screen_width;//屏幕宽度 int screen_height;//屏幕高度 int startup_volume;//启动音量 //播放刷新循环线程 std::thread m_tPlayLoopThread;//播放刷新循环线程 int m_frameW;//帧宽度 int m_frameH;//帧高度signals: void SigPlayMsg(QString strMsg); //&lt; 错误信息 void SigFrameDimensionsChanged(int nFrameWidth, int nFrameHeight); //&lt;视频宽高发生变化 void SigVideoTotalSeconds(int nSeconds);//视频总秒数 void SigVideoPlaySeconds(int nSeconds);//视频播放秒数 void SigVideoVolume(double dPercent);//视频音量 void SigPauseStat(bool bPaused);//暂停状态 void SigStop();//停止 void SigStopAndNext();//停止并下一个 void SigStopFinished(); // 停止播放完成 void SigStartPlay(QString strFileName);public slots: void OnPlaySeek(double dPercent);//播放进度 void OnPlayVolume(double dPercent);//播放音量 void OnPause();//暂停 void OnStop();//停止 void OnSeekForward();//向前搜索 void OnSeekBack();//向后搜索 void OnAddVolume();//增加音量 void OnSubVolume();//减少音量&#125;;#endif","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"ffmpeg结构体分析","slug":"ffmpeg结构体分析","permalink":"https://yaheii.github.io/tags/ffmpeg%E7%BB%93%E6%9E%84%E4%BD%93%E5%88%86%E6%9E%90/"},{"name":"播放器工作流","slug":"播放器工作流","permalink":"https://yaheii.github.io/tags/%E6%92%AD%E6%94%BE%E5%99%A8%E5%B7%A5%E4%BD%9C%E6%B5%81/"}],"author":null},{"title":"大型C/C++项目的编译噩梦","slug":"大型CC++项目的编译噩梦","date":"2025-07-07T16:00:00.000Z","updated":"2025-07-13T03:23:54.288Z","comments":true,"path":"2025/07/08/大型CC++项目的编译噩梦/","permalink":"https://yaheii.github.io/2025/07/08/%E5%A4%A7%E5%9E%8BCC++%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A9%E6%A2%A6/","excerpt":"","text":"对于大型的C\\C++项目编译而言，毫无疑问是一个非常复杂，非常令人作呕的过程。C++不同于其他语言，对于编译过程中的ABI兼容都做出了处理。同时，在这门语言中还有非常复杂，近乎于另外一门语言的Cmake，以及两个版本的编译器。你还需要考虑编译过程中的库版本，编译器版本，甚至最后的最后还需要手动将.dll文件复制。下面希望在逐层的抽丝剥茧中可以帮助你我更深刻的理解这个问题，从而解决这个问题。 编译过程实际上对于一个程序员来说，编译过程应该已经非常熟悉了，但是在这里还需要进一步的阐述以下。 预处理预处理器（cpp）将所有的#define删除，并且展开所有的宏定义。 处理所有的条件预编译指令，比如#if、#ifdef、#elif、#else、#endif等。 处理#include预编译指令，将被包含的文件直接插入到预编译指令的位置。 删除所有的注释。 添加行号和文件标识，以便编译时产生调试用的行号及编译错误警告行号。 保留所有的#pragma编译器指令，因为编译器需要使用它们。 使用gcc -E hello.c -o hello.i命令来进行预处理， 预处理得到的另一个程序通常是以.i作为文件扩展名。 编译词法分析：扫描器（Scanner）将源代的字符序列分割成一系列的记号（Token）。lex工具可实现词法扫描。 语法分析：语法分析器将记号（Token）产生语法树（Syntax Tree）。yacc工具可实现语法分析(yacc: Yet Another Compiler Compiler)。 语义分析：静态语义（在编译器可以确定的语义）、动态语义（只能在运行期才能确定的语义）。 源代码优化：源代码优化器(Source Code Optimizer)，将整个语法书转化为中间代码（Intermediate Code）（中间代码是与目标机器和运行环境无关的）。中间代码使得编译器被分为前端和后端。编译器前端负责产生机器无关的中间代码；编译器后端将中间代码转化为目标机器代码。 目标代码生成：代码生成器(Code Generator). 目标代码优化：目标代码优化器(Target Code Optimizer)。 汇编汇编器（as）将hello.s翻译成机器语言指令，把这些指令打包成一种叫做可重定位目标程序的格式，并将结果保存在目标文件中，这是一个二进制文件 链接连接器的主要作用是把各个模块之间相互引用的部分处理好,使得各个模块之间能够正确的衔接。同时这里存在着动态链接和静态链接。静态链接使用静态库进行链接，生成的程序包含程序运行所需要的全部库，可以直接运行，不过静态链接生成的程序比较大。动态链接使用动态链接库进行链接，生成的程序在执行的时候需要加载所需的动态库才能运行。动态链接生成的程序体积较小，但是必须依赖所需的动态库，否则无法执行。 其他语言对于编译过程所做的优化1. C&#x2F;C++ 的问题：ABI不兼容 问题核心：代码被直接编译成特定CPU架构、特定操作系统、特定编译器、特定运行时库的原生机器码。任何一个环节不匹配，就会导致二进制层面的不兼容。 具体表现：.lib&#x2F;.a 链接错误，.dll&#x2F;.so 运行时崩溃，Debug&#x2F;Release 库混用导致内存错误等。 2. 现代原生编译语言的改进像 Rust 和 Go 这类语言，在设计之初就吸取了 C++ 的教训，从根本上解决了这类问题。 Rust 解决方案：语言自带了官方的构建系统和包管理器 Cargo。 如何解决： 统一的构建标准：所有 Rust 库都通过 Cargo 进行编译，保证了编译器版本和标志的一致性。 中央仓库：有一个官方的包仓库 (crates.io)，开发者可以轻松下载和分享库。 依赖解析：Cargo 会自动处理依赖的版本和传递性依赖，几乎杜绝了版本冲突。 C&#x2F;C++ 互操作：当 Rust需要调用 C&#x2F;C++ 库时，C++的“硬核”问题会再次出现在两种语言的边界上，但 Rust 内部生态是完全免疫的。 Go (Golang) 解决方案：默认静态链接，语言内置包管理工具。 如何解决： 无 DLL 地狱：go build 命令默认会将所有依赖的库（包括 Go 的运行时）静态编译成一个单一的可执行文件。部署时只需拷贝这一个文件，完全没有 .dll 或 .so 的烦恼。 Go Modules：内置的依赖管理系统，可以很好地控制版本。 小结：这类语言依然编译到原生代码，但通过强大的官方工具链统一了构建和分发过程，从源头上避免了二进制不兼容问题。 3. 虚拟机&#x2F;托管语言的改进Java&#x2F;Kotlin (JVM) 和 C#&#x2F;.NET 这类语言，采用了完全不同的策略。 解决方案：引入一个中间层——虚拟机 (VM) 或**运行时 (Runtime)**。 如何解决： 编译到字节码：代码不直接编译成机器码，而是编译成一种平台无关的中间语言（Java 的 Bytecode，C# 的 IL）。 运行时 JIT 编译：在程序运行时，由虚拟机（JVM）或 .NET 运行时将字节码即时编译（JIT）成本地机器码。 依赖是字节码：库依赖（如 Java 的 .jar 文件，.NET 的 .dll 文件）也是字节码。只要目标机器上安装了相应版本的运行时，这些库就可以直接使用，完全不存在编译器版本、运行时库（Debug&#x2F;Release）、平台架构的匹配问题。 成熟的包管理器：它们拥有非常成熟的包管理器，如 Java 的 Maven&#x2F;Gradle 和 .NET 的 NuGet，可以完美处理库的下载、版本和依赖。 小结：这类语言通过牺牲一些启动性能和内存占用，换来了彻底的跨平台和二进制兼容性，依赖管理体验非常好。 4. 动态&#x2F;解释性语言版本与环境兼容问题Python 和 JavaScript (Node.js) 这类语言不存在我们讨论的“编译”问题，但它们有自己的“依赖地狱”。 问题核心：库与库之间、库与解释器版本之间的API兼容性问题，以及环境隔离问题。 具体表现： 一个项目依赖 library-A 的 1.0 版本，另一个项目依赖 2.0 版本，这两个版本不兼容，导致全局安装时会出问题。 代码在一个库的新版本下行为不一致或直接报错。 解决方案： 虚拟环境：通过 venv (Python) 或 nvm (Node.js) 等工具创建隔离的项目环境。 包管理器：使用 pip (Python) 和 npm&#x2F;yarn (Node.js) 来管理 requirements.txt 或 package.json 中定义的依赖版本。 与 C++ 的再次相遇： 当 Python&#x2F;Node.js 需要高性能计算时，它们通常会调用 C&#x2F;C++ 编写的底层库（例如 Python 的 NumPy, PyTorch）。这时，C++ 的二进制兼容问题就“死灰复燃”了。你用 pip install 安装这类库时，它会尝试下载一个预编译好的二进制包（.whl 文件），这个包必须精确匹配你的操作系统、CPU架构（x86&#x2F;x64）和 Python 版本。如果找不到匹配的包，pip 就会尝试从 C++ 源码开始编译，这时你电脑上就必须装有正确的 C++ 编译器和库，然后你又会遇到我们之前讨论过的各种问题。 实际项目中需要做的准备工作 首先阅读项目文件中对于各个依赖库的版本要求，注意各个库文件的兼容性。当然如果是docker打包就可以减免这个环节了。 接下来修改CMakeLists，改一下其中的文件路径等等。 接下来最好使用命令行进行编译。使用cmake –build build –config Release 注意检查你的工具链和环境变量，由于C++中没有统一的虚拟环境或者包管理（除了docker），你需要格外注意你的编译器工具链配置。在我的电脑中有三个版本的MingW，包括普通版本，多线程版本，和QT中自带的。另外我一般使用Clion，因此需要同时配置普通的工具链和Cmake本身的工具链，以及环境变量。 常见报错及处理方法","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"C\\C++","slug":"C-C","permalink":"https://yaheii.github.io/tags/C-C/"},{"name":"编译流程","slug":"编译流程","permalink":"https://yaheii.github.io/tags/%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B/"},{"name":"库依赖","slug":"库依赖","permalink":"https://yaheii.github.io/tags/%E5%BA%93%E4%BE%9D%E8%B5%96/"},{"name":"C、C++混合编译","slug":"C、C-混合编译","permalink":"https://yaheii.github.io/tags/C%E3%80%81C-%E6%B7%B7%E5%90%88%E7%BC%96%E8%AF%91/"}],"author":null},{"title":"对网络的理解","slug":"对网络的理解","date":"2025-07-02T16:00:00.000Z","updated":"2025-07-23T08:55:07.175Z","comments":true,"path":"2025/07/03/对网络的理解/","permalink":"https://yaheii.github.io/2025/07/03/%E5%AF%B9%E7%BD%91%E7%BB%9C%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"","text":"方面一：垂直分层模型 (The “How” - 数据如何传输) TCP&#x2F;IP 五层模型（或更理论化的 OSI 七层模型）是理解网络的基石。 核心思想：它将一个极其复杂的网络通信过程，切分成了一系列定义清晰、功能独立的层次。每一层都只关心自己的任务，并使用下一层提供的服务，同时向上一层提供服务。 具体分工： 应用层 (Application Layer): 定义应用程序如何交互。例如，HTTP 协议定义了浏览器如何向 Web 服务器请求页面。 传输层 (Transport Layer): 提供端到端(End-to-End)的通信。例如，TCP 协议确保数据可靠、有序地送达；UDP 协议则提供更快速但不可靠的数据报服务。端口号是这一层的关键。 网络层 (Network Layer): 负责数据包在整个网络中的路径规划和转发。IP 地址是这一层的核心，路由器工作在这一层。 数据链路层 (Data Link Layer): 负责在相邻的两个网络节点之间（例如，您的电脑和家里的路由器之间）传输数据帧。MAC 地址是这一层的关键，交换机工作在这一层。 物理层 (Physical Layer): 负责传输原始的比特流（0和1）。它定义了网线、光纤、无线电波等的物理特性。 这个模型是自底向上构建的，描述了通信的实现机制。 方面二：水平通信架构 (The “Who” - 谁在和谁通信)客户端、服务端、（服务器）等架构，正是这个方面。它描述了网络中各个参与者的角色和关系。 核心思想： **定义角色与交互模式 (Define Roles and Interaction Patterns)**。它不关心数据包如何穿越网络，只关心谁发起请求，谁提供服务，以及它们之间的逻辑关系。 主要模式： 客户端-服务端架构 (Client-Server, C&#x2F;S): 这是最常见的架构。 服务端 (Server): 被动地等待请求，拥有资源或提供计算能力。通常拥有固定的 IP 地址，并且 7x24 小时在线。 客户端 (Client): 主动地发起请求，消费资源或服务。 特点： 角色分工明确，易于管理和扩展。我们之前讨论的 Web 服务就是典型例子。 对等网络架构 (Peer-to-Peer, P2P): 对等端 (Peer): 网络中没有中心服务器，每个节点既是客户端也是服务器。它们可以直接相互通信，共享资源和服务。 特点： 去中心化，扩展性好，不易产生单点故障。我们讨论的 WebRTC 在建立连接后，其音视频流传输就是 P2P 模式。 这个模型是端到端的，描述了应用的逻辑结构。 两者如何关联无论采用哪种水平通信架构（C&#x2F;S 或 P2P），其底层的数据传输都必须遵循垂直分层模型（TCP&#x2F;IP）。 一个客户端向服务器发送 HTTP 请求时，这个请求数据会从应用层开始，依次经过传输层、网络层……打包后发送出去。 一个 Peer 向另一个 Peer 发送 WebRTC 的音视频流时，这些数据流（通常封装在 UDP 包里）同样会经过这个垂直分层模型进行传输。 通信的动态过程通信模式 (Communication Paradigms)。它描述了数据交换随时间展开的方式。 请求-响应 (Request-Response): 客户端发起请求，然后等待服务器的响应。一次交互就结束了。这是 HTTP 的经典模式。 发布-订阅 (Publish-Subscribe): 一个发布者（Publisher）发布消息，多个订阅者（Subscriber）可以接收到这个消息，而发布者无需知道订阅者是谁。常用于消息队列（如 Kafka, RabbitMQ）。 流式传输 (Streaming)： 一旦建立连接，数据会像水流一样持续不断地从一端传输到另一端。例如视频直播、WebSocket 的数据流。 当分析一个系统（比如 WebRTC）时： 它使用了什么架构？（初始是 C&#x2F;S 获取网页和信令，最终是 P2P 传输媒体） 它的底层遵循什么协议栈？（HTTP, WebSocket, STUN, SRTP 等都运行在 TCP&#x2F;IP 之上） 它的通信模式是什么？（信令是请求-响应或流式，媒体是流式）","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://yaheii.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"分层模型","slug":"分层模型","permalink":"https://yaheii.github.io/tags/%E5%88%86%E5%B1%82%E6%A8%A1%E5%9E%8B/"},{"name":"通信架构","slug":"通信架构","permalink":"https://yaheii.github.io/tags/%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/"},{"name":"通信模式","slug":"通信模式","permalink":"https://yaheii.github.io/tags/%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%BC%8F/"}],"author":null},{"title":"RTP、RTCP、RTSP协议详解","slug":"RTP协议详解","date":"2025-06-26T16:00:00.000Z","updated":"2025-07-23T14:08:21.946Z","comments":true,"path":"2025/06/27/RTP协议详解/","permalink":"https://yaheii.github.io/2025/06/27/RTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"三者关系RTSP是一个实时的传输协议，是一个应用层的协议，包RTSP协议，RTP协议，RTCP协议。 RTSP协议主要是负责建立服务器和客户端之间的请求和相应，建立通信链路。 RTP协议是负责在服务器和客户端进行传输数据， RTCP协议四负责提供RTP传输质量的反馈，确保RTP传输的质量。 三者都处于应用层RTSP不会发送媒体数据，知识完成服务器和客户端之间的信令交互，RTP协议负责媒体数据传输，RTCP负责RTP数据包的监视和反馈，RTP和RTCP并没有规定传输层的类型，可以选择UDP或者是TCP，RTSP则要求最好是基于TCP。总体来说，RTSP是建立在RTP和RTCP之上的。 RTP首先要明确一点，RTP是架构在UDP之上的。在TCP协议中由于严格的重传机制，以及可靠协议特性，使其难以成为一个实时音视频领域协议。而UDP是一个不可靠协议，它并不严格要求包到达的顺序以及可靠到达，因此实时性最好。但是如何处理网络抖动以及丢包就成了难题，本部分重点在于RTP是如何解决这些问题的。 UDP 数据报的结构一个UDP数据报非常简单，它由两部分组成： **UDP头 (UDP Header)**：8个字节，包含源端口、目标端口、长度和校验和。 **UDP数据区 (UDP Payload&#x2F;Data)**：用来存放需要传输的数据。 1[ UDP头 | UDP数据区 (用来放东西的地方) ] 基于UDP的RTP RTP协议首先构建好自己的数据包： RTP头 (RTP Header)：12字节或更多，包含序列号、时间戳等。 RTP载荷 (RTP Payload)：实际的音视频数据，比如一小段H.264编码的视频帧。 1[ RTP头 | RTP载荷 (音视频数据) ] &lt;-- 这整个是一个完整的RTP包 RTP把整个RTP包（RTP头 + RTP载荷）不加改变地，“塞”进了上面提到的 UDP数据区。 最终形成的数据包结构 所以，当数据包在网络中传输时，它的嵌套结构是这样的： 123456789101112+-------------------------------------------------------------------+| IP头 (网络层) || +---------------------------------------------------------------+ || | UDP头 (传输层) | || | +-----------------------------------------------------------+ | || | | 这整个部分是 UDP 的“数据区” / “Payload” | | || | | +----------------------+--------------------------------+ | | || | | | RTP头 (应用层) | RTP载荷 (真正的音视频数据) | | | || | | +----------------------+--------------------------------+ | | || | +-----------------------------------------------------------+ | || +---------------------------------------------------------------+ |+-------------------------------------------------------------------+ 丢包问题在RTP头中会对每个数据包进行编号，因此接收端很容易就可以判断哪些包丢失了，这个字段是sequence number RTP扩展头通过设置RTP头中的X字段为1可开启扩展头 其中profile是为了区分不同的配置，{0x10，0x0X}和{0xBE,0xDE}分别代表two-byte-header或者one-byte-header节来解析数据。length表示有几个header_extension。 在header-extension中，one-byte-header表示在he中的数据由一个字节的header和N字节的Body组成。而header中有4位的ID和4位的len。 RTP中的填充数据通过设置RTP头的P位可开启填充数据 当RTP中包含填充数据时数据包最后一个字节记录包中填充数据字节个数（包括自己），解包的时候从后往前去掉即可。 端口数据类型区分 在一个 RTP 会话中，可能会有多个参与者发送媒体流。每个发送者都会选择一个唯一的 SSRC 值来标识自己产生的流。这使得接收端能够区分来自不同发送者的 RTP 包（即使在同一个端口输入或输出）。SSRC 字段是一个 32 位的数值，用于标识产生 RTP 包的源端系统或设备。对于任何给定的 RTP 会话，由同一个源产生的所有 RTP 包都应该具有相同的 SSRC 值。另外需要注意以下两种情况： 当一个混音器接收到来自多个源的 RTP 包并将它们混合成一个单一的流时，混音器会作为新的源产生 RTP 包，并且会使用它自己的 SSRC 值。原始的源 SSRC 值可能会被记录在 RTP 扩展头中。 一个转发器可能会在不同的网络之间转发 RTP 包，它通常会保持原始的 SSRC 值不变。 RTCPRTCP与RTP一样都是属于应用层的协议，其是RTP的控制协议。包括丢包控制；发送或者接受报告，其中包括上次报告到本次报告中间丢包率，延时等信息。 报文分类 SR和RR报文，分别用于发送和接收报文 SDES,用于描述音视频媒体源。 BYE报文用于说明哪些媒体源已经不可用，应该删除 APP报文，自定义报文 RTPFB,PSFB报文，未搞清楚什么作用。 需要注意这里的报文概念，与SDP，json、xml这类消息概念易混。 其中很重要的一个区别就是RTCP报文是二进制的。 RTCP协议头 重点是count字节，对于不同的报文类型，其含义是不同的。length字节表示整个的RTCP大小。 PT即payload Type RTSPRTSP的组成 OPTIONS： 用途： 客户端用 OPTIONS请求服务器支持的 RTSP 方法。 请求示例： OPTIONS rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 1\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复一个包含 Public:OPTIONS 头部的响应，列出它支持的方法： DESCRIBE, SETUP, PLAY, PAUSE, TEARDOWN DESCRIBE： 用途： 客户端使用 请求服务器上媒体文件的描述信息。这个描述信息通常以 SDP （Session Description Protocol） 格式提供，包含了媒体的类型（例如音频、视频）、编码格式、传输方式等。 请求示例： DESCRIBE rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 2\\r\\nAccept: application&#x2F;sdp\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复一个包含媒体描述信息的 SDP 内容。 SETUP： 用途： 客户端在播放媒体之前，需要使用 SETUP 命令为每个媒体流组件（例如音频和视频轨道）建立传输会话。客户端会指定它希望使用的传输协议（例如 RTP&#x2F;UDP 或 RTP&#x2F;TCP）以及接收数据的端口。 请求示例 （RTP&#x2F;UDP）： SETUP rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4&#x2F;trackID&#x3D;1 RTSP&#x2F;1.0\\r\\nCSeq: 3\\r\\nTransport: RTP&#x2F;AVP;unicast;client_port&#x3D;12345-12346\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 请求示例 （RTP&#x2F;TCP）： SETUP rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4&#x2F;trackID&#x3D;1 RTSP&#x2F;1.0\\r\\nCSeq: 3\\r\\nTransport: RTP&#x2F;AVP&#x2F;TCP;unicast;interleaved&#x3D;0-1\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复确认所选的传输参数，并分配一个会话。 PLAY： 用途： 客户端使用 命令指示服务器开始发送流媒体数据。客户端可以指定播放的时间范围。 请求示例 (从头开始播放): PLAY rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 4\\r\\nSession: 1234567890\\r\\nRange: npt&#x3D;0.000-\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 请求示例 (播放特定时间段): PLAY rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 4\\r\\nSession: 1234567890\\r\\nRange: npt&#x3D;10.5-25.0\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复确认开始播放。 PAUSE： 用途： 客户端使用 PAUSE 命令暂停流媒体的播放。客户端可以指定暂停的时间。 请求示例: PAUSE rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 5\\r\\nSession: 1234567890\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复确认已暂停播放。 TEARDOWN： 用途： 客户端使用 TEARDOWN 命令通知服务器终止流媒体会话并释放相关资源。 请求示例: TEARDOWN rtsp:&#x2F;&#x2F;example.com&#x2F;media.mp4 RTSP&#x2F;1.0\\r\\nCSeq: 6\\r\\nSession: 1234567890\\r\\nUser-Agent: MyClient\\r\\n\\r\\n 响应： 服务器会回复确认会话已终止。 其他不太常用但仍然重要的命令： GET_PARAMETER： 用于查询服务器或会话的参数。 SET_PARAMETER： 用于设置服务器或会话的参数。 REDIRECT： 服务器可以发送 响应，指示客户端连接到另一个服务器。REDIRECT RTSP的过程一次基本的RTSP操作过程是，首先，客户端线连接到流媒体服务器，并发送i个（DESCRIBE）。流服务器通过一个SDP描述来进行反馈，反馈信息包括流数量，媒体类型等信息，客户端在分析描述符后 ,并为每个流发送SETUP命令，这个命令来告诉服务器，客户端用于接收媒体数据的端口，流媒体连接建立完成后，客户端发送i个播放命令PLAY服务器就可以在UDP上传送媒体流，RTP包到客户端，在播放过程中客户端还可进行快进，快退，等操作。 RTSP与HTTP的关系 RTSP引入了几种新的方法，比如DESCRIBE、PLAY、SETUP 等，并且有不同的协议标识符，RTSP为rtsp 1.0,HTTP为http 1.1； HTTP是无状态的协议，而RTSP为每个会话保持状态； RTSP协议的客户端和服务器端都可以发送Request请求，而在HTTPF协议中，只有客户端能发送Request请求。 在RTSP协议中，载荷数据一般是通过带外方式来传送的(除了交织的情况)，及通过RTP协议在不同的通道中来传送载荷数据。而HTTP协议的载荷数据都是通过带内方式传送的，比如请求的网页数据是在回应的消息体中携带的。 使用ISO10646(UTF-8) 而不是ISO 8859-1，以配合当前HTML的国际化； RTSP使用URI请求时包含绝对URI。而由于历史原因造成的向后兼容性问题，HTTP&#x2F;1.1只在请求中包含绝对路径，把主机名放入单独的标题域中； RTSP协议格式文章 在client和server的连接中，还需要使用SDP描述符，连接中有关于SDP的协议说明。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"WebRtc","slug":"WebRtc","permalink":"https://yaheii.github.io/tags/WebRtc/"},{"name":"RTCP","slug":"RTCP","permalink":"https://yaheii.github.io/tags/RTCP/"},{"name":"RTP","slug":"RTP","permalink":"https://yaheii.github.io/tags/RTP/"},{"name":"RTSP","slug":"RTSP","permalink":"https://yaheii.github.io/tags/RTSP/"}],"author":null},{"title":"WebRTC中的拥塞控制","slug":"拥塞控制","date":"2025-06-26T16:00:00.000Z","updated":"2025-07-03T11:43:02.105Z","comments":true,"path":"2025/06/27/拥塞控制/","permalink":"https://yaheii.github.io/2025/06/27/%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"在WebRTC中有多种控制算法，包括GCC,BBR,PCC。GCC又可以根据基于发送端或接收端，分为Transport-CC，Goog-REMB。 Goog-REMB 图中左侧为发送端，控制码流的发送；右侧为接收端，用于拥塞的评估和码流的计算 RemoteBitrate Estimator是接收端延迟拥塞控制算法的管理模块，从网络收发磨矿获取RTP包的传输信息用于拥塞评估。或者将内部评估出的下一时刻的发送码流大小，输出给网络收发模块，从而进行流控。另一方面，它要组织内部的Inter Arrival、OverUser Estimator等模块，根据当前观测到的延时差，和之前的评估指推断出下一时刻的网络拥塞情况 Inter Arrival首先将数据包按照帧分组，然后对相邻的两组数据包进行单项梯度计算。计算内容包括 每组数据包的发送时长 每组数据包的接收时长 两组数据包的大小差 OverUser Estimator利用IA计算出的结果，通过卡尔曼滤波估算出下一时刻发送队列的增长趋势。网络带宽是不断变化的，卡尔曼滤波器能够根据间接测量值，估算出真实结果。因此选用卡尔曼根据数据包时延来对带宽进行估计。 OverUse Detector用于检测当前网路中的拥塞情况，利用OE计算出的队列梯度延时以及自适应阈值进行比较。决定发包量的策略。 AIMD Rate Controller用于计算发送码流的大小，通过OD模块检测出当前网络状态，从而变更自己的状态，并计算出发送码流的大小 原理很简单，我们假设发送码流越大，状态越好。当OD检测出网络质量相比于现在呈上升，趋势，那么状态也上升。反之亦然 Transport CC在这个算法中，将拥塞算法从接收端移到了发送端，将卡尔曼换成了TrendLine GooCcNetworkController同样是对各个模块的控制。会调用子模块评估出下一时刻的网络拥塞状态和码流大小。并将评估出的码流交由Pacer和编码器模块进行码流控制 SendSideBandwidthEstimation比较基于接收端延时与发送端延时评估出的码流值，以及基于丢包，从中选择最小的码流值作为最终 DelayBaseBwe由多个模块构成，用于延时拥塞评估 trendline最小二乘法滤波器，拟合曲线。通过某一时刻线的斜率来判断此时线路是否拥塞，评估下一时刻码流大小 基于丢包的拥塞评估前面已经有两种方法用于拥塞评估，分别是卡尔曼和TrendLine，都是基于延时的。 还有一种基于丢包的，实际上就是通过设置丢包门限来评定此时的网络传输质量。 &lt;2%,网络质量很好，可以加大码率 2%&lt;x&lt;10%,说明网络与发送速率匹配 10%&lt;,需要降低码率至（1-0.5丢包率）当前码率 拥塞控制算法","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"WebRtc","slug":"WebRtc","permalink":"https://yaheii.github.io/tags/WebRtc/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"https://yaheii.github.io/tags/%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/"}],"author":null},{"title":"WebRTC中的SDP规范","slug":"WebRTC中的SDP规范","date":"2025-06-25T16:00:00.000Z","updated":"2025-08-02T06:17:04.876Z","comments":true,"path":"2025/06/26/WebRTC中的SDP规范/","permalink":"https://yaheii.github.io/2025/06/26/WebRTC%E4%B8%AD%E7%9A%84SDP%E8%A7%84%E8%8C%83/","excerpt":"","text":"SDP结构123456789101 v=0 2 o=- 3409821183230872764 2 IN IP4 127.0.0.1 3 … 4 m=audio 9 UDP/TLS/RTP/SAVPF 111 103 104 … 5 … 6 a=rtpmap :111 opus /48000/2 7 a=rtpmap :103 ISAC /16000 8 a=rtpmap :104 ISAC /32000 标准SDP规范的规定较为简单，前三行为会话描述，对整个SDP有约束作用；第四行为媒体描述，各个媒体描述之间互不影响。在整个SDP中只能有一个会话描述，但是可以有多个媒体描述。 SDP的描述格式同样较为简单1&lt;type&gt;=&lt;value&gt;SDP中的信息webrtc为了实现实时的通信，对标准的SDP做了较大的调整。这里仅展示wR中的SDP内容 会话部分1234v:协议版本o:会话创建者s:会话名t:会话时长 (下面部分选自李超老师书籍WebRTC音视频技术) 媒体描述 媒体信息在SDP中最重要的内容就是媒体信息。我们看一下 SDP中媒体信息的具体格式，如下所示：1m＝＜media＞＜port＞／＜numbers＞＜transport＞＜fmt＞．．． 其中，＜media＞表示媒体类型，可以是audio、video等。 ＜port＞／＜numbers＞表示该媒体使用的端口号。对于WebRTC而言，由于它不使用SDP中描述的网络信息，所以该端口号对它没任何意义。＜transport＞表示使用的传输协议，可以是UDP、TCP等。 ＜fmt＞表示媒体数据类型，一般为PayloadType列表，其具体含义需要使用＂a＝rtpmap：＂属性做进一步阐述。 我们来看一个具体的例子，如代码7．2所示。从代码中可以看到， media的值为audio，表示该媒体的类型为音频；port为9，可以直接忽略，因为WebRTC不使用标准SDP中的网络信息，所以这里的端口也就失去了意义；transport为UDP／TLS／RTP／SAVFP，表示底层使用了哪些传输协议；fmtlist的值为一串从111到126的数字，每个数字代表一个PayloadType，不同的PayloadType表示媒体数据使用了不同的编解码器或编解码器参数。 上面提到的UDP／TLS／RTP／SAVFP，其含义为：传输时底层使用 UDP；在UDP之上使用了DTLS协议来交换证书；证书交换好后，媒体数据由RTP进行传输（RTP运行在UDP之上），保证传输的可靠性；媒体数据（音视频数据）的安全性是由SRTP负责的，即对RTP包中的Body部分进行加密。此外，传输时还使用RTCP的feekback机制对传输信息进行实时反馈（SAVPF），以便进行拥塞控制。代码7．2 媒体信息 12341 ...2 m=audio 9 UDP/TLS/RTP/SAVPF 111 103 104 9 0 8 106 105 13110 112 113 1263 ... 通过上面的介绍，我们已经清楚了SDP中的媒体信息是用来做什么的。不过媒体信息不只有上面的这些内容，它还有很多＂$a&#x3D;$＂的属性用来对前面的信息做进一步解释，如每个PayloadType的详细参数就是由它们说明的。音频媒体的描述前面已经介绍过了，但还有很多细节没有介绍。这些细节是无法通过一条＂ $\\mathrm{m}&#x3D;$＂行就能够描述清楚的，必须通过 ＂$a&#x3D;r t p m a p$＂对其做进一步解释才行。如代码7．3所示，在这段代码中，使用大量的＂ $\\mathrm{a}&#x3D;\\mathrm{rtpmap”}$ 属性对＂ $\\mathrm{m}&#x3D;$＂行做进一步阐释。 代码7．3 音频媒体示例 123456789m = audio 9 UDP/TLS/RTP/SAVPF 111 103 104 9 ......a=rtpmap :111 opus /48000/2a=rtcp -fb:111 transport -cca=fmtp :111 minptime =10; useinbandfec =1a=rtpmap :103 ISAC /16000a=rtpmap :104 ISAC /32000a=rtpmap :9 G722 /8000... 这段代码中的第 1 行代码是对音频媒体的描述；第 $3 、 6 、 7 、 8$ 行代码使用＂a＝rtpmap＂解释了PayloadType使用的编解码器及其参数是什么；第 5 行代码＂$a&#x3D;f m t p$＂属性指定了PayloadType的数据格式，即音频帧最小 10 ms —帧，使用带内FEC。 在WebRTC的SDP中，＂a＝rtpmap＂＂a＝fmtp＂属性随处可见。无论是音频媒体中，还是视频媒体中，都使用它们对媒体做进一步的解释。 ＂a＝rtpmap＂属性 rtpmap（rtp map），通过字面含义可以知道它是一张 PayloadType与编码器的映射表，每个PayloadType都对应一个编码器。其格式定义在RFC4566中，如下所示：a＝rtpmap：＜payload type＞＜encoding name＞／＜clock rate＞［／＜encodingparameters＞］ 通过上面rtpmap的格式，可以很容易理解代码7．3中第3行代码的含义：Payload Type值为 111 的编码器是Opus，其时钟频率（采样率）为48000，音频通道数为2。同理，PayloadType值为103的编码器是ISAC，采样率为 16000 ；PayloadType值为 104 的编码器也是 ISAC，只不过其采样率变成了 32000 ；PayloadType值为 9 的编码器是G722，采样率是 $8000 . . . . .$. ＂ $\\mathrm{a}&#x3D;\\mathrm{fmtp}$＂属性fmtp（format parameters）用于指定媒体数据格式。 ＂$a&#x3D;f m t p$＂属性的格式与rtpmap一样也是定义在RFC4566的第6节中，如下所示：a＝fmtp：＜format＞＜format specific parameters＞ 现在再来看一下代码7．3中的第5行代码，它描述了PayloadType值为 111 的数据（Opus数据）：以 10 ms 长的音频数据为一帧，并且数据是经FEC编码的。其中，＂usein bandfec $&#x3D;1$＂是WebRTC针对 Opus增加的fmtp值。如果你想了解这些细节，可以看一下相关的草案。 与音频媒体信息相比，视频媒体信息要复杂一些，在SDP中视频相关的描述如代码7．4所示。 代码7．4 视频媒体 123456789101112131415161718m=video 9 UDP/TLS/RTP/SAVPF 96 ... 102 121 124 ...a=mid:1a=rtpmap :96 VP8 /90000...a=rtpmap :97 rtx /90000a=fmtp :97 apt =96...a=rtpmap :102 H264 /90000...a=fmtp :102 level -asymmetry -allowed =1; packetization -mode =1; profile -level -id =42001f13 a=rtpmap :121 rtx /9000014 a=fmtp :121 apt =10215 ...16 a=rtpmap :124 red /9000017 a=rtpmap :119 rtx /9000018 a=fmtp :119 apt =12419 ... 其中，第 1 行代码为视频的＂ $\\mathrm{m}&#x3D;$＂行，其与音频的＂ $\\mathrm{m}&#x3D;$＂行类似，区别在于两者的媒体类型不同：一个是＂video＂，另一个则是 ＂audio＂。此外，第1行中的PayloadType列表也发生了变化，这个很好理解，视频媒体使用的编码器本就与音频媒体使用的不同。第3行代码表明视频媒体的ID编号为1，而音频媒体的ID编号为0。如果有更多的媒体，编号会一直累加。第 $5 \\sim 18$ 行代码是对不同 PayloadType的解释，下面看一下它们是如何解释PayloadType的吧。 第 5 行代码，PT（PayloadType）值为 96 表示媒体数据使用的编码器是VP8，其时钟频率为 90000 。又因为其排在＂ $\\mathrm{m}&#x3D;$＂行PT列表的第一位，所以它还是视频的默认编码器。同理，代码第 10 行，PT值为102表示媒体数据使用的是H264编码器，时钟频率也是 90000 。 第 7 行代码，PT值为 97 表示的含义与之前 PT值为 96 的情况有所不同，rtx表示的不再是编码器，而是丢包重传。要想弄明白第 7 行代码的含义，必须与第 8 行代码结合着一起看。在第 8 行代码中， apt（associated payload type）的值为 96 ，说明 96 与 97 是关联在一起的，PT＝97是PT＝96的补充。因此第7行代码的含义是：当 WebRTC使用的媒体数据类型（PayloadType）为96时，如果出现丢包需要重传，重传数据包的PayloadType为97。同理，第13～14行代码指明121是PT＝102重传包的PayloadType。 第16～18行代码较为特殊，要想了解这三行代码的含义，你还需要了解一些额外知识：一是red，它是一种在WebRTC中使用的FEC算法，用于防止丢包；二是red编码流程，默认情况下WebRTC会将 VP8／H264等编码器编码后的数据再交由red模块编码，生成带一些冗余信息的数据包，这样当传输中某个包丢了，就可以通过其他包将其恢复回来，而不用重传丢失的包。了解了上面这些内容后，第 $20 \\sim 22$行代码的含义应该就清楚了，即PT值为 124 表示需要使用red对之前编码好的数据再进行 red 处理， 119 是 PT $&#x3D;124$ 重传数据包的 PayloadType。如果用Wireshark等抓包工具抓取WebRTC媒体数据包时会发现它们都是red包，而在red包里装的是VP8／H264编码的数据。 再看一下与 H 264 相关的 fmtp 内 容。第 12 行代码，level－ asymmetry－allowed $&#x3D;1$ 指明通信双方使用的 H264Level是否要保持一致：0，必须一致；1，可以不一致。packetization mode指明经 H264编码后的视频数据如何打包，其打包模式有三种：0，单包；1，非交错包；2，交错包。三种打包模式中，模式0和模式1用在低延时的实时通信领域。其中模式 0 的含义是每个包就是一帧视频数据；模式1的含义是可以将视频帧拆成多个顺序的RTP包发送，接收端收到数据包后，再按顺序将其还原。profile－level－id由三部分组成，即 profile＿idc、profile＿iop以及level＿idc，每个组成部分占8位，因此可以推测出profile＿idc $&#x3D;42$ 、profile＿iop $&#x3D;00$ 、level＿idc $&#x3D;1$ f。关于这几个值的具体含义，如果读者感兴趣，可以自行查看H264规范手册。 以上分析将SDP中视频媒体信息相关的内容及其含义讲解清楚了。音视频媒体信息是SDP中最为重要的内容，读者一定要牢牢掌握。 另外一个媒体描述是SSRC，它是媒体源的唯一标识。需要注意的是，虽然原则上要求每一路媒体流都只有一个唯一的SSRC来标识它。但是我们可以使用 1a = ssrc -grou:FID XXXXX MMMMM 不同的ssrc标识符来区分真正的视频流，以及重传的视频流。 SDP的版本1标准SDP-&gt;PlanB-&gt;UnifiedPlan PlanB和Unified的最大的区别是，前者只有两个媒体描述，而如果要穿上多路的音视频流，那么这个时候要使用SSRC来进行区分，而在后这中可以有多个媒体描述，因此对于多路视频的情况只需要拆分成多个媒体描述（“m&#x3D; ”）即可 RTP扩展头通过使用“a&#x3D;exemap”扩展头，在原有的UDP基础上完完成扩展，进行SDP传输 服务质量“a&#x3D;rtcp-fb”,需要注意的是，这个字段即可以表示RTCP中专门反馈消息的一类消息。二是设置终端支持哪些feedback消息，通过设置编码器，拥塞算法等参数，影响服务质量。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"WebRtc","slug":"WebRtc","permalink":"https://yaheii.github.io/tags/WebRtc/"},{"name":"音视频服务质量","slug":"音视频服务质量","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"}],"author":null},{"title":"提高实时通信中音视频服务质量的方法","slug":"webrtc中的实时通信增强方法","date":"2025-06-22T16:00:00.000Z","updated":"2025-07-03T11:46:34.597Z","comments":true,"path":"2025/06/23/webrtc中的实时通信增强方法/","permalink":"https://yaheii.github.io/2025/06/23/webrtc%E4%B8%AD%E7%9A%84%E5%AE%9E%E6%97%B6%E9%80%9A%E4%BF%A1%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/","excerpt":"","text":"实时通信与带宽大小网络质量息息相关，根据香农定理和奈奎斯特定理其实很容易知道，码率、延迟和服务质量本身就是相悖的。而再加上信号传输本身的带宽有限（排除WebRtc中使用的架构），更加难以达到一个均衡。 那么为了提高音视频服务质量又该如何下手呢。有以下五个方面，这篇文章主要记录每个方面的一些重要算法。 增加带宽 减少数据量 适当增加时延 提高网络质量 快速准确评估带宽 增加带宽首先应该提到的肯定不是WebRtc中的一些算法，而是譬如5G网络，星链，新型传输材料，波分复用，时分复用，频分复用，等等通信方向的知识。但这并不在笔者将要探讨的WebRtc框架体系之内。 在WR内，客户端方面主要使用了一种极妙的选路方案来提高整体通信的带宽，即TURN-STUN结构。 STUN-TURN架构减少数据量 压缩算法，通过更好的压缩算法可以实现更高的码率 SVC技术，将视频按照时间、空间、质量分成多层编码、，然后将他们装在同一路发送给服务端，服务端收到后再根据每个用户的带宽情况不同来选择不同的层下发。 simulcast，将视频编码分成多个不同分辨率的多路码流，然后上传至服务端，服务端在根据客户的不同情况来进行下发。 动态码率 甩帧，减少业务， 增加时延实际上增加时延是增加一个缓冲区，先把到来的数据方法队列中缓冲一下，这样的话就可以减少网络抖动造成的卡断、快播、吞音等现象。一般而言对于实时音视频延迟应该控制在500ms以内。 提高网络质量对于网络质量主要有三个影响因素 丢包，优质网络丢包率应该小于2%，对于WR丢包率应该限制在2%-10% 延迟，拥塞 抖动，抖动较小的情况下，可以通过循环队列将其消除，抖动过大就会将乱序包作为丢包处理。在WR中抖动时长不能超过10ms，超过10ms会被视为丢包 解决上面问题的方法： NACK&#x2F;RTX NACK是RTCP的一种消息类型，向接收端报告一段时间内有哪些包丢失了，RTX是指向发送端重新发送丢失包 前向纠错，使用异或方式进行发送，以便在丢包时可以通过这种机制恢复丢失的包 JitterBufer 用于防抖动， 可以将少量乱序包恢复成有序 NetEQ 用于音频的防抖动","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"WebRtc","slug":"WebRtc","permalink":"https://yaheii.github.io/tags/WebRtc/"},{"name":"音视频服务质量","slug":"音视频服务质量","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"}],"author":null},{"title":"SIMPLEST_FFMPEG_PLAYER源码阅读","slug":"SIMPLEST_FFMPEG_PLAYER源码阅读","date":"2025-06-09T16:00:00.000Z","updated":"2025-07-20T07:36:12.075Z","comments":true,"path":"2025/06/10/SIMPLEST_FFMPEG_PLAYER源码阅读/","permalink":"https://yaheii.github.io/2025/06/10/SIMPLEST_FFMPEG_PLAYER%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"对于FFmpeg的使用中，理解各个结构体的作用至关重要。首先在使用之前，avdevice_register_all()和 avformat_network_init()来分别对设备和网络进行初始化 解协议（http,rtsp,rtmp,mms）AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”） AVIOContext是FFmpeg中用于执行输入输出操作的抽象层，提供协议层之上的抽象接口，它负责处理数据流的读写和定位，而不需要知道底层数据的来源，因此可以处理本地，网络，内存缓冲区多个流来源。这个结构体包含了读写缓冲区的指针，缓冲区的大小，当前读写位置，和读写位置的指针（这些具体的指针指向集体的协议实现）等 在古早版本的FFmpeg中UP用于定义和注册不同的底层协议（如file\\http\\rtp\\tcp\\udp等），每个这样的结构体中都包含了针对该协议的一组回调函数（如url_open\\url_read\\url_read\\url_write\\url_seek\\url_close等）。但是在现代的FFmpeg中已经更多的整合和抽象进入AC中了。 同样的在古早版本中，UC是UP的一个具体实例，用于存贮某个已经打开协议连接的上下文信息。但是在新版的FFmpeg中这个结构也已经被整合到了AC中 解封装（flv,avi,rmvb,mp4）AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。 avformat_alloc_contex()申请一个AVFormatContext内存 avformat_free_contex 释放该结构里的所用东西以及该结构本身 avformat_close_input() 关闭解复用器，关闭后就不需要使用avfoemat_free_contex来进行释放，常用 avformat_open_input 打开输入视频文件 avformat_find_stream_info 获取音视频文件信息 afvv_read_frame 读取音视频包 avformat_seek_file 定位文件，拖拉进度条，根据pts av_seek_frame 定位文件，根据比例 在阅读雷博士写的播放器的时候能够轻易地发现这个结构体是一个很重要的结构体，它不仅是用于解封装到解码中间，在其他地方也是可以看到的。但是在这个结构体中实际上并没有什么很复杂的信息。只有类似音视频流的个数，音视频流，时长，比特率之类的东西。以及显示音视频流（文件）的源信息 解码（h264,mpeg2,aac,mp3） avcodec_alloc_contex3() 分配解码器上下文 avcodec_find_decoder() 根据ID查找解码器 avcodec_find_decoder_by_name() 根据名字查找解码器。不同厂家所制造的解码器ID相同，但是名字可以不同 avcodec_open2 打开编解码器 avcodecd_send_packet() 发送编码数据包 avcodec_recieve_frame() 接收解码器数据 avcodec_free_contex 释放解码器上下文包含 avcodec_close 关闭解码器每个AVStream存储一个视频&#x2F;音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频&#x2F;音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频&#x2F;音频对应的解码器。每种解码器都对应一个AVCodec结构。 AVStream 这个结构体可以理解成为媒体文件中的轨道信息。 AVCodecContex 这个结构体在编码和解码中都有应用，用于存储编解码器的上下文实例 解码过程，用于存储和接收编码器配置，打开一个媒体文件时候FFmpeg会从文件头部解析出视频流的编码参数，包括像视频宽度高度，像素格式，时间基准，音视频采样率等。另外配置编码器选项，跳帧策略，线程数，错误隐藏策略，是否允许多线程解码。传递压缩数据和接收原始数据： 会把压缩后的 AVPacket （例如一个H.264 NALU）传递给 AVCodecContext 对应的解码器（通过 avcodec_send_packet）。解码器处理后，会从 AVCodecContext 对应的解码器中读取解码后的 AVFrame （例如YUV像素数据或PCM采样数据，通过 avcodec_receive_frame）。 编码过程。首先还是设置编码参数，包括输入原始视频的尺寸和格式，编码器的时间基准，B帧最大数量等。配置编码选项，例如一些预设和调优，编码速度，编码模式等。传递原始数据和接收压缩数据： 你会把原始的 AVFrame （例如YUV像素数据或PCM采样数据）传递给 AVCodecContext 对应的编码器（通过 avcodec_send_frame）。编码器处理后，你会从 AVCodecContext 对应的编码器中读取压缩后的 AVPacket （例如H.264 NALU或AAC帧，通过 avcodec _receive_packet）。 这个接口一般是应用程序和底层编解码算法之间的桥梁，FFmpeg由此提供了统一的接口来配置不同的编解码器。有关于其他参数的解析还是看一下雷博士的博客。 AVCodec AVCodec代表了一个具体的已经注册的编码器算法，它是一个只读的结构体，包含着这个编码器本身的静态通用信息，和能力。以及只想内部实现函数的指针。一般通过avcodec_find_decoder或者avcodec_find_encoder来获取这样一个指针。可以理解这是一个解码算法所提供的指针。 要注意AC的存储是以一个全局的注册列表形式（可能是一个链表或是其他的数据结构）实现的。由于FFmpeg时支持多种音视频编解码的这些编解码器在编译时可以被静态链接，作为共享库进行动态加载，允许 FFmpeg 在运行时动态地查找和选择合适的编解码器。 AVCodec 定义了编解码器的抽象接口（例如 decode, encode, init, close 等函数指针）。具体的编解码器实现（例如 libx264、libvpx、libfdk_aac 等第三方库或 FFmpeg 内部实现）则通过 AVCodec 结构体将自己注册到 FFmpeg 中。这种设计使得 FFmpeg 核心库无需知道所有编解码器的具体实现细节，只需通过统一的 AVCodec 接口来调用它们。 存数据视频的话，每个结构一般是存一帧；音频可能有好几帧 解码前数据：AVPacket 这个结构体比较简单，都是一些时间戳，大小之类的数据， 解码后数据：AVFrame AF中包含了多个码流参数，其中又以data数组为核心，主要是存储原始数据。在data数组中对于packed和planar数据的存储格式是不一样的。 在AVPictureType结构体中IBP较为常见，但是要注意S\\SI\\BI\\BP帧类型，S 帧是一种特殊的编码帧，它本身是帧间预测的（像 P 帧），但它的预测信息（运动矢量、残差）可以被后续的 I 帧或 P 帧作为参考。SI 帧是一种特殊的 I 帧。它所有的宏块都是帧内编码的（像 I 帧一样），所以它是一个独立的帧，不依赖其他帧进行预测。 它的主要作用也是提供随机访问点，但它通常是为了更快速、更鲁棒的切换而设计。与普通的 I 帧相比，SI 帧可能在编码方式或解码端处理上有一些特定优化，以确保在它这里可以立即开始一个新的解码序列。BI 帧是一种非常特殊的帧，它包含的宏块全部是帧内编码的（就像 I 帧），但它却像 B 帧一样，既可以被向前参考，也可以被向后参考。SP 帧是一种特殊的 P 帧。它的大部分宏块都是帧间预测的（像 P 帧），但它的预测信息（运动矢量、残差）可以被后续的 I 帧或 P 帧作为参考。 qscale_table，网上绝大部分关于这个结构体的论述都是搬得雷博士的，需要注意宏块在视频帧指的是一个矩形块，而QP值其实是QP_step的一个索引，规定了宏块的采样步数，这个索引表是可以被查到的。 另外以及像运动矢量和运动估计参考帧都比较容易理解 在内存中packet和frame是如何进入队列，并等待被利用的呢 首先packet的拷贝有两种，深拷贝和浅拷贝，packet内部会初始化一个buffer指针，通过引用这个指针实现对数据包的引用。以浅拷贝为例，由于指针固定，无法精准保证释放的时候别的部分没有利用这部分数据空间，因此引入计数器，引用+1.释放—1，如果引用计数为0，那么就可以释放这片空间了。Frame的机理也是一样的 av_packet—alloc,ava_packet_free。申请和释放空间 av_init_packet 初始化AVpacket字段 av_new_packet为buffer分配内存，计数初始化为1 av_packet_ref 增加计数 av_packet_unref 减少计数 av_pakcet_move_ref 转移计数 av_packet_clone 申请结构体，并且计数加1 接下来是以雷博士做的SIMPLEST_FFMPEG_PLAYER为例（注意是第一版），在上面添加注释，详细分析每个语句的作用，以及视频播放器的工作流程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317/** **核心流程：****FFmpeg 部分：*** 初始化 FFmpeg 库。* 打开视频文件并查找视频流。* 查找并打开视频解码器。* 分配用于存储解码帧和转换后YUV帧的内存。* 初始化 SWS_Scaler (用于图像格式转换)。**SDL 部分：*** 初始化 SDL 库。* 创建 SDL 窗口、渲染器和纹理。**主循环：*** 从视频文件中读取数据包 (`AVPacket`)。* 将数据包发送给解码器 (`avcodec_decode_video2`)。* 如果解码器成功解码出图像 (`got_picture` 为真)，则：* 使用 SWS_Scaler 将解码后的帧转换为 YUV420P 格式。* （可选）将 YUV420P 数据写入文件。* 使用 SDL 更新纹理、清空渲染器、复制纹理到渲染器并呈现，从而显示视频帧。* 延时一小段时间以控制播放速度。* 刷新解码器（处理剩余的帧）。* 释放所有分配的资源并退出。**//** * 最简单的FFmpeg播放器 2 * Simplest FFmpeg Player 2 * * 作者：雷霄骅 Lei Xiaohua.注释作者：yahei * 版本2：使用SDL 2.0取代了版本1中的SDL 1.2。 * Version 2 use SDL 2.0 instead of SDL 1.2 in version 1. * * 本程序实现了一个视频文件的解码和显示（支持HEVC, H.264, MPEG2等）。 * 这是一个最简单的FFmpeg视频解码方法。 * 通过学习这个示例可以初步了解FFmpeg的解码流程。 * This software is a simplest video player based on FFmpeg. * Suitable for beginner of FFmpeg. * */#include &lt;cstddef&gt;#include &lt;stdio.h&gt;// 宏定义，确保stdint.h中的常量宏可用，例如UINT64_C#define __STDC_CONSTANT_MACROS// 根据操作系统类型包含不同的头文件路径#ifdef _WIN32// Windows 平台extern &quot;C&quot; // 声明为C风格链接，因为FFmpeg和SDL库是C语言编写的&#123;#include &quot;include/libavcodec/avcodec.h&quot; // 包含FFmpeg编解码器库头文件#include &quot;include/libavformat/avformat.h&quot; // 包含FFmpeg封装格式库头文件#include &quot;include/libswscale/swscale.h&quot; // 包含FFmpeg图像缩放/格式转换库头文件#include &quot;SDL2/SDL.h&quot; // 包含SDL2库头文件&#125;;#else// Linux 或其他类Unix平台#ifdef __cplusplus // 如果是C++编译器，使用extern &quot;C&quot;extern &quot;C&quot;&#123;#endif#include &lt;libavcodec/avcodec.h&gt;#include &lt;libavformat/avformat.h&gt;#include &lt;libswscale/swscale.h&gt;#include &lt;SDL2/SDL.h&gt;#ifdef __cplusplus&#125;;#endif#endif// 宏定义：是否将YUV420P数据输出到文件// 如果定义为1，则会将解码并转换后的YUV420P数据写入output.yuv文件// 如果定义为0，则不写入文件#define OUTPUT_YUV420P 0int main(int argc, char* argv[])&#123; // FFmpeg 相关的结构体指针 AVFormatContext *pFormatCtx; // 封装格式上下文，包含了文件的整体信息（如流的数量、时长等） int i, videoindex; // i用于循环，videoindex用于存储视频流的索引 AVCodecContext *pCodecCtx; // 编解码器上下文，包含了编解码器操作所需的所有参数（如宽度、高度、像素格式等） AVCodec *pCodec; // 编解码器，代表一个具体的编解码器（如H.264解码器） AVFrame *pFrame, *pFrameYUV; // pFrame用于存储解码前的数据包（原始数据），pFrameYUV用于存储解码后并转换为YUV420P格式的帧 uint8_t *out_buffer; // 用于存储转换后YUV420P数据的缓冲区 AVPacket *packet; // 数据包，存储从文件中读取的压缩数据（如H.265或H.264帧） int y_size; // Y分量的大小，用于计算YUV数据的总大小和写入YUV文件 int ret, got_picture; // ret用于接收函数返回值，got_picture指示是否解码出一张完整的图片 // SWS_Scaler 上下文，用于图像格式转换（例如从解码器的原生像素格式转换为YUV420P） struct SwsContext *img_convert_ctx; // 输入文件路径 char filepath[] = &quot;bigbuckbunny_480x272.h265&quot;; // 要播放的视频文件路径 // SDL 相关变量 int screen_w = 0, screen_h = 0; // 屏幕（窗口）的宽度和高度 SDL_Window *screen; // SDL 窗口对象 SDL_Renderer* sdlRenderer; // SDL 渲染器，用于在窗口上绘图 SDL_Texture* sdlTexture; // SDL 纹理，用于存储YUV数据并渲染到屏幕 SDL_Rect sdlRect; // SDL 矩形，定义纹理在窗口上的显示区域 FILE *fp_yuv; // 用于输出YUV文件时的文件指针 // FFmpeg 初始化 av_register_all(); // 注册所有可用的编解码器、复用器/解复用器等组件 avformat_network_init(); // 初始化网络模块，如果需要处理网络流（例如HTTP, RTSP等） pFormatCtx = avformat_alloc_context(); // 分配一个AVFormatContext结构体 // 打开输入流（文件或网络流） // avformat_open_input 负责打开媒体文件并读取其头部信息 if (avformat_open_input(&amp;pFormatCtx, filepath, NULL, NULL) != 0) &#123; printf(&quot;Couldn&#x27;t open input stream.\\n&quot;); // 如果文件无法打开，打印错误信息 return -1; &#125; // 查找输入流信息 // avformat_find_stream_info 负责读取媒体文件的一部分，填充pFormatCtx中的流信息（nb_streams, streams等） if (avformat_find_stream_info(pFormatCtx, NULL) &lt; 0) &#123; printf(&quot;Couldn&#x27;t find stream information.\\n&quot;); // 如果无法找到流信息，打印错误信息 return -1; &#125; // 找到视频流的索引 videoindex = -1; // 视频流索引初始化为-1（未找到） for (i = 0; i &lt; pFormatCtx-&gt;nb_streams; i++) &#123; // 遍历所有流 // 检查当前流的类型是否为视频流 if (pFormatCtx-&gt;streams[i]-&gt;codec-&gt;codec_type == AVMEDIA_TYPE_VIDEO) &#123; videoindex = i; // 找到视频流，记录其索引 break; // 退出循环 &#125; &#125; if (videoindex == -1) &#123; printf(&quot;Didn&#x27;t find a video stream.\\n&quot;); // 如果没有找到视频流，打印错误信息 return -1; &#125; // 获取视频流的编解码器上下文 pCodecCtx = pFormatCtx-&gt;streams[videoindex]-&gt;codec; // 从视频流中获取其对应的AVCodecContext // 找到视频解码器 // avcodec_find_decoder 根据编解码器ID（pCodecCtx-&gt;codec_id）找到对应的解码器 pCodec = avcodec_find_decoder(pCodecCtx-&gt;codec_id); if (pCodec == NULL) &#123; printf(&quot;Codec not found.\\n&quot;); // 如果没有找到解码器，打印错误信息 return -1; &#125; // 打开解码器 // avcodec_open2 负责打开解码器，并为它分配必要的资源 if (avcodec_open2(pCodecCtx, pCodec, NULL) &lt; 0) &#123; printf(&quot;Could not open codec.\\n&quot;); // 如果无法打开解码器，打印错误信息 return -1; &#125; // FFmpeg 内存分配 pFrame = av_frame_alloc(); // 分配一个AVFrame结构体，用于存储解码后的原始帧数据 pFrameYUV = av_frame_alloc(); // 分配一个AVFrame结构体，用于存储转换为YUV420P格式后的帧数据 // 计算YUV420P格式的图像数据所需的大小，并分配缓冲区 out_buffer = (uint8_t *)av_malloc(avpicture_get_size(PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height)); // 将分配的缓冲区与pFrameYUV关联起来，avpicture_fill会设置pFrameYUV-&gt;data和pFrameYUV-&gt;linesize // 注意：在FFmpeg 4.0+版本中，PIX_FMT_YUV420P 已被 AV_PIX_FMT_YUV420P 取代 avpicture_fill((AVPicture *)pFrameYUV, out_buffer, PIX_FMT_YUV420P, pCodecCtx-&gt;width, pCodecCtx-&gt;height); packet = (AVPacket *)av_malloc(sizeof(AVPacket)); // 分配一个AVPacket结构体，用于存储从文件中读取的压缩数据包 // 输出文件信息 printf(&quot;--------------- File Information ----------------\\n&quot;); av_dump_format(pFormatCtx, 0, filepath, 0); // 打印媒体文件的详细信息到控制台 printf(&quot;-------------------------------------------------\\n&quot;); // 初始化SWS_Scaler上下文，用于图像格式转换 // 从解码器输出的原始像素格式（pCodecCtx-&gt;pix_fmt）转换为YUV420P格式 // SWS_BICUBIC 是一个常用的缩放算法 img_convert_ctx = sws_getContext(pCodecCtx-&gt;width, pCodecCtx-&gt;height, pCodecCtx-&gt;pix_fmt, pCodecCtx-&gt;width, pCodecCtx-&gt;height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL);// 如果定义了OUTPUT_YUV420P宏，则打开YUV文件#if OUTPUT_YUV420P fp_yuv = fopen(&quot;output.yuv&quot;, &quot;wb+&quot;); // 以二进制写模式打开output.yuv文件#endif // SDL 初始化 // 初始化SDL视频、音频和定时器子系统 if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) &#123; printf(&quot;Could not initialize SDL - %s\\n&quot;, SDL_GetError()); // 初始化失败，打印错误信息 return -1; &#125; screen_w = pCodecCtx-&gt;width; // 设置窗口宽度为视频宽度 screen_h = pCodecCtx-&gt;height; // 设置窗口高度为视频高度 // 创建SDL窗口 // SDL_WINDOWPOS_UNDEFINED 表示窗口位置由系统决定 // SDL_WINDOW_OPENGL 提示使用OpenGL渲染 screen = SDL_CreateWindow(&quot;Simplest ffmpeg player&#x27;s Window&quot;, SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, screen_w, screen_h, SDL_WINDOW_OPENGL); if (!screen) &#123; printf(&quot;SDL: could not create window - exiting:%s\\n&quot;, SDL_GetError()); // 窗口创建失败，打印错误信息 return -1; &#125; // 创建SDL渲染器 sdlRenderer = SDL_CreateRenderer(screen, -1, 0); // 创建SDL纹理 // SDL_PIXELFORMAT_IYUV 指定纹理的像素格式为IYUV (即YUV420P) // SDL_TEXTUREACCESS_STREAMING 表示纹理数据会频繁更新 sdlTexture = SDL_CreateTexture(sdlRenderer, SDL_PIXELFORMAT_IYUV, SDL_TEXTUREACCESS_STREAMING, pCodecCtx-&gt;width, pCodecCtx-&gt;height); // 设置SDL矩形，定义纹理在窗口上的显示区域 sdlRect.x = 0; sdlRect.y = 0; sdlRect.w = screen_w; sdlRect.h = screen_h; // SDL 初始化结束---------------------- // 主循环：读取、解码和显示视频帧 // av_read_frame 从输入流中读取一个AVPacket while (av_read_frame(pFormatCtx, packet) &gt;= 0) &#123; // 检查当前数据包是否属于视频流 if (packet-&gt;stream_index == videoindex) &#123; // 解码视频包 // avcodec_decode_video2 将压缩的AVPacket解码为AVFrame（原始帧） // got_picture 会在成功解码出完整图像时设置为非0 ret = avcodec_decode_video2(pCodecCtx, pFrame, &amp;got_picture, packet); if (ret &lt; 0) &#123; printf(&quot;Decode Error.\\n&quot;); // 解码错误 return -1; &#125; if (got_picture) &#123; // 如果成功解码出了一张完整的图片 // 图像格式转换 // sws_scale 将pFrame中的图像数据转换为YUV420P格式，并存储到pFrameYUV中 // pFrame-&gt;data 和 pFrame-&gt;linesize 包含原始帧的像素数据和行步长 // 0 表示从图像的第0行开始转换 // pCodecCtx-&gt;height 是图像的高度 // pFrameYUV-&gt;data 和 pFrameYUV-&gt;linesize 接收转换后的YUV数据和行步长 sws_scale(img_convert_ctx, (const uint8_t* const*)pFrame-&gt;data, pFrame-&gt;linesize, 0, pCodecCtx-&gt;height, pFrameYUV-&gt;data, pFrameYUV-&gt;linesize); // 如果定义了OUTPUT_YUV420P宏，则将YUV数据写入文件 #if OUTPUT_YUV420P y_size = pCodecCtx-&gt;width * pCodecCtx-&gt;height; // Y分量大小 fwrite(pFrameYUV-&gt;data[0], 1, y_size, fp_yuv); // 写入Y分量数据 fwrite(pFrameYUV-&gt;data[1], 1, y_size / 4, fp_yuv); // 写入U分量数据 (YUV420P中U/V是Y的1/4大小) fwrite(pFrameYUV-&gt;data[2], 1, y_size / 4, fp_yuv); // 写入V分量数据 #endif // SDL 渲染部分--------------------------- #if 0 // 这是一个旧的SDL_UpdateTexture用法，通常用于YUV数据的更新 SDL_UpdateTexture( sdlTexture, NULL, pFrameYUV-&gt;data[0], pFrameYUV-&gt;linesize[0] ); #else // 使用SDL_UpdateYUVTexture更新YUV纹理，更适合YUV420P三平面格式 SDL_UpdateYUVTexture(sdlTexture, &amp;sdlRect, pFrameYUV-&gt;data[0], pFrameYUV-&gt;linesize[0], // Y分量数据和行步长 pFrameYUV-&gt;data[1], pFrameYUV-&gt;linesize[1], // U分量数据和行步长 pFrameYUV-&gt;data[2], pFrameYUV-&gt;linesize[2]); // V分量数据和行步长 #endif SDL_RenderClear( sdlRenderer ); // 清空渲染器 // 将纹理复制到渲染器，NULL表示复制整个纹理到整个sdlRect定义的区域 SDL_RenderCopy( sdlRenderer, sdlTexture, NULL, &amp;sdlRect); SDL_RenderPresent( sdlRenderer ); // 更新屏幕显示 // SDL 渲染部分结束----------------------- // 延时以控制播放速度，这里简单固定延时40ms，约每秒25帧（1000ms / 40ms = 25帧） SDL_Delay(40); &#125; &#125; av_free_packet(packet); // 释放当前AVPacket的内存，准备读取下一个 &#125; // 刷新解码器（处理解码器中剩余的帧） // 在文件末尾，解码器内部可能还缓存了一些帧，需要通过不断调用解码函数直到没有更多帧输出 // 此时packet参数可以设置为NULL或一个空packet while (1) &#123; ret = avcodec_decode_video2(pCodecCtx, pFrame, &amp;got_picture, NULL); // 注意这里 packet 为 NULL if (ret &lt; 0) // 解码错误或没有更多数据 break; if (!got_picture) // 没有解码出新的图像 break; // 对剩余的帧进行格式转换和显示 sws_scale(img_convert_ctx, (const uint8_t* const*)pFrame-&gt;data, pFrame-&gt;linesize, 0, pCodecCtx-&gt;height, pFrameYUV-&gt;data, pFrameYUV-&gt;linesize); #if OUTPUT_YUV420P int y_size=pCodecCtx-&gt;width*pCodecCtx-&gt;height; // 重新计算y_size，虽然这里应该和之前一样 fwrite(pFrameYUV-&gt;data[0],1,y_size,fp_yuv); // Y fwrite(pFrameYUV-&gt;data[1],1,y_size/4,fp_yuv); // U fwrite(pFrameYUV-&gt;data[2],1,y_size/4,fp_yuv); // V #endif // SDL 显示剩余帧 SDL_UpdateTexture( sdlTexture, &amp;sdlRect, pFrameYUV-&gt;data[0], pFrameYUV-&gt;linesize[0] ); // 修正，这里应该用 SDL_UpdateYUVTexture SDL_RenderClear( sdlRenderer ); SDL_RenderCopy( sdlRenderer, sdlTexture, NULL, &amp;sdlRect); SDL_RenderPresent( sdlRenderer ); SDL_Delay(40); &#125; // 释放资源 sws_freeContext(img_convert_ctx); // 释放SWS_Scaler上下文#if OUTPUT_YUV420P fclose(fp_yuv); // 关闭YUV输出文件#endif SDL_Quit(); // 退出SDL子系统 av_frame_free(&amp;pFrameYUV); // 释放YUV帧的内存 av_frame_free(&amp;pFrame); // 释放原始帧的内存 avcodec_close(pCodecCtx); // 关闭编解码器上下文 avformat_close_input(&amp;pFormatCtx); // 关闭输入流上下文 return 0; // 程序成功退出&#125;","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"FFmpeg","slug":"FFmpeg","permalink":"https://yaheii.github.io/tags/FFmpeg/"}],"author":null},{"title":"CMakeLists中的常见字段","slug":"CMakeLists常见字段","date":"2025-06-06T16:00:00.000Z","updated":"2025-07-03T11:45:37.655Z","comments":true,"path":"2025/06/07/CMakeLists常见字段/","permalink":"https://yaheii.github.io/2025/06/07/CMakeLists%E5%B8%B8%E8%A7%81%E5%AD%97%E6%AE%B5/","excerpt":"","text":"CMakeLists.txt 中的核心是一系列的命令（commands）和变量（variables）。通过这些命令，我们向 CMake 声明项目的各种特性和构建规则。 以下是一些 CMakeLists.txt 中最常见和重要的“字段”（或者说，命令和变量）： 核心项目配置命令 cmake_minimum_required(VERSION &lt;major&gt;.&lt;minor&gt; [FATAL_ERROR]) 作用： 指定项目所需的最低 CMake 版本。这是每个 CMakeLists.txt 文件的第一行。它确保你的项目不会在旧版本的 CMake 上编译，旧版本可能不支持你使用的某些命令或特性。 示例： cmake_minimum_required(VERSION 3.10) project(&lt;PROJECT_NAME&gt; [LANGUAGES &lt;language&gt;...] [VERSION &lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;[.&lt;tweak&gt;]]]]) 作用： 定义项目的名称。这是顶级 CMakeLists.txt 中仅次于 cmake_minimum_required 的第二条命令。它也可能指定项目支持的语言（如 CXX, C, Fortran）和版本。 示例： project(simplest_ffmpeg_player CXX VERSION 1.0) 源文件、目标和依赖管理 add_executable(&lt;target_name&gt; [source1] [source2] ...) 作用： 定义一个可执行目标。这是最常见的命令之一，它告诉 CMake 将指定的源文件编译并链接成一个可执行程序。 示例： add_executable(my_app main.cpp helper.cpp) add_library(&lt;target_name&gt; [STATIC | SHARED | MODULE] [source1] [source2] ...) 作用： 定义一个库目标。你可以指定它是静态库 (STATIC)、动态库 (SHARED) 还是模块库 (MODULE)。 示例： add_library(my_static_lib STATIC static_func.cpp) add_library(my_shared_lib SHARED shared_func.cpp) target_sources(&lt;target&gt; [PRIVATE|PUBLIC|INTERFACE] &lt;source1&gt; [source2] ...) 作用： 向一个已存在的 target 添加源文件。当源文件很多或需要根据条件添加时，这个命令比直接在 add_executable&#x2F;add_library 中列出更灵活。 示例： target_sources(my_app PRIVATE main.cpp) target_include_directories(&lt;target&gt; [PRIVATE|PUBLIC|INTERFACE] [dir1] [dir2] ...) 作用： 指定某个目标（可执行文件或库）的头文件搜索路径。 PRIVATE：只影响当前目标本身的编译。 PUBLIC：影响当前目标本身的编译，也影响链接到此目标的任何其他目标。 INTERFACE：只影响链接到此目标的任何其他目标。 示例： target_include_directories(my_app PUBLIC $&#123;CMAKE_SOURCE_DIR&#125;/include) target_link_libraries(&lt;target&gt; [PRIVATE|PUBLIC|INTERFACE] [item1] [item2] ...) 作用： 指定某个目标需要链接的库。这可以是其他 CMake 目标，也可以是外部库。链接顺序在某些情况下很重要。 示例： target_link_libraries(my_app PRIVATE my_shared_lib Qt5::Widgets) add_subdirectory(&lt;source_dir&gt; [binary_dir] [EXCLUDE_FROM_ALL]) 作用： 包含另一个子目录中的 CMakeLists.txt 文件。这对于组织大型项目结构非常有用。 示例： add_subdirectory(src) 变量和属性设置 set(&lt;variable&gt; &lt;value&gt; [CACHE &lt;type&gt; &lt;docstring&gt; [FORCE]]) 作用： 设置 CMake 变量的值。变量在 CMake 脚本内部使用，可以存储路径、选项等。CACHE 选项用于创建用户可以在 CMake GUI 或命令行中配置的缓存变量。 示例： set(SOURCE_FILES main.cpp) set(CMAKE_CXX_STANDARD 17) (设置C++标准，这是一个重要的内置变量) set(BUILD_SHARED_LIBS ON CACHE BOOL &quot;Build shared libraries&quot; FORCE) option(&lt;variable&gt; &quot;Help string&quot; [initial value]) 作用： 创建一个布尔选项，用户可以在 CMake 配置时启用或禁用。 示例： option(BUILD_TESTS &quot;Build unit tests&quot; ON) 查找包和依赖 find_package(&lt;PackageName&gt; [version] [REQUIRED] [COMPONENTS &lt;comp1&gt; &lt;comp2&gt;...] [OPTIONAL_COMPONENTS &lt;comp3&gt;...] [NO_MODULE] [NO_CONFIG]) 作用： 查找并加载外部依赖包（如 Boost, OpenCV, Qt 等）。如果找到，它会设置一些变量（如 &lt;PackageName&gt;_FOUND，&lt;PackageName&gt;_INCLUDE_DIRS，&lt;PackageName&gt;_LIBRARIES）或导入目标（如 Qt5::Widgets）。 示例： find_package(SDL2 REQUIRED) 条件和循环控制流 if (...) ... elseif (...) ... else (...) ... endif() 作用： 条件语句，根据条件执行不同的命令。 示例： 12345if(WIN32) message(&quot;Building on Windows&quot;)elseif(UNIX) message(&quot;Building on Unix-like system&quot;)endif() foreach(&lt;loop_var&gt; &lt;item1&gt; [&lt;item2&gt; ...] ) ... endforeach() 作用： 循环遍历列表。 示例： 1234set(MY_SOURCES a.cpp b.cpp c.cpp)foreach(src_file $&#123;MY_SOURCES&#125;) message(&quot;Processing $&#123;src_file&#125;&quot;)endforeach() 其他常用命令 message([STATUS|WARNING|AUTHOR_WARNING|FATAL_ERROR|CHECK_START|CHECK_DONE|CHECK_FAIL] &quot;message to display&quot;) 作用： 在 CMake 配置过程中输出信息到控制台。对于调试和用户提示非常有用。 示例： message(STATUS &quot;Configuring $&#123;PROJECT_NAME&#125; project...&quot;) file(GLOB &lt;variable&gt; [LIST_DIRECTORIES true|false] [RELATIVE &lt;path&gt;] [pattern1] [pattern2] ...) 作用： 查找匹配给定模式的文件，并将结果存储到变量中。常用于收集源文件。 示例： file(GLOB SOURCE_FILES &quot;src/*.cpp&quot; &quot;src/*.c&quot;) 注意： 虽然方便，但 file(GLOB) 在某些情况下可能不是最佳实践，因为它不处理文件删除的情况（需要重新运行 CMake）。更推荐显式列出源文件或使用 target_sources。 install(...) 作用： 定义项目的安装规则，例如将可执行文件、库、头文件安装到系统目录或指定目录。 示例： 12install(TARGETS my_app DESTINATION bin)install(DIRECTORY include/ DESTINATION include)","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"编程基础","slug":"编程基础","permalink":"https://yaheii.github.io/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"},{"name":"CMake","slug":"CMake","permalink":"https://yaheii.github.io/tags/CMake/"}],"author":null},{"title":"音视频传输","slug":"音视频","date":"2025-05-31T16:00:00.000Z","updated":"2025-07-23T13:36:56.844Z","comments":true,"path":"2025/06/01/音视频/","permalink":"https://yaheii.github.io/2025/06/01/%E9%9F%B3%E8%A7%86%E9%A2%91/","excerpt":"","text":"视频播放器原理视频播放器拓扑结构 1234567解协议的作用，就是将流媒体协议的数据，解析为标准的相应的封装格式数据。视音频在网络上传播的时候，常常采用各种流媒体协议，例如HTTP，RTMP，或是MMS等等。这些协议在传输视音频数据的同时，也会传输一些信令数据。这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。解协议的过程中会去除掉信令数据而只保留视音频数据。例如，采用RTMP协议传输的数据，经过解协议操作后，输出FLV格式的数据。解封装的作用，就是将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。封装格式种类很多，例如MP4，MKV，RMVB，TS，FLV，AVI等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。例如，FLV格式的数据，经过解封装操作后，输出H.264编码的视频码流和AAC编码的音频码流。解码的作用，就是将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据。音频的压缩编码标准包含AAC，MP3，AC-3等等，视频的压缩编码标准则包含H.264，MPEG2，VC-1等等。解码是整个系统中最重要也是最复杂的一个环节。通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如YUV420P，RGB等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如PCM数据。视音频同步的作用，就是根据解封装模块处理过程中获取到的参数信息，同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。 流媒体协议 封装格式 视频编码 音频编码 名称 推出机构 推出时间 目前使用领域 LDAC Sony Corporation 2015 高分辨率无线音频传输，主要应用于索尼耳机&#x2F;播放器与安卓设备之间（需要设备支持） 关于LDAC的补充说明： 高分辨率音频传输： LDAC是索尼开发的一种音频编码技术，旨在通过蓝牙连接传输高分辨率（Hi-Res Audio）音频。它能够传输比传统蓝牙编解码器（如SBC）更高码率的数据，从而在无线传输中保留更多的音频细节。 码率： LDAC支持多种传输码率，最高可达990 kbps（在最佳连接条件下），这远高于SBC（最高约328 kbps）和aptX HD（576 kbps）。 应用场景： 主要用于索尼自家的音频产品（如WH-1000XM系列耳机、Walkman播放器）以及支持LDAC的安卓智能手机。它是安卓8.0（Oreo）及更高版本系统中的一个标准蓝牙音频编解码器。 局限性： 尽管LDAC能够传输高码率音频，但其传输质量受限于蓝牙连接的稳定性。在复杂的无线环境下，码率可能会自适应下降以维持连接。此外，其使用范围主要集中在索尼和安卓生态系统内，苹果iOS设备目前不支持LDAC。 与传统编码器的区别： 与你表格中列出的AAC、MP3、AC-3、WMA等主要用于音频文件存储和流媒体分发的编码器不同，LDAC更侧重于无线传输过程中的高质量编码，尤其是在蓝牙这个带宽有限的载体上。AAC、MP3等通常是音频文件的格式，而LDAC是传输协议的一部分，用于将这些格式的音频数据通过蓝牙高效传输。 上面介绍了常见的协议、封装、编码。下面举其中经典的例子来做原理说明 协议之RTSP封装之FLV flv封装由文件头和文件体组成，其中body部分由多个previous tag size四个字节，要来标志前一个tag的字节数据长度，第一个PTS为0 + tag有三种类型，包括音频、视频、和脚本类型.其中同样包括header和data。header一般为11字节组成。 具体如下： UI表示无符号整形，后面跟的数字表示其长度是多少位。比如UI8，表示无法整形，长度一个字节。UI24是三个字节,UI[8*n]表示多个字节。UB表示位域，UB5表示一个字节的5位。可以参考c中的位域结构体。 tag部分详解 脚本类型tag这里会存放一些相关与FLV视频和音频的元信息。比如:duration，width,heigth 等。通常该类型Tag会作为FLV文件的第一个tag，并且只有一个，跟在File Header后。该类型Tag DaTa的结构如下所示:第一个AMF包：第1个字节表示AMF包类型，一般总是0x02，表示字符串。第2-3个字节为UI16类型值，标识字符串的长度，一般总是0x000A（“onMetaData”长度）。后面字节为具体的字符串，一般总为“onMetaData”（6F,6E,4D,65,74,61,44,61,74,61）。 第二个AMF包：第1个字节表示AMF包类型，一般总是0x08，表示数组。第2-5个字节为UI32类型值，表示数组元素的个数。后面即为各数组元素的封装，数组元素为元素名称和值组成的对。常见的数组元素如下表所示。 音频类型tag第一个字节是音频信息，第二个字节之后是音频信息。 视频类型tag第一个字节信息第二个字节之后是视频流 视频编码之H264视频帧内编码视频编码词用压缩技术来减少码率，而压缩的理论依据主要来源于： 数据冗余，通过关联图像中的各像素，来实现无损压缩 视觉冗余，在人眼的可分辨范围外通过引入客观失真来实现有损压缩。 变换编码：首先将源图像切割，然后对切割后的小块进行DCT变换，这个小块叫做宏块，在对图像块经过DCT变换后的系数进行量化，在传送过程中只传递一部分数据。实现有损压缩。实现有损压缩。 视频帧间编码采用运动估计和运动补偿的方法来实现，第一步还是实现图像分割，然后在前一图像或者后抑恶图像某个搜索窗口的范围内未每一一个图像块寻找最为相似的图像块，通过计算这两个图像块的变换关系得到运动矢量。将两个图像块相减得到残差图像。前一个过程叫做运动估计，后一个过程叫做运动补偿。在H264中为了提高视频压缩质量，引入I帧、P帧、B帧。 I帧只使用本帧内数据编码，不需要考虑消除时间序列相关性。P帧使用前面的I帧或P帧来做运动估计和补偿。B帧使用前面的一个I帧或P帧，或后面一个I帧或P帧；来进行预测。使用B帧可以实现高压缩比。但是如果P帧和参考B帧遭到破坏，其他所有依赖于它们的帧就不能完整解码，这会直接导致视频故障。视频通常无法从此类问题中恢复。然而，当被破坏的视频流到达I帧，因为I帧被独立地编码解码，所以视频问题可以从I帧恢复。 一个序列的第一个图像叫做IDR 图像（立即刷新图像），IDR 图像都是 I 帧图像。引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码。 一个序列就是一段内容差异不太大的图像编码后生成的一串数据流。当运动变化比较少时，一个序列可以很长，因为运动变化少就代表图像画面的内容变动很小，所以就可以编一个I帧，然后一直P帧、B帧了。当运动变化多时，可能一个序列就比较短了，比如就包含一个I帧和3、4个P帧。 在视频编码序列中，GOP即Group of picture（图像组），指两个I帧之间的距离，Reference（参考周期）指两个P帧之间的距离。两个I帧之间形成一组图片，就是GOP。 编码器算法视频编码器能够自主的比较帧内预测和帧间预测的结果，选择出最佳结果，即模式选择。并且编码器应该对每个宏块能做出如下处理： 后向预测（使用未来的帧） 前向预测（使用过去的帧） 无帧间预测，仅帧内预测 完全跳过（帧内或帧间预测） 下面重点解释一下B帧预测的逻辑。 我们用一个简化的模型来描述 B 帧的预测过程。假设我们有一个 GOP 结构，例如：I B B P B B I。 当编码器处理某个 B 帧时，例如，在 P1 和 P2 之间的 B1 帧：P1 ---&gt; B1 &lt;--- P2 B帧的预测通常涉及以下步骤： 确定参考帧列表 (Reference Picture Lists)： 每个 B 帧在编码时会维护两个参考帧列表： List 0 (L0)： 包含在其显示时间戳之前的参考帧（通常是 I 或 P 帧）。 List 1 (L1)： 包含在其显示时间戳之后的参考帧（通常是 I 或 P 帧）。 这些参考帧可以是比当前 B 帧更早或更晚解码的 I&#x2F;P 帧。例如，对于 B1，P1 位于 L0，P2 位于 L1。 宏块或子块级别预测：B帧的预测是以宏块（Macroblock，16x16 像素）或更小的子块为单位进行的。对于当前 B 帧中的一个宏块： a. 向前预测 (Forward Prediction)： 编码器在 List 0 中的参考帧（例如 P1）中搜索与当前宏块最相似的区域。 找到最相似的区域后，计算出**运动矢量 (Motion Vector, MV)**，这个 MV 指示了从参考帧中的哪个位置到当前宏块位置的位移。 记录下这个运动矢量和对应的预测残差（当前宏块与向前预测结果的差异）。 b. 向后预测 (Backward Prediction)： 编码器在 List 1 中的参考帧（例如 P2）中搜索与当前宏块最相似的区域。 计算出另一个运动矢量 (MV’)。 记录下这个运动矢量和对应的预测残差。 c. 双向预测 (Bi-directional Prediction)： 这是 B 帧特有的强大功能。编码器会尝试结合 向前预测的结果 和 向后预测的结果 来生成一个更准确的预测。 加权平均： 最常见的方法是对向前预测和向后预测的结果进行加权平均。例如，如果 B1 刚好位于 P1 和 P2 的中间，可能会对两个预测结果各取 50% 进行叠加。 选择更好的预测模式： 编码器会比较三种预测模式（向前、向后、双向）产生的预测残差大小，选择残差最小的模式。残差越小，说明预测越准确，需要编码的数据量就越少。 编码残差和运动信息： 无论选择哪种预测模式，B 帧最终编码的都是预测残差（当前宏块的实际像素值与预测结果之间的差异）以及用于预测的运动矢量和参考帧索引。 由于预测残差通常包含的能量非常小（因为预测得很准确），所以经过变换、量化和熵编码后，数据量会非常小。 举例说明 假设我们有三帧画面，编码顺序和显示顺序可能如下： 显示顺序： F1 (I) -&gt; F2 (B) -&gt; F3 (P) -&gt; F4 (B) -&gt; F5 (P) 解码顺序（为了先解码参考帧）： F1 (I) -&gt; F3 (P) -&gt; F2 (B) -&gt; F5 (P) -&gt; F4 (B) 我们聚焦在 F2 (B帧) 如何编码： 解码 F1 (I帧)： F1 是一个完整的独立帧，不依赖其他帧。 解码 F3 (P帧)： F3 依赖 F1 进行预测。编码器从 F1 中找到 F3 各个宏块的相似区域，记录下运动矢量和残差。 解码 F2 (B帧)： F2 知道它在显示顺序上介于 F1 和 F3 之间。 对于 F2 中的一个宏块： 向前预测： 编码器在 F1 中找一个最像的块，记录 MV。 向后预测： 编码器在 F3 中找一个最像的块，记录 MV’。 双向预测： 将 F1 的预测块和 F3 的预测块进行平均或加权平均，形成一个双向预测块。 编码器会比较这三种方式的预测残差大小。例如，如果 F2 上的一个物体是从 F1 运动到 F3 过程中的中间位置，那么双向预测往往能得到最小的残差。如果 F2 上的一个静止背景在 F1 和 F3 中都有，那么向前或向后预测就足够了。 最终，F2 编码并存储：哪个预测模式、哪个参考帧（L0或L1）、运动矢量、以及实际的预测残差。 H264解析同样的在H264中也有IBP帧类型，但是更重要的时其中的两层功能概念、NALU概念、slice、以及两种格式： 在H264中图像以序列为单位进行组织，一个序列是一段图像编码后的数据流。即NALU，它的功能分为两层，VCL（视频编码层）和NAL（网络提取层）。 两层功能 VCL包括核心压缩引擎和块，宏块和片的语法级别定义，设计目标是尽可能独立于网路进行高效的编码。具体步骤如下： 压缩：预测（帧内预测和帧间预测）-&gt; DCT 变化和量化 -&gt; 比特流编码； 切分数据，主要为了第三步。这里一点，网上看到的“切片（slice）”、“宏块（macroblock）”是在VCL 中的概念，一方面提高编码效率和降低误码率、另一方面提高网络传输的灵活性。 压缩切分后的 VCL 数据会包装成为 NAL 中的一部分。 NAL负责将VCL产生的比特字符串适配到各种各样的网络和多元环境中去。覆盖了所有片级以上语法。 NALUNALU &#x3D; header+Payloadheader一般有一个字节组成，如下图 forbidden位在网络发生i错误的时候会被置为1，告诉对方丢掉这个单元。 nal_ref_idc表示当前NALU的重要性，若值小，在解码器处理不过来的时候可以选择丢掉 nal_unit_type表示NALU类型 1-4：I&#x2F;P&#x2F;B帧，如果 nal_ref_idc 为 0，则表示 I 帧，不为 0 则为 P&#x2F;B 帧。 5：IDR帧，I 帧的一种，告诉解码器，之前依赖的解码参数集合（接下来要出现的 SPS\\PPS 等）可以被刷新了。 6：SEI，英文全称 Supplemental Enhancement Information，翻译为“补充增强信息”，提供了向视频码流中加入额外信息的方法。 7：SPS，全称 Sequence Paramater Set，翻译为“序列参数集”。SPS 中保存了一组编码视频序列（Coded Video Sequence）的全局参数。因此该类型保存的是和编码序列相关的参数。 8: PPS，全称 Picture Paramater Set，翻译为“图像参数集”。该类型保存了整体图像相关的参数。 9：AU 分隔符，AU 全称 Access Unit，它是一个或者多个 NALU 的集合，代表了一个完整的帧，有时候用于解码中的帧边界识别。 SPS 和 PPS 存储了编解码需要一些图像参数，SPS,PPS 需要在 I 帧前出现，不然解码器没法解码。而 SPS,PPS 出现的频率也跟不同应用场景有关，对于一个本地 h264 流，可能只要在第一个 I 帧前面出现一次就可以，但对于直播流，每个 I 帧前面都应该插入 sps 或 pps，因为直播时客户端进入的时间是不确定的。 对于Payload,H264也做了不同类型的规定 SODB，英文全称 String Of Data Bits，称原始数据比特流，就是最原始的编码&#x2F;压缩得到的数据。 RBSP英文全称 Raw Byte Sequence Payload，又称原始字节序列载荷。和 SODB 关系如下：RBSP &#x3D; SODB + RBSP Trailing Bits（RBSP尾部补齐字节），引入 RBSP Trailing Bits 做 8 位字节补齐。 EBSP英文全称 Encapsulated Byte Sequence Payload，称为扩展字节序列载荷。和 RBSP 关系如下： EBSP ：RBSP插入防竞争字节（0x03）这里说明下防止竞争字节（0x03）：可以先认为 H264 会插入一个叫做 StartCode 的字节串来分割 NALU，于是问题来了，如果 RBSP 中也包括了 StartCode（0x000001 或 0x00000001）怎么办呢？所以，就有了防止竞争字节（0x03）： sliceH264视频压缩后会成为一个序列帧，帧里包含图像，图像分为很多片，每个片可以分为宏块，每个宏块由许多子块组成 H264结构中，一个视频图像编码后的数据叫做一帧，一帧由一个片（slice）或多个片组成，一个片由一个或多个宏块（MB）组成，一个宏块由16x16的yuv数据组成。宏块作为H264编码的基本单位。 场和帧：视频的一场或者一帧可以用来产生一个编码图像。在电视中，每个电视帧都是通过扫描屏幕两次而产生的，第二个扫描的线条刚好填满第一次扫描所留下的缝隙。每个扫描即称为一个场。因此 30 帧&#x2F;秒的电视画面实际上为 60 场&#x2F;秒 片：每个图像中，若干个宏块被排列成片。片的目的：为了限制误码的扩散和传输，使编码片相互间保持独立。片共有5种类型：I片（只包含I宏块）、P片（P和I宏块）、B片（B和I宏块）、SP片（用于不同编码流之间的切换）和SI片（特殊类型的编码宏块）。NALU中承载的就是这些片 宏块：一个编码图像首先要划分成多个块（4x4 像素）才能进行处理，显然宏块应该是整数个块组成，通常宏块大小为16x16个像素。宏块分为I、P、B宏块，I宏块只能利用当前片中已解码的像素作为参考进行帧内预测；P宏块可以利用前面已解码的图像作为参考图像进行帧内预测；B宏块则是利用前后向的参考图形进行帧内预测 两种格式分别为字节流AnnexB格式 和 AVCC格式。前者用于实时播放，后者用于存储 AnnexB格式在这种格式中每个NALU都必须使用start code来分割（0x000001,单帧多个slice或0x00000001，帧之间，或者SPS、PPS等之前一般是四字节），并且SPS和PPS按流的方式写在头部。 NALU &#x3D; header+EBSP AVCC在这种格式中，每一个NALU包都加上了一个指定其长度(NALU包大小)的前缀(in big endian format大端格式)，这种格式的包非常容易解析，但是这种格式去掉了Annex B格式中的字节对齐特性，而且前缀可以是1、2或4字节，这让AVCC格式变得更复杂了，指定前缀字节数(1、2或4字节)的值保存在一个头部对象中(流开始的部分)，这个头通常称为’extradata’或者’sequence header’。 音频编码之AAC同样在音频方向也包括有损压缩和无损压缩。有损即去掉弱音信号或者去掉人耳听觉范围外的频率（&lt;20Hz||&gt;&#x3D;20KHz）. 关于音频的采样量化等方法以传统的PCM为例。 常见的无损方法： FLAC (Free Lossless Audio Codec): 目前最流行的无损音频编码格式，开源、免费。 APE (Monkey’s Audio): 另一种流行的无损格式，但解码复杂度较高。 ALAC (Apple Lossless Audio Codec): 苹果开发的无损格式，用于其生态系统。 WavPack (WV): 灵活的混合模式无损格式。 DSD&#x2F;DSF&#x2F;DFF: 用于高解析度音频的特殊无损格式，不完全是 PCM 编码。 有损方法较多，后面再详细学习，这里只聊一下AAC。 AAC编码文件格式文件有两种： ADIF：Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在磁盘文件中。 ADTS：Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。它的特征类似于mp3数据流格式。这种格式可以用于广播电视。 简言之。ADIF只有一个文件头，ADTS每个包前面有一个文件头。 对于AAC的头，一般为七个字节或者五个字节。具体头格式查表。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"}],"tags":[{"name":"音视频传输","slug":"音视频传输","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E4%BC%A0%E8%BE%93/"},{"name":"RTSP","slug":"RTSP","permalink":"https://yaheii.github.io/tags/RTSP/"},{"name":"音视频编解码","slug":"音视频编解码","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"}],"author":null},{"title":"MCP中的细节问题","slug":"MCP","date":"2025-05-22T16:00:00.000Z","updated":"2025-07-03T11:45:44.554Z","comments":true,"path":"2025/05/23/MCP/","permalink":"https://yaheii.github.io/2025/05/23/MCP/","excerpt":"","text":"MCP协议以及其中的客户端和服务器 首先附上up画的对我启发极大的一张图片。这张图片中讲了各大部件之间的关系。 从MCP协议为起点，MCP（Model Context Protocol），即模型上下文协议，正是这样一个旨在解决大型语言模型（LLM）与外部世界交互问题的开放标准。 MCP采用客户端-服务器（Client-Server）架构模式： Host（主机）&#x2F;LLM 应用： 代表大型语言模型应用程序，例如Claude Desktop、Cursor等，它们是发起连接的一方。 Client（客户端）： 在Host应用程序内部负责与MCP服务器建立连接，其实可以理解成Agent与MCP协议的集合。 MCP Server（MCP服务器）： 这是MCP系统中最关键的环节。它是一个程序，提供工具和数据访问能力供LLM使用。MCP服务器可以作为本地应用运行在用户设备上，也可以部署到远程服务器。每个MCP服务器都提供一组特定的工具（Tools）、资源（Resources）和提示（Prompts）： 12341. 工具（Tools）： 供AI模型调用的函数或操作，例如查询数据库、发送邮件、执行代码等。在面对一些没有提供程序接口的软件的时候，可以考虑写一些Py脚本或者命令行工具来实现我们的需求，例如打开项目文件，修改模型参数，运行仿真，提取仿真结果等。另外一种方式，时可以选择一些自动化的GUI库，这样的话就需要很多OCR的token了2. 资源（Resources）： 供用户或AI模型使用的上下文和数据，例如API回复、文件内容等。3. 提示（Prompts）： 用于完成特定任务的预定义提示模板。当LLM需要获取信息或执行操作时，它会通过MCP客户端向MCP服务器发送请求。MCP服务器会与相应的外部数据源或工具进行交互，获取数据并按照MCP协议规范进行格式化，最后将格式化后的数据返回给LLM。 回到上图，由于这个协议是用来规范Agent和各种tools之间的通信，调用格式规范的，由此可以将客户端的开发与服务器的开发分离开。因此开发可以分成三个大部分， 大模型部分的工作与RAG架构第一个部分是大模型部分，包括微调、如何设计一个专用模型，如何减少幻觉、目前的RAG架构如何进行优化等等工作。 简单介绍一下RAG架构 想象一下，LLM就像一个博览群书但只活在“过去”的人（因为它的知识截止于训练数据）。RAG给它配了一个“实时图书馆管理员”和一套“搜索工具”。 RAG核心思路：当用户提出一个问题时，RAG不是直接让LLM回答，而是分两步走： 检索 (Retrieval)： 从一个外部知识库（你的“实时图书馆”）中找出与用户问题最相关的几段信息。 增强生成 (Augmented Generation)： 将这些检索到的信息和用户的问题一起喂给LLM，让LLM基于这些“上下文”来生成答案。 RAG架构详解与举例：假设你正在研究一个新的相控阵天线设计，并且你有一个内部的技术文档库，里面包含了最新的天线设计规范、材料特性和仿真结果。 用户问题： “最新的MIMO相控阵天线设计中，氮化镓（GaN）材料的关键优势是什么？” 第一步：知识库准备 (提前进行)这是RAG的基础，你需要把你的“实时图书馆”整理好。 文档收集： 收集所有相关的技术文档、研究论文、设计规范等。 Chunking (分块)： 概念： 大文档被分割成更小的、有意义的文本片段，称为“Chunk”（块）。 目的： LLM的输入有长度限制（上下文窗口），而且太大的块会稀释关键信息。把文档切成小块，可以更精准地检索。 举例： 你的一篇关于GaN材料特性的长论文，会被切分成多个Chunk，例如： Chunk 1: “GaN在射频（RF）应用中的基本特性和历史…” Chunk 2: “GaN在MIMO相控阵天线中的功率密度和效率优势…” Chunk 3: “GaN与其他半导体材料（如GaAs）的对比…” Chunk 4: “GaN器件的散热挑战和解决方案…” 切分策略： 可以按句子、段落、固定长度（带重叠）等方式切分。对于技术文档，考虑语义完整性很重要。 Embedding (嵌入)： 概念： 将每个文本Chunk转换成一个高维的数字向量（一串数字），这个向量能够捕捉Chunk的语义信息。语义相似的Chunk，它们的Embedding向量在向量空间中也会靠得很近。 技术： 使用专门的Embedding模型（例如OpenAI的text-embedding-ada-002，或者开源的BERT、Sentence-BERT等模型）来完成。 举例： Chunk 1的Embedding向量：$[0.1, -0.5, 0.3, …, 0.8]$ Chunk 2的Embedding向量：$[0.15, -0.48, 0.32, …, 0.79]$ (与Chunk 1在向量空间中距离较近，因为都与GaN相关) Chunk 3的Embedding向量：$[0.6, 0.2, -0.1, …, 0.9]$ (与前两个距离较远，主题不同) 存储： 这些Embedding向量会被存储在一个向量数据库（Vector Database，如Pinecone, Weaviate, Milvus, ChromaDB等）中，以便快速检索。 第二步：实时查询 (当用户提问时) 用户问题Embedding： 当用户提出问题 “最新的MIMO相控阵天线设计中，氮化镓（GaN）材料的关键优势是什么？” 时，首先会使用与Chunk Embedding相同的Embedding模型将这个问题也转换成一个Embedding向量。 举例： 问题的Embedding向量：$[0.12, -0.51, 0.31, …, 0.81]$ 向量相似度搜索 (Retrieval)： 将用户问题的Embedding向量与向量数据库中所有Chunk的Embedding向量进行比较。 寻找语义上最相似的Chunk（即向量距离最近的Chunk）。 举例： 向量数据库会找出 Chunk 2 (“GaN在MIMO相控阵天线中的功率密度和效率优势…”) 和其他几个与GaN或MIMO相关的Chunk，因为它们的向量与问题向量距离最近。通常会检索Top-K个（比如Top-3或Top-5）最相关的Chunk。 增强生成 (Augmented Generation)： 将检索到的相关Chunk的原文内容（不是Embedding向量）和用户的问题一起打包成一个Prompt，发送给LLM。 Prompt结构示例： 123456789请根据以下提供的信息，回答用户的问题：---信息1：[Chunk 2 的原文内容] &quot;GaN在MIMO相控阵天线中的功率密度和效率优势使其成为关键材料。其高击穿电压和电子迁移率，使得GaN器件能够工作在更高的频率和功率水平，从而实现更紧凑、更高性能的天线模块...&quot;信息2：[Chunk 5 的原文内容] &quot;此外，GaN在高温下的稳定性也优于其他半导体材料，这对于MIMO天线在高功率运行时的散热设计至关重要...&quot;---用户问题：最新的MIMO相控阵天线设计中，氮化镓（GaN）材料的关键优势是什么？ LLM接收到这个“增强”后的Prompt，它现在不仅有自己原有的知识，还有了来自外部知识库的最新、最具体的信息。 LLM结合这些信息进行理解、推理和生成，给出更准确、更专业的答案。 LLM的最终回答： “在最新的MIMO相控阵天线设计中，氮化镓（GaN）材料的关键优势体现在其高功率密度、高效率以及在高温下的卓越稳定性。GaN的高击穿电压和电子迁移率使其能在更高频率和功率水平下运行，从而实现更紧凑、高性能的天线模块。此外，其优异的热稳定性对高功率MIMO天线的散热设计至关重要。” 但是就像我之前读过的一个知乎大佬所写的文章所说的，能不自己部署模型，就不自己部署。 自己部署的硬件成本和维护成本，对于小团队来说，很可能是压垮骆驼的一座大山。 Agent开发第二个是Agent的开发部分，也就是客户端部分，关于这部分已经写过一些内容了。 这里还应该注意的是，由于MCP协议，其实不需要关注agent输入与输出的设计，只需要根据不同的业务场景选择，Agent的输出究竟选择JSON格式还是选择使用Prompt格式即可，如何设计Prompt也就成了重点。 服务器部分第三个是服务器部分，未来会在这个方向做一些探索。 这里包含很多的部分，像Tools的设计，也就是如何根据业务场景写一个函数的库来供Agent进行调用。另外在写库函数时要格外注意docstr的书写，函数变量的命名，这里其实和Prompt设计有异曲同工之妙了。 最后，一些思考。关于整套架构，其中的工具设计是比较容易做到的（除了大模型部分的算法部分），但是如何为MCP的落地提供应用场景就成了难事，巨大的token耗费成本极大的限制了应用场景，如何落地呢？","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"}],"tags":[{"name":"MCP","slug":"MCP","permalink":"https://yaheii.github.io/tags/MCP/"},{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"}],"author":null},{"title":"C++中的STL标准库","slug":"STL_r1_output","date":"2025-05-04T16:00:00.000Z","updated":"2025-07-03T11:46:27.748Z","comments":true,"path":"2025/05/05/STL_r1_output/","permalink":"https://yaheii.github.io/2025/05/05/STL_r1_output/","excerpt":"","text":"C++ STL标准库源码解析与设计思想 最近在学习侯捷老师关于STL源码的解析（以GNU 2.9实现为例），本文记录核心知识点与设计哲学 一、STL六大组件全景图 STL主要分成了六个部分，其中包括容器，迭代器，分配器，算法，仿函数以及一些适配器。在学习STL之前应该首先对泛型编程以及对象编程有清晰的认识，对象编程倾向于设计一个Class类，实现一个具备自身数据和自身功能的一个整体，并提供复用的接口。泛型编程广泛的应用了模板的相关知识，通过模板参数T或者Foo来分别设计函数和数据的具体实现。对于不同版本的库文件，各家的编写方式也不尽相同，例如VC库文件和GNU库文件。同时由于标准规范存在，代码能够有较好的可移植性 STL（Standard Template Library）由以下六大核心组件构成： 容器（Containers）：管理数据的集合（如vector、list、map） 迭代器（Iterators）：泛化的指针，提供容器元素的访问接口 分配器（Allocators）：内存管理的底层实现（如std::alloc） 算法（Algorithms）：通用算法（如sort、find） 仿函数（Functors）：行为类似函数的对象（如less&lt;T&gt;） 适配器（Adapters）：组件接口转换器（如stack、queue） 容器主要分成关联式和序列式容器，对于序列是容器能够支持一些排序等操作。而关联式容器支持快速的查找工作在学习容器部分的时候应该格外关注他们的底层实现，这关乎于不同算法的实现效率，另外以及内存占用情况，以及容器内部定义的可以调用的一些基本函数。注意容器内部定义的一些函数和全局定义的函数的区别，例如find、sort等函数。 对于vector是使用三个指针进行控制，头部指针，内容尾部指针，以及内存尾部指针。vecotr的迭代器，不必设计成一个类。 deque是使用分段连续实现前后端均可以扩充对于deque的迭代器，使用四个属性进行控制。cur.first.last,node,deque的底层索引表也是通过vector来写的，进行二倍增长 stack 和queue的底层都是采用qeque来实现的。两者都不允许进行遍历，那么就是不提供迭代器 不同的容器其本身由于包括了多个迭代器，其指针等其本身就占用多个字节，例如deque占用40个字节，其内容要根据动态分配分配内存。需要注意的是，deque查找等动作，都需要首先检查指针是否已经在边界，进行索引表的步进，然后进行具体查找。 红黑树和散列表是关联式容器实现的关键。红黑树是一种平衡的二元搜寻树。不应该使用迭代器改变红黑树的元素值。因为红黑树内部有一定的排序规则。红黑树有多个参数，其中包括key，value，KeyOfValue,compare,alloc。红黑树内部有两种插入方式，包括insert_equal和insert_unique 二、容器实现深度解析2.1 序列式容器 容器 底层结构 关键特性 vector 动态数组 三指针控制：start, finish, end_of_storage deque 分段连续+索引表 迭代器含cur, first, last, node指针 list 双向链表 节点含prev, next, data指针 vector内存增长示例： 1234vector&lt;int&gt; v;v.push_back(1); // 容量1v.push_back(2); // 容量2（翻倍）v.push_back(3); // 容量4（再次翻倍） 2.2 关联式容器1234567graph TD A[关联式容器] --&gt; B[红黑树实现] A --&gt; C[哈希表实现] B --&gt; set/multiset B --&gt; map/multimap C --&gt; unordered_set C --&gt; unordered_map 红黑树关键特性： 每个节点非红即黑 根节点必须为黑 红色节点的子节点必须为黑 任意节点到叶子的路径包含相同数量黑节点 三、迭代器设计哲学3.1 迭代器核心接口12345678template&lt;class T&gt;struct iterator &#123; typedef T value_type; typedef ptrdiff_t difference_type; typedef T* pointer; typedef T&amp; reference; typedef random_access_iterator_tag iterator_category;&#125;; 迭代器是一种泛化的指针。要注意迭代器内部操作符重载的写法，例如++操作符、*、&amp;等重载的具体实现。另外以及iterator中的设计原则，iterator必须提供五种Type。这些参数在 &lt;iterator&gt; 头文件中通过 iterator 结构体定义，并在自定义迭代器时需要用到。这五个参数按顺序分别是： value_type: 迭代器所指向的元素的类型。通过迭代器，我们可以访问到容器中存储的元素，而 value_type 就定义了这些元素的类型。例如，对于 std::vector&lt;int&gt;::iterator，其 value_type 就是 int。 difference_type: 用于表示迭代器之间距离的类型。通常情况下，这个类型是带符号的整型，例如 std::ptrdiff_t。它可以用来计算两个迭代器之间的元素个数。例如，如果你有两个指向 std::vector&lt;int&gt; 中不同元素的迭代器 it1 和 it2，那么 it2 - it1 的结果类型就是 difference_type。 pointer: 指向 value_type 的指针类型。通常情况下，它是 value_type*。这个类型在某些迭代器（例如原始指针迭代器）中直接使用。 reference: 指向 value_type 的引用类型。通常情况下，它是 value_type&amp;。当我们通过解引用迭代器（使用 * 运算符）访问元素时，得到的就是一个 reference 类型的对象。 iterator_category: 描述迭代器所支持的操作的标签类型。STL 定义了五种主要的迭代器类别，它们之间存在着功能上的包含关系： std::input_iterator_tag: 只支持单向读取操作，即只能使用 *it 读取元素，并使用 ++it 使迭代器前进。输入迭代器通常用于单次遍历的输入流。 std::output_iterator_tag: 只支持单向写入操作，即只能使用 *it = value 写入元素，并使用 ++it 使迭代器前进。输出迭代器通常用于单次遍历的输出流。 std::forward_iterator_tag: 支持输入迭代器的所有操作，并且可以多次遍历容器中的元素。这意味着你可以保存一个前向迭代器的副本，并在之后再次使用它从相同的位置开始遍历。 std::bidirectional_iterator_tag: 支持前向迭代器的所有操作，并且可以双向移动，即可以使用 --it 使迭代器后退。std::list、std::set 和 std::map 等容器的迭代器通常是双向迭代器。 std::random_access_iterator_tag: 支持双向迭代器的所有操作，并且提供随机访问的能力。这意味着你可以像操作数组指针一样，使用 it + n、it - n、it[n] 以及比较运算符（&lt;、&gt;、&lt;=、&gt;=）在常数时间内访问任意位置的元素。std::vector、std::deque 和数组的迭代器都是随机访问迭代器。 3.2 迭代器分类与能力 迭代器类型 支持操作 典型容器 随机访问迭代器 ++, --, +n, -n, [] vector, deque 双向迭代器 ++, -- list, set&#x2F;map 前向迭代器 ++ forward_list 输入&#x2F;输出迭代器 单次遍历 istream, ostream 对于STL的容器等部件实际上是一个类模板，而算法实际上是一个函数的模板。另外函数一般会有多个重载方式，通过参数类型、数量等来区分。算法只会接收迭代器，看不到容器，所以它所需要的所有信息必须从迭代器中获得，而迭代器必须能够回答算法的所有问题。 四、算法与仿函数协作机制4.1 算法模板示例12345678template &lt;class InputIterator, class T&gt;InputIterator find(InputIterator first, InputIterator last, const T&amp; value) &#123; while (first != last &amp;&amp; *first != value) ++first; return first;&#125; 仿函数（functors），仿函数是通过设计一种类通过重载小括号来近似实现函数的功能，是为算法进行服务的，分别有算术类，逻辑运算类，以及相对关系类。例如针对自定义的一种类，来定义一种独特的排序方式。 再STL中规定了每个Adaptor都应该挑选一个适配者继承，因为在函数应用到仿函数的时候很有可能会询问仿函数一些基础参数问题，因此需要像迭代器那样，给它一个继承的身份。 4.2 仿函数与适配器算术仿函数示例： 123456template &lt;class T&gt;struct plus : binary_function&lt;T, T, T&gt; &#123; T operator()(const T&amp; x, const T&amp; y) const &#123; return x + y; &#125;&#125;; 适配器应用场景： 1234// 将普通函数转换为仿函数ptr_fun(my_function); // 绑定参数bind2nd(less&lt;int&gt;(), 40); 五、内存管理：分配器实现5.1 GNU 2.9 allocator设计12345678910class __malloc_alloc_template &#123; // 一级分配器 static void* allocate(size_t n) &#123; /* 直接调用malloc */ &#125;&#125;;class __default_alloc_template &#123; // 二级分配器 enum &#123; __ALIGN = 8 &#125;; static size_t ROUND_UP(size_t bytes) &#123; return (((bytes) + __ALIGN-1) &amp; ~(__ALIGN - 1)); &#125;&#125;; 5.2 内存池工作流程 维护16个自由链表（$8-128$字节） 内存不足时向系统申请大块内存 碎片回收通过自由链表管理 六、STL设计精髓总结 泛型编程思想：通过模板实现算法与数据类型的解耦 低耦合高内聚：容器、迭代器、算法通过标准接口协作 效率优先：通过内存池、红黑树等结构优化性能 可扩展性：允许用户自定义分配器、仿函数等组件","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"https://yaheii.github.io/tags/STL/"}],"author":null},{"title":"MiniAgi源码阅读笔记","slug":"MiniAgi 源码阅读","date":"2025-05-02T16:00:00.000Z","updated":"2025-07-02T02:58:30.399Z","comments":true,"path":"2025/05/03/MiniAgi 源码阅读/","permalink":"https://yaheii.github.io/2025/05/03/MiniAgi%20%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"spinner.py在这个文件中定义了一个光标效果 1234567def spinner_task(self): while self.busy: sys.stdout.write(next(self.spinner_generator)) sys.stdout.flush() #强制立即刷新 time.sleep(self.delay) # 定义帧率 sys.stdout.write(&#x27;\\b&#x27;) # 光标回退， 实现覆盖字符 sys.stdout.flush() 12345678910def __enter__(self): # 启动动画线程 self.busy = True threading.Thread(target=self.spinner_task).start()def __exit__(self, exception, value, tb): #停止动画 self.busy = False time.sleep(self.delay) if exception is not None: return False return True exceptions.py这个文件主要是用来捕获模型输出中的异常，例如格式不对，缺字段，非json等。然后将这个异常抛回给上层进行处理。继承了python内置的Exception类 1234567class InvalidLLMResponseError(Exception): &quot;&quot;&quot;Exception raised when the LLM response can&#x27;t be parsed. Attributes: None &quot;&quot;&quot; commond.py在这个文件中规定了不同命令的思考链 库文件说明 import subprocess 作用：用于创建子进程，执行外部命令或脚本，并获取其输入输出结果。 典型用途： 调用系统命令（如 ls, ping, gcc, python 等）； 执行外部程序或脚本（如 shell 脚本、批处理）； 控制输入输出重定向。 from io import StringIO 作用：提供一个类 StringIO，它创建一个内存中的字符串缓冲区，可像文件一样读写。 典型用途： 在不涉及实际文件的情况下模拟文件操作； 用于测试代码中涉及文件读写的部分； 捕捉输出文本（通常与 redirect_stdout 配合使用）。 from contextlib import redirect_stdout 作用：上下文管理器，用于临时将标准输出重定向到指定对象（比如 StringIO）。 典型用途： 捕捉 print() 语句的输出； 在测试或调试中查看函数内部输出； 搭配 StringIO 捕获控制台输出为字符串处理。 from duckduckgo_search import DDGS 作用：导入 DuckDuckGo Search 的 Python 接口 DDGS 类，用于调用 DuckDuckGo 的搜索功能。 典型用途： 用代码直接进行 DuckDuckGo 搜索并获取结果； 抓取网页搜索摘要、链接、图像等； 用于构建搜索引擎接口或信息爬取工具。 class类说明在execute_commond中首先将命令进行分发 12345678if command == &quot;memorize_thoughts&quot;: 调用 Commands.memorize_thoughts(arg)elif command == &quot;execute_python&quot;: 调用 Commands.execute_python(arg)...else: 返回 Unknown command&#123;commond&#125;如果过程中出现异常，返回异常信息 然后分别定义各个函数内容 miniagi.pyprompt设计关于prompt的具体设计原则，会写另外一篇博客进行说明在MiniAgi项目中prompt的设计是这样的设计中融入了思维链、少样本提示，并且强调了自我一致性。同时，在prompt的设计中加入了自我批评CRITIC_PROMPT，以及记忆重整HISTORY_SUMMARY_HINT等。具体如下 角色设定（Role Conditioning）1f&quot;You are an autonomous agent running on &#123;operating_system&#125;.&quot; 明确模型的“身份”：你是运行在某操作系统上的自主代理智能体；通过设定操作系统，有利于后续与 Shell 命令、文件路径等交互时保持一致（如 Linux vs Windows）。 目标导向任务（Objective Conditioning） 1OBJECTIVE: &#123;objective&#125; (e.g. &quot;Find a recipe for chocolate chip cookies&quot;) 为当前任务设定一个清晰目标，使模型行为聚焦； 模仿人类代理行为，把目标当成“长期任务”，以实现规划式思维。 历史上下文注入（Context）1Previous steps: &#123;context&#125; 引入以往已执行的动作&#x2F;观察结果，作为“记忆”或状态追踪； 支持多步推理和上下文保持，是链式思维（Chain-of-Thought）的关键。 明确命令集（命令语言接口设计） 定义一个有限状态机风格的 API 接口，明确智能体能执行的动作范围； 强制每一步只能执行一个命令，避免无控制的自然语言混乱； 命令覆盖典型任务：记忆、推理、代码执行、数据处理、与用户对话、终止。 明确格式约束（动作语法模板） 12&lt;r&gt;[YOUR_REASONING]&lt;/r&gt;&lt;c&gt;[COMMAND]&lt;/c&gt;[ARGUMENT] &lt;r&gt;：模型的推理或动机说明； &lt;c&gt;：结构化命令名称（对应预定义命令集）； [ARGUMENT]：命令参数； 该格式是构化动作标注，利于解析、监督、记录、回放等； 防止输出中出现自然语言噪声或模糊行为。 行为限制与安全提示 不要重复命令 不要链式多个命令 Python 要以 print 输出结尾 process_data &#x2F; ingest_data 只能处理单文件 不搜索 GPT 已知知识 保证行为唯一性、可解释性、可控性； 限制模型可能的幻觉或冗余行为； 强化对命令执行前提条件的检查（如输出格式约束）。 示例示范（Few-shot Prompting） 123&lt;r&gt;Think about skills and interests...&lt;/r&gt;&lt;c&gt;memorize_thoughts&lt;/c&gt;&lt;r&gt;Search for websites...&lt;/r&gt;&lt;c&gt;web_search&lt;/c&gt; 提供多个风格统一、结构良好的行为范例； 用作 few-shot learning 的提示模板，让模型模仿人类代理行为； 示例覆盖了多种命令使用方式、输入格式、常见应用。 MiniAgi类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216class MiniAGI: &quot;&quot;&quot; 表示一个自主智能体（Agent）类。 属性（Attributes）定义了 agent 的各种组件和运行状态。 &quot;&quot;&quot; def __init__( # 参数初始化列表 self, agent_model: str, summarizer_model: str, objective: str, max_context_size: int, max_memory_item_size: int, debug: bool = False ): &quot;&quot;&quot; 构造函数：创建 MiniAGI 实例，初始化内部模型和参数。 &quot;&quot;&quot; # 初始化用于生成行为的主模型 self.agent = ThinkGPT( model_name=agent_model, request_timeout=600, verbose=False ) # 初始化用于生成摘要的模型 self.summarizer = ThinkGPT( model_name=summarizer_model, request_timeout=600, verbose=False ) # 保存目标、内存限制、是否调试等配置参数 self.objective = objective self.max_context_size = max_context_size self.max_memory_item_size = max_memory_item_size self.debug = debug # 以下是状态相关的字符串属性，初始设为空 self.summarized_history = &quot;&quot; self.criticism = &quot;&quot; self.thought = &quot;&quot; self.proposed_command = &quot;&quot; self.proposed_arg = &quot;&quot; # 使用 tiktoken 获取模型的 tokenizer 编码器 self.encoding = tiktoken.encoding_for_model(self.agent.model_name) def __update_memory(self, action: str, observation: str, update_summary: bool = True): &quot;&quot;&quot; 内部方法：根据行动和观察结果更新记忆。 如果 observation 太长，会先进行摘要处理。 &quot;&quot;&quot; # 如果 observation 超过允许大小，进行摘要 if len(self.encoding.encode(observation)) &gt; self.max_memory_item_size: observation = self.summarizer.chunked_summarize( observation, self.max_memory_item_size, instruction_hint=OBSERVATION_SUMMARY_HINT ) # 构造新记忆格式 if &quot;memorize_thoughts&quot; in action: new_memory = f&quot;ACTION:\\nmemorize_thoughts\\nTHOUGHTS:\\n&#123;observation&#125;\\n&quot; else: new_memory = f&quot;ACTION:\\n&#123;action&#125;\\nRESULT:\\n&#123;observation&#125;\\n&quot; # 如果需要更新摘要，调用 summarizer 的 summarize 方法 if update_summary: self.summarized_history = self.summarizer.summarize( f&quot;Current summary:\\n&#123;self.summarized_history&#125;\\nAdd to summary:\\n&#123;new_memory&#125;&quot;, self.max_memory_item_size, instruction_hint=HISTORY_SUMMARY_HINT ) # 将新记忆交由 agent 存储 self.agent.memorize(new_memory) def __get_context(self) -&gt; str: &quot;&quot;&quot; 内部方法：构造 agent 当前的上下文字符串。 上下文包括摘要、最近的行为和批评。 &quot;&quot;&quot; summary_len = len(self.encoding.encode(self.summarized_history)) criticism_len = len(self.encoding.encode(self.criticism)) if self.criticism else 0 # 从 agent 中获取最多能容纳的最近记忆片段 action_buffer = &quot;\\n&quot;.join( self.agent.remember( limit=32, sort_by_order=True, max_tokens=self.max_context_size - summary_len - criticism_len ) ) # 构建最终上下文字符串 return f&quot;SUMMARY\\n&#123;self.summarized_history&#125;\\nPREV ACTIONS:\\n&#123;action_buffer&#125;\\n&#123;self.criticism&#125;&quot; def criticize(self) -&gt; str: &quot;&quot;&quot; 调用模型对 agent 最近的行为进行批评。 &quot;&quot;&quot; context = self.__get_context() self.criticism = self.agent.predict( prompt=CRITIC_PROMPT.format(context=context, objective=self.objective) ) return self.criticism def think(self): &quot;&quot;&quot; 调用模型进行推理，生成下一步操作计划。 &quot;&quot;&quot; context = self.__get_context() if self.debug: print(context) # 基于 prompt 和上下文生成原始响应 response_text = self.agent.predict( prompt=PROMPT.format(context=context, objective=self.objective) ) if self.debug: print(f&quot;RAW RESPONSE:\\n&#123;response_text&#125;&quot;) # 使用正则表达式提取 &lt;r&gt;思考&lt;/r&gt;&lt;c&gt;命令&lt;/c&gt;arg PATTERN = r&#x27;^&lt;r&gt;(.*?)&lt;/r&gt;&lt;c&gt;(.*?)&lt;/c&gt;\\n*(.*)$&#x27; try: match = re.search(PATTERN, response_text, flags=re.DOTALL | re.MULTILINE) _thought = match[1] _command = match[2] _arg = match[3] except Exception as exc: raise InvalidLLMResponseError from exc _arg = _arg.replace(&quot;```&quot;, &quot;&quot;) # 去除可能的 markdown 格式符号 # 保存模型推理结果 self.thought = _thought self.proposed_command = _command self.proposed_arg = _arg def read_mind(self) -&gt; tuple: &quot;&quot;&quot; 获取 agent 最近的思考、命令和参数。 &quot;&quot;&quot; _arg = self.proposed_arg.replace(&quot;\\n&quot;, &quot;\\\\n&quot;) if len(self.proposed_arg) &lt; 64\\ else f&quot;&#123;self.proposed_arg[:64]&#125;...&quot;.replace(&quot;\\n&quot;, &quot;\\\\n&quot;) return (self.thought, self.proposed_command, _arg) @staticmethod def __get_url_or_file(_arg: str) -&gt; str: &quot;&quot;&quot; 根据参数读取 URL 或本地文件内容。 &quot;&quot;&quot; if _arg.startswith(&quot;http://&quot;) or _arg.startswith(&quot;https://&quot;): with urlopen(_arg) as response: html = response.read() data = BeautifulSoup(html, features=&quot;lxml&quot;).get_text() else: with open(_arg, &quot;r&quot;) as file: data = file.read() return data def __process_data(self, _arg: str) -&gt; str: &quot;&quot;&quot; 对 URL 或文件进行处理，格式为：prompt|url或文件路径 &quot;&quot;&quot; args = _arg.split(&quot;|&quot;) if len(args) == 1: return &quot;Invalid command. The correct format is: prompt|file or url&quot; if len(args) &gt; 2: return &quot;Cannot process multiple input files or URLs. Process one at a time.&quot; prompt, __arg = args try: input_data = self.__get_url_or_file(__arg) except urllib.error.URLError as e: return f&quot;Error: &#123;str(e)&#125;&quot; except OSError as e: return f&quot;Error: &#123;str(e)&#125;&quot; if len(self.encoding.encode(input_data)) &gt; self.max_context_size: input_data = self.summarizer.chunked_summarize( input_data, self.max_context_size, instruction_hint=OBSERVATION_SUMMARY_HINT ) return self.agent.predict( prompt=f&quot;&#123;RETRIEVAL_PROMPT&#125;\\n&#123;prompt&#125;\\nINPUT DATA:\\n&#123;input_data&#125;&quot; ) def __ingest_data(self, _arg: str) -&gt; str: &quot;&quot;&quot; 只读取 URL 或文件内容（不进行指令解析），返回文本或摘要。 &quot;&quot;&quot; try: data = self.__get_url_or_file(_arg) except urllib.error.URLError as e: return f&quot;Error: &#123;str(e)&#125;&quot; except OSError as e: return f&quot;Error: &#123;str(e)&#125;&quot; if len(self.encoding.encode(data)) &gt; self.max_memory_item_size: data = self.summarizer.chunked_summarize( data","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"}],"tags":[{"name":"MCP","slug":"MCP","permalink":"https://yaheii.github.io/tags/MCP/"},{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"},{"name":"MiniAGI","slug":"MiniAGI","permalink":"https://yaheii.github.io/tags/MiniAGI/"}],"author":null},{"title":"Prompt设计（2）","slug":"prompt设计(2)","date":"2025-05-01T16:00:00.000Z","updated":"2025-07-02T03:05:10.792Z","comments":true,"path":"2025/05/02/prompt设计(2)/","permalink":"https://yaheii.github.io/2025/05/02/prompt%E8%AE%BE%E8%AE%A1(2)/","excerpt":"","text":"零样本提示提示 123将文本分类成中性、负面或正面文本：我认为这次假期还可以情感： 输出 1中性 指令调整已被证明可以改善零样本学习Wei等人（2022）。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，RLHF（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。这一最新发展推动了像ChatGPT这样的模型。我们将在接下来的章节中讨论所有这些方法和方法。 少样本提示即在prompt中给出一个实例，来帮助模型进行理解 1234这太棒了！// Negative这太糟糕了！// Positive哇，那部电影太棒了！// Positive多么可怕的节目！// 1Negative COT 关于零样本COT：即在后面加入”让我们逐步思考” 自动思维链（Auto-Cot），即利用 LLMs “让我们一步一步地思考” 提示来生成一个接一个的推理链。这种自动过程仍然可能在生成的链中出现错误。为了减轻错误的影响，演示的多样性很重要。 一般首先要先将任务分解成为不同的、连续的步骤 使用XML来构建清晰的交接 对每个任务要求构建清晰的目标 要根据模型的表现进行Prompt的迭代 自我一致性自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能。其实本质上是设计相同范式的问题与解答，帮助模型建立一致性 Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？ A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 &#x3D; 6棵树。答案是6。 Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？ A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 &#x3D; 5辆汽车。答案是5。 …. 当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？ TOTTOT本质上是将思考过程建模成一个多步，分支的思考树。每个节点都是一个思考过程。可以通过合理设计Prompt来实现TOT的结构。目前较为流行的TOT结构包括基于深度搜索、广度搜索等策略的。另一种基于强化学习训练出的TOT训练器。123456假设三位不同的专家来回答这个问题。所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。然后，所有专家都写下他们思考的下一个步骤并分享。以此类推，直到所有专家写完他们思考的所有步骤。只要大家发现有专家的步骤出错了，就让这位专家离开。请问...如何将TOT应用于模型的建构中是下一步要了解的方向自动推理并使用工具（ART）是在2023年提出的一个新的框架，这个框架使用冻结的LLM来自动生成包含中间推理步骤的程序 在接到新的任务的时候，从任务库中选择多部推理和使用工具的示范 在测试中，调用外部工具，先暂停生成，将工具整合后继续生成 就目前所了解到的，基本的Prompt设计主要还是基于普通的设计，或加入一些思维链提示链等。另外还有一部分为了提高泛化性能，使用了LLM来生成prompt进而生成答案，","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"},{"name":"Prompt","slug":"Prompt","permalink":"https://yaheii.github.io/tags/Prompt/"}],"author":null},{"title":"Prompt设计（1）","slug":"prompt设计","date":"2025-04-30T16:00:00.000Z","updated":"2025-07-02T03:05:13.650Z","comments":true,"path":"2025/05/01/prompt设计/","permalink":"https://yaheii.github.io/2025/05/01/prompt%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"prompt是用户输入给LLM的文本信息，是用来明确告知模型想要解决的问题，或者完成的任务。市面上已经有了一些关于prompt扩写、完善的工具，例如百炼。另外对prompt的设计实际上是一个迭代过程，可以通过openai等平台的playground进行大量的试验。在prompt中避免说不要做什么，而是应该要做什么。要非常具体地说明你希望模型执行的指令和任务。提示越具描述性和详细，结果越好。特别是当你对生成的结果或风格有要求时，这一点尤为重要。不存在什么特定的词元（tokens）或关键词（tokens）能确定带来更好的结果。更重要的是要有一个具有良好格式和描述性的提示词。事实上，在提示中提供示例对于获得特定格式的期望输出非常有效。 在设计提示时，还应注意提示的长度，因为提示的长度是有限制的。想一想你需要多么的具体和详细。包含太多不必要的细节不一定是好的方法。这些细节应该是相关的，并有助于完成手头的任务。这是你需要进行大量实验的事情。我们鼓励大量实验和迭代，以优化适用于你应用的提示。 基础结构较为通用的prompt结构如下： 输出：应该明确指出模型的输出内容的具体形式，确保LLM的输出能够满足后续的需求。 受众：需要明确指出面向的读者群体。以及适用的平台，在输出代码时可以有好的兼容性。 优化prompt 在prompt中提供期望的输出样例，可以让LLM模仿我们所要求的规范、格式、概念等要求进行输出。同时也能够使输出更加的统一。从而稳定模型表现。12345678910111213141516171819202122232425背景你很擅长编写小红书种草笔记，喜欢增加丰富的emoji元素。目的请生成一篇小红书种草笔记，推广强森吹风机。吹风机的优点是：体积小、高颜值、风力大、干得快、智能控温不伤发。受众喜欢追求时尚的年轻人，尤其是年轻女性输出小红书文章格式，充满emoji元素，简洁但内容充实语气与风格（提供了几种示例）我亲测过+n种好物+谁适合谁受益 这个秘诀让你的话语超有信服力！ 比如：亲自尝试了很多美白神器，终于挖到宝！仅俩月，肌肤变得嫩滑透亮，自我感觉飘飘欲仙~ 此法特为想大晒体验的小伙伴们量身定制，还能精准安利，助人避坑！难题出没+揭秘原因+终极解药 这公式助你条理清晰地分享，内容价值爆棚！ 案例：渴望秀发如丝？揭秘时刻来啦！原来我一直遗漏关键一步，直到遇见它！换用这款洗发水，秀发显著改善，光泽get！ 此法逻辑严密，不仅分享秘籍，还引导读者找到问题破解之道。独到见解+深度剖析+巧妙推荐 这公式帮你自然流露心声，还能温馨种草！ 示例：我觉得每个女孩都该有份挚爱，生活因此而精彩。手帐成了我的小确幸，每当提笔，幸福指数飙升！ 它助你畅所欲言，同时不经意间传递心头好，双赢策略！亲身经历+成果展示 这公式让你的情感表达鲜活又感人！ 场景：回想起夏夜海边的蚊灾，满身红包的绝望，直到遇见救星！现在，光滑肌肤让我裙摆飞扬，自信回归！ 它让你的故事活灵活现，分享喜悦与感恩之情，触动人心！ 而对于复杂任务，为LLM设定一个任务完成的步骤是十分重要的。（但如何设计一个泛化能力更强的任务步骤） 使用不常见的分隔符号来区分内容区域的界限标识。123在构建复杂的 Prompt 时，采用特定的分隔符来界定不同内容单元是极为关键的，这一做法显著增强了 LLM 对 Prompt 正确解析的能力。随着任务复杂度的增加，合理利用分隔符越能提升 LLM 的表现。分隔符的选择应着眼于那些在自然语言文本中罕见的、独特的字符组合，例如：###、===、&gt;&gt;&gt;等。这些特殊符号序列并无固定规则，关键在于其辨识度高，确保模型能够明确区分这些符号是作为内容区域的界限标识，而非文本中的普通标点或语法组成部分。 思维链和提示链我们可以通过要求输出整个的推理过程进行思维链，另外还有思维树，Boosting of thought等。 另：文生图prompt指南提示词 &#x3D; 主体（主体描述）+ 场景（场景描述）+ 风格（定义风格）+ 镜头语言 + 氛围词 + 细节修饰 1234567891011主体描述：确定主体清晰地描述图像中的主体，包括其特征、动作等。例如，“一个可爱的10岁中国小女孩，穿着红色衣服”。场景描述：场景描述是对主体所处环境特征细节的描述，可通过形容词或短句列举。定义风格：定义风格是明确地描述图像所应具有的特定艺术风格、表现手法或视觉特征。例如，“水彩风格”、“漫画风格”常见风格化详见下方提示词词典。镜头语言：镜头语言包含景别、视角等，常见镜头语言详见提示词词典。氛围词：氛围词是对预期画面氛围的描述，例如“梦幻”、“孤独”、“宏伟”，常见氛围词详见提示词词典。细节修饰：细节修饰是对画面进一步的精细化和优化，以增强图像的细节表现力、丰富度和美感。例如“光源的位置”、“道具搭配”、“环境细节”，“高分辨率”等。 另：文生视频prompt提示词 &#x3D; 主体 + 场景 + 运动 1234567主体：主体是视频内容的主要表现对象，可以是人、动物、植物、物品或非物理真实存在的想象物体。场景：场景是主体所处的环境，包含背景、前景，可以是物理存在的真实空间或想象出来的虚构场景。运动：运动包含主体的具体运动和非主体的运动状态，可以是静止、小幅度运动、大幅度运动、局部运动或整体动势。运镜描述： 运镜描述 + 主体（主体描述）+ 场景（场景描述）+ 运动（运动描述）+ 镜头语言 + 氛围词 + 风格化","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"},{"name":"Prompt","slug":"Prompt","permalink":"https://yaheii.github.io/tags/Prompt/"}],"author":null},{"title":"agent学习","slug":"agent","date":"2025-04-27T16:00:00.000Z","updated":"2025-07-03T08:57:51.705Z","comments":true,"path":"2025/04/28/agent/","permalink":"https://yaheii.github.io/2025/04/28/agent/","excerpt":"","text":"Agent工作逻辑以AutoGPT为例子，记录一下Agent的工作逻辑 1. 什么是像 AutoGPT 这样的 Agent 框架？它们是高级自动化系统，基本逻辑是： 不是简单「单轮提问-回答」 而是根据任务自己制定计划，分步行动，多轮决策，直到任务完成。普通的大模型是通过一轮轮的问答来实现最终的任务的但是创建一个合适的Agent可以实现自己想目标，自己想策略，自己执行，自己检查。 2. AutoGPT类 Agent 的运行框架核心步骤 它们基本遵循下面这个 循环逻辑： 12345678910111213141516171819202122232425262728293031323334353637(1) 接收目标用户给一个高层目标，比如：- &quot;写一份关于人工智能历史的详细报告，并生成成 Word 文档。&quot; (2) 自主规划Agent自己思考出**计划 (Plan)**，比如：- 查询人工智能历史资料- 按时间线整理事件- 写成条理清晰的段落- 格式化成Word文档 (3) 行动(Action)Agent根据计划，开始一步步执行：- 调用搜索引擎 API- 分析网页内容- 写文档- 保存文件每一步都是自己调用工具、处理结果！(4) 观察(Observation)每次行动后，会自己**检查行动结果**：- 成功了？继续下一步- 失败了？重新想方法- 信息不够？再去找资料(5) 决策(Thinking)根据观察结果，决定：- 修改计划- 补充信息- 结束任务**这就是所谓的：自主决策、自主行动循环。** 3. 它们内部通常包括哪些模块？ 模块 功能 Memory（记忆） 记录任务过程，避免忘记之前做过什么 Planning（规划） 自动分解任务成小步骤 Tools（工具链） 能用的外部接口（如Web搜索、文件系统、数据库等） Reasoning（推理） 分析当前状况，决定下一步怎么做 Critic（自我评估） 检查结果，判断是否需要修正 4. 模型的参数设置 Temparature简单来说，temperature 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。（调小temperature）实质上，你是在增加其他可能的 token 的权重。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 temperature 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 temperature 参数值可能会更好。 top_p 同样，使用 top_p（与 temperature 一起称为核采样（nucleus sampling）的技术），可以用来控制模型返回结果的确定性。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。 使用Top P意味着只有词元集合（tokens）中包含top_p概率质量的才会被考虑用于响应，因此较低的top_p值会选择最有信心的响应。这意味着较高的top_p值将使模型考虑更多可能的词语，包括不太可能的词语，从而导致更多样化的输出。 一般建议是改变 Temperature 和 Top P 其中一个参数就行，不用两个都调整。 MAX Length您可以通过调整 max length 来控制大模型生成的 token 数。指定 Max Length 有助于防止大模型生成冗长或不相关的响应并控制成本。 stop sequence这同样是一种控制模型响应长度和结构的另外一种方法 Frequency Penalty是对下一个生成的token进行惩罚，控制重复数量。 Presence Penaltypresence penalty 也是对重复的 token 施加惩罚，但与 frequency penalty 不同的是，惩罚对于所有重复 token 都是相同的。出现两次的 token 和出现 10 次的 token 会受到相同的惩罚。 此设置可防止模型在响应中过于频繁地生成重复的词。 如果您希望模型生成多样化或创造性的文本，您可以设置更高的 presence penalty，如果您希望模型生成更专注的内容，您可以设置更低的 presence penalty。 5. 具体以 AutoGPT 举例（运行时流程）12345678910111213141516171819202122231. 用户输入：我要了解马斯克的一生2. AutoGPT: - 想一想：需要做哪些事？ - 计划出步骤： ① 搜索马斯克的生平资料 ② 按时间整理重要事件 ③ 生成简要介绍文档3. AutoGPT: - 开始第1步：调用搜索API - 得到网页结果4. AutoGPT: - 第2步：分析网页 - 挑出马斯克生平重要事件5. AutoGPT: - 第3步：组织成文档 - 保存成文本文件6. AutoGPT: - 任务完成，提示用户 这一整套都是 Agent 自己思考-执行的！ 一些典型Agent框架 项目 特点 地址 AutoGPT 早期爆火，超全面，但偏重实验 https://github.com/Torantulino/Auto-GPT BabyAGI 极简Agent，只要几百行代码，便于学习 https://github.com/yoheinakajima/babyagi CrewAI 多Agent协作系统（模拟一个小团队） https://github.com/joaomdmoura/crewAI LangChain Agent LangChain框架内置的Agent模块，商业项目多用 https://docs.langchain.com/docs/modules/","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"},{"name":"agent","slug":"agent","permalink":"https://yaheii.github.io/tags/agent/"}],"author":null},{"title":"先验概率与后验概率","slug":"先验概率与后验概率","date":"2025-03-27T16:00:00.000Z","updated":"2025-07-03T11:42:48.892Z","comments":true,"path":"2025/03/28/先验概率与后验概率/","permalink":"https://yaheii.github.io/2025/03/28/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87/","excerpt":"","text":"贝叶斯公式P(A|B) &#x3D; P(AB)&#x2F;P(B) &#x3D; {P(AB)&#x2F;P(A)}*P(A)&#x2F;P(B) 先验概率与后验概率的概念理解在区分先验概率和后验概率的时候，首先的任务是确定结果与因素。确定之后就像概率论所学，由因推果为先验概率，由果推因为后验概率。在上面的公式中，假设B为结果A为因素，那么我们在上式就完成了先验概率到后验概率的转换。 与粒子滤波的联系在粒子滤波中我们将位姿的估计转换为了求解一个联合后验概率，为了求解这个联合后验概率，通过使用CK方程将它拆解成了各个时刻的状态乘积，并求解了在已知控制输入与传感器观测值的状态先验概率，即为预测。接下来利用下一个时刻得到的传感器输入融合这个时刻的位姿和环境特征来修正先验概率，即为更新。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"SLAM","slug":"technology/SLAM","permalink":"https://yaheii.github.io/categories/technology/SLAM/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://yaheii.github.io/tags/SLAM/"},{"name":"概率","slug":"概率","permalink":"https://yaheii.github.io/tags/%E6%A6%82%E7%8E%87/"}],"author":null},{"title":"视觉问题中的手眼标定算法","slug":"标定","date":"2025-03-19T16:00:00.000Z","updated":"2025-07-02T03:04:48.916Z","comments":true,"path":"2025/03/20/标定/","permalink":"https://yaheii.github.io/2025/03/20/%E6%A0%87%E5%AE%9A/","excerpt":"","text":"手眼标定分为两种情况，眼在手外与眼在手上手眼标定的目标是获得相机坐标系到机器人的基坐标系变换矩阵 对于基矩阵到摄像头矩阵，首先选取标定板的三个基向量拼接成为R矩阵。然后以标定板原点的坐标为t。最后为了防止标定板的xy向量不正交，最好在xz方向重新叉乘得到新的y向量，以此提升精度在选取标定板基向量时选取较远的起点与中点较为准确 将标定板放置在机械臂的末端法兰上，而这个矩阵是较为难以估计的，想办法消除左乘基坐标到摄像头矩阵的逆，左乘法兰到基坐标的矩阵的逆多次照相并联立方程","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"SLAM","slug":"technology/SLAM","permalink":"https://yaheii.github.io/categories/technology/SLAM/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://yaheii.github.io/tags/SLAM/"},{"name":"标定","slug":"标定","permalink":"https://yaheii.github.io/tags/%E6%A0%87%E5%AE%9A/"}],"author":null},{"title":"台球训练心得","slug":"台球训练","date":"2024-07-05T16:00:00.000Z","updated":"2025-07-18T03:59:49.135Z","comments":true,"path":"2024/07/06/台球训练/","permalink":"https://yaheii.github.io/2024/07/06/%E5%8F%B0%E7%90%83%E8%AE%AD%E7%BB%83/","excerpt":"","text":"入位我之前常用的入位方式是找到那根瞄准线，然后用杆延申向后拉，同时把身体趴下去。同时要注意双脚的平行。 现在采用的方式是先用右脚踩在那根瞄准线上，然后俯身，注意无所谓两只脚是否平行。俯身时夹紧大臂，小拇指完全放松，虎口靠住台球杆。 运杆 首先选中白球击球点，确定走位方式。我现在击打球时居然会偏右{% emoji blobcat 0_0 。这实在是令人难以接受，打了这么多年台球居然现在打点不纯，不过今天晚上的训练自认为已经纠正过来了。 在瞄准白球击球点的时候应该向左偏移一些。 %} 击打这个地方没什么太大改变，只需要注意运杆和击打的时候幅度不要变化太大，避免身体的剧烈起伏。同时，要注意在末尾的时候要提高杆速，这样才能够打出效果，另外，我现在的杆打杆非常硬的，杆的支撑性很强，皮头也偏硬，击打反也很明显，这都是要求我在击打中缓慢去感知的。 击打后不要急着起身。其实根本在于动作的连贯性，如果动作很连贯，那么应该是有惯性存在的，想快速起身也做不到。 总的来说现在的标准流程应该是 踩中那根瞄准线 注意后手的稳定性 调整白球击打点 运杆，击打","categories":[{"name":"life","slug":"life","permalink":"https://yaheii.github.io/categories/life/"},{"name":"运动","slug":"life/运动","permalink":"https://yaheii.github.io/categories/life/%E8%BF%90%E5%8A%A8/"}],"tags":[{"name":"台球","slug":"台球","permalink":"https://yaheii.github.io/tags/%E5%8F%B0%E7%90%83/"}],"author":null},{"title":"C++阅读笔记","slug":"c++","date":"2024-04-04T16:00:00.000Z","updated":"2025-07-02T03:04:37.303Z","comments":true,"path":"2024/04/05/c++/","permalink":"https://yaheii.github.io/2024/04/05/c++/","excerpt":"","text":"左值引用 左值引用是一种绑定，在使用时必须进行初始化 引用并非一种对象，而只是为一个已经存在的变量所起的另一种名字（我们把具有存储空间的叫做对象）。因此引用无法再绑定到其他对象。 引用类型的初始值必须是一个对象。不应该是一个值。除了常量引用。 指针 指针本身也是一个对象。与引用类似，也实现了对其他对象的间接访问。 指针一定要初始化 指针类型需要和它所指向的对象严格匹配。但有两种例外：指向常量的指针允许指向一个非常量对象。 指针的值应该为以下四种状态之一 指向一个对象 指向紧邻对象的下一个位置 空指针，即没有指向任何对象 无效指针，即除上述情况的其他值。访问无效指针将发生错误。 某些符号具有多重含义 12345678&#123;int i = 42; int &amp;r = i; //紧随类型名出现，因此是声明的一部分r是一个引用int *p; //紧随类型名出现，因此是声明的一部分，p是一个指针p = &amp;i; //&amp;出现在表达式中，是一个取址*p = i; //*出现在表达式中，是一个解引用int &amp;r2 = *p; //&amp;是声明的一部分，*是解引用&#125; 生成空指针的方法 12345&#123; int p1 = nullptr;//int*p1 = 0;可以转换成任意其它类型的指针 int *p2 = 0; int *p3 = NULL;&#125; void* 指针是一种特殊的指针类型，可以存放任意对象的地址。 如何检查指针是否指向了一个合法的对象呢？ 检查是否为NULL123if (p != NULL) &#123; // p 可能是一个有效指针（但不保证）&#125; 确保指针指向的是已分配的内存12345p = malloc(sizeof(int));p = new int;if (p) &#123; // new 在分配失败时通常会抛出异常，除非使用 `nothrow`&#125; 检查指针是否为悬空指针12345678910int *p = (int*)malloc(sizeof(int));free(p); // p 现在是悬空指针if (p) &#123; printf(&quot;p 不是 NULL，但仍然是无效的！\\n&quot;);&#125;//处理方法free(p);p = NULL;//智能指针std::unique_ptr&lt;int&gt; p = std::make_unique&lt;int&gt;(10); // 自动管理内存 123456789&#123; const int i = 42; auto j = i; const auto &amp;k = i; auto *p = &amp;i; const auto j2 = i,&amp;k2 = i; cout&lt;&lt;j&lt;&lt;&quot; &quot;&lt;&lt;k&lt;&lt;&quot; &quot;&lt;&lt;*p&lt;&lt;&quot; &quot;&lt;&lt;j2&lt;&lt;&quot; &quot;&lt;&lt;k2; //42 42 42 42 42&#125; auto与decltype指示符 auto是自动配置声明变量的类型 auto一般会忽略掉顶层const，而底层const会被保留 string 注意在字符串相加时，必须确保“+”两侧至少有一个string类型。，不能直接使用字面值相加。 string中包含了相当多的库函数，使用时可以问问gpt 迭代器 容器的访问有两种方式12345678910//下标访问，这里有一行比较巧妙地代码vector&lt;unsigned&gt; scores(10,0);unsigned grade;while (cin&gt;&gt;grade)&#123; if(grade&lt;=100) &#123; ++scores[grade/10]; &#125;&#125; 但是需要注意的是，我们无法通过下标来实现添加元素。，应该实验push_back; 另外值得注意的是，通过下标访问容器中不存在地元素会导致严重的错误，即缓冲区溢出。 另外一种访问方式是迭代器方法12345&#123; string s(&quot;my name&quot;); auto it = s.begin(); *it = toupper(*it);&#125; 在这种方式下要注意迭代器与指针的相似性与差异性 数组 数组是一种类似vector的数据结构，但是数组的大小不变，不能够随时向数组内添加元素。 在数组初始化的时候要注意字符数组的特殊，字符串字面值的结尾处还有一个空白字符。 对于复杂数组的理解（类型修饰符从右向左依次绑定）1234int *ptrs[10];//10个整型变量数组指针int (*Parray)[10] = &amp;arr;//指向十个整型变量的数组的指针int (&amp;arrRef)[10] = arr;//引用十个整型变量数组int *(&amp;arry)[10] = ptrs;//arry是数组的引用，该数组含有10个指针 在使用数组时编译器一般会自动将其替换成为一个指向数组收地址的指针。 运算符 重载运算符不能改变运算对象的个数，运算符的优先级，结合律 当一个对象被用作右值时，用的时对象的值（内容）。当一的对象被用作左值时，用的是对象的身份（在内存中的位置）。 对于逻辑与和逻辑或而言，都是先求左侧对象的值再求右侧对象的值，当且仅当左侧对象无法确定表达式的结果的时候才会继续计算右侧对象1234int i = 0, j = 0;j = i++; //j = 0,i = 1j = ++i; //j = 2;i = 2cout&lt;&lt;*p++&lt;&lt;endl;//输出当前值并指针后移一个单位 static_cast(name); &#x2F;&#x2F;类型强转 const_cast(name); &#x2F;&#x2F;常用于去掉变量的const属性 但是强制类型转化干扰了正常的类型检查，所以应该减少使用。 C++11版本的for循环语句12345for(declaration:expression)//expression所表达的必须是一个序列。例如一个花括号括起来的初始值列表、数组、容器//declaretion需要是一个能转换成该变量的类型。最好使用auto来声明。 statement; 泛型算法 标准库定义了一组泛型算法实现了一些经典算法的公共接口，如排序和搜索，他们可以用于不同类型的元素和多种容器类型。 注意泛型算法并不会改变容器本身的大小，并不能执行容器操作。 常见函数 find（） accumulate() sort() unique()函数可以将重复元素放在末端，并返回一个指向最后一个不重复元素之后的位置。再通过erase函数实现元素的删除。 lamda表达式又称为匿名函数，一般的表达形式为 [capture list](parameter list) -&gt;return type {function body} 其中capture list是一个局部变量列表 我们可以忽略列表和返回类型，但是必须永远包含捕获列表和函数体。 迭代器的类型也有很多，包括输出迭代器，输入迭代器，前向迭代器，双向迭代器，随机访问迭代器。 关联容器 关联容器并不支持顺序容器位置相关的操作，也不接受构造函数或者插入操作。 map容器可以用于键值-值的算法。set容器可以用于查找算法。 对于multimap，同样一个键值可以对应不同的值，对于multiset，容器内可以存放重复的值 pair标准库类型，可以用于生成一个键值对，并可以为这个键值对命名 动态内存（堆） 全局对象再程序启动时分配，程序结束时销毁；局部对象第一次使用前分配，函数结束时销毁，static程序启动时在静态存储区分配内存，程序结束时释放内存，析构函数会在程序退出时调用（如果有）。const如果是基础类型（如int、float），且编译器能够确定值，则通常优化为编译期常量，直接替换为字面值，不分配实际内存。如果编译器无法确定值（如引用外部变量），会分配在栈上。如果分配在栈上，则函数调用结束时销毁如果被优化为字面值，不存在分配和销毁的过程。 而对于动态变量我们可以指定他的生存周期，也即我们需要显式的销毁这个对象。 新标准库提供了两种新的智能指针，并且都在memory头文件中。 shared_ptr 允许多个指针指向同一个对象 unique_ptr 单独指针指向对象 weak_ptr 弱引用，指向shared_ptr的对象 最安全的分配使用动态内存的方式时调用make_shared函数1shared_ptr&lt;int&gt; p3= make_shared&lt;int&gt; 42; 当指向这个对象的最后一个shared_ptr被销毁的时候，sahred_ptr类会自动销毁这个对象 传递给delete的指针必须要指向动态分布的内厝或者一个空指针，要注意，编译器无法分辨指针指向的是静态还是动态分配的一个对象。 const指针指向的对象同样可以被释放 当我们delete一个指针后指针变为无效了，但是可能指针仍保存着已经被释放了的动态内存的地址，这儿时候我们应该将其赋值为NULL 智能指针需要遵守的规范 不使用相同的内置指针初始化多个智能指针 不delete get（）返回的指针 不使用get（）初始化或者reset（）另一个智能指针 如果使用了get返回的指针，那么最后一个智能指针销毁的时候，对应的指针会无效 使用的指针管理的资源不是new分配的内存，那么应该传递给他一个delete weak_ptr是一种不控制所指向对象生存期的智能指针。他指向一个shared_ptr管理的对象，并且将一个weak_ptr绑定到shared_ptr不会改变shared_ptr的引用次数 要注意动态数组在新版本下的性能往往不如一个容器。并且我们所创建的动态数组往往是数组元素类型的一个指针 面向对象程序设计 面向对象程序设计基于三个基本概念：数据抽象，继承，动态绑定。数据抽象即设计一个类来实现同类数据的存储，可以帮我我们将类的接口与实现相分离；继承可以帮助我们定义一个相似但是并不完全相同的新类；动态绑定是与继承相适应的一个函数形态，是一种多态的函数，通过传入不同的形参来实现绑定不同的派生类。","categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"编程基础","slug":"编程基础","permalink":"https://yaheii.github.io/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"}],"author":null},{"title":"一篇菜谱速记","slug":"菜谱速记","date":"2024-02-17T16:00:00.000Z","updated":"2025-07-02T03:04:47.161Z","comments":true,"path":"2024/02/18/菜谱速记/","permalink":"https://yaheii.github.io/2024/02/18/%E8%8F%9C%E8%B0%B1%E9%80%9F%E8%AE%B0/","excerpt":"","text":"虎皮鸡爪所需材料：鸡爪，香料1.首先处理鸡爪的指甲，然后把鸡爪劈半2.油温烧至五成热(150度左右，油略微翻涌)；倒入鸡爪（记住要盖锅盖）。刚下的时候不要翻动成型后分开炸大概5-6分钟，有小泡即可出锅。3.凉水浸泡待鸡爪表面出现大泡，调酱汁：耗油一勺；生抽两勺；老抽一勺；五香粉少许；糖醋黄酒适量4.起锅烧油，倒入葱姜桂皮八角香叶（葱姜后下）。炒出香味后导入鸡爪，酱汁翻炒上色5.倒热水炖至软烂 小炒黄牛肉所需材料：牛肉，香料1.切牛肉（横切牛羊竖切猪）。加入：烧烤料，生抽两勺，老抽一勺，一勺小苏打，适量食用油，适量耗油花椒油2.整个过程中不要加盐，先拌匀香料再油封。腌制一天。3.拍蒜末（多一些），下锅翻炒，不要超过两分钟！出锅。 红烧鸡块所需材料：鸡肉，土豆，粉丝（配菜在加盐时放入）1.备料：干辣椒，蒜，姜。鸡块焯水（冷水），倒黄酒，打浮沫。2.下入一勺豆瓣酱炒出红油，下姜葱，下蒜干辣椒3.炒至香味混合均匀，下鸡块炒至变色。再沿锅边烹生抽老抽，炒至变色。加入八角大料，适量冰糖，热水。4.炖半小时后加适量盐（汤微咸为宜）。再炖半小时出锅。5.出锅后汤汁勾芡，烹少量香油 肉末香菇所需材料：香菇，肉末1.酱汁：两勺生抽，一勺老抽，一勺耗油，适量糖，勾碗芡。香菇焯水，加黄酒，打浮沫。2.加小米辣，葱蒜翻炒，后加肉末，后加香菇。翻炒出汁后加芡汁再翻炒。出锅烹香油 地三鲜所需材料：茄子，土豆，青椒1.茄子土豆切滚刀块，青椒手掰。茄子表面拍粉（可以用塑料袋把茄子和粉装起来然后摇晃塑料袋）2.过油：茄子（三分钟，最好复炸）土豆（五分钟，至表面酥脆）青椒（过油即可）3.酱汁：生抽三勺，耗油两勺，老抽一勺，糖一勺，老抽半勺，勾碗芡。4.加蒜爆香，翻炒出锅。 地三鲜所需材料：茄子，土豆，青椒1.茄子土豆切滚刀块，青椒手掰。茄子表面拍粉（可以用塑料袋把茄子和粉装起来然后摇晃塑料袋）2.过油：茄子（三分钟，最好复炸）土豆（五分钟，至表面酥脆）青椒（过油即可）3.酱汁：生抽三勺，耗油两勺，老抽一勺，糖一勺，老抽半勺，勾碗芡。4.加蒜爆香，翻炒出锅。 五香毛豆所需材料：毛豆，香料1.先用清水和盐搓洗两遍毛豆，减去两头2.冷水下毛豆，加小苏打，盐（不要盖盖烧开5分钟）3.煮好后放入冰水，4.卤水配比：桂皮5克，香叶五片，八角三个，花椒五克，草果三个，干辣椒适量，小茴香五克，葱姜适量。加入盐30克，白糖十克，黄酒一勺，鸡精适量。5.烧开小火十分钟，加毛豆，密封冷藏。 炒蚬子所需材料：花岘1.先用清水，香油，盐，让蚬子吐沙（3小时以上）。凉水下锅焯水，加黄酒，打浮沫。2.豆瓣酱，干辣椒，蒜末翻炒（可放鸡精）3.出锅烹香油 葱爆羊肉所需材料：羊肉，葱1.备料：姜米，蒜末，蒜片，葱（滚刀块，多一些）2.起锅烧油，先放姜爆锅。下羊肉，翻炒至变色，烹陈醋，下黄豆酱油。3.待羊肉出汤后，放蒜末。收汁。放葱，再放蒜片，再烹醋。4.翻炒均匀，出国烹香油 红烧鲤鱼所需材料：鲤鱼，五花肉，香菇等配菜1.处理鱼：去背鳍，喉牙，血线；打花刀（可截两段）。2.起锅烧油，先放葱姜蒜大料桂皮，五花肉。料炸香3.鱼肉拍粉下锅煎（油温七成，即冒青烟）。全程大火4.另起锅加油，煸香菇等配料。5.将2&#x2F;4的料放入，加热水，倒入黄酒，酱油，炖至软烂出锅。出锅加盐味定味。6.出锅勾芡少量多次。 咸蛋黄茄子所需材料：咸蛋黄，茄子1.取咸蛋黄磨碎（现用现取！）。茄子去皮切条，拍粉2.油温六成，茄子过油3.放入蒜，干辣椒，翻炒。再加咸蛋黄翻炒（用小火，会立刻起泡），加茄子，加味精4.翻炒出锅，加葱花","categories":[{"name":"life","slug":"life","permalink":"https://yaheii.github.io/categories/life/"},{"name":"菜谱","slug":"life/菜谱","permalink":"https://yaheii.github.io/categories/life/%E8%8F%9C%E8%B0%B1/"}],"tags":[{"name":"菜谱","slug":"菜谱","permalink":"https://yaheii.github.io/tags/%E8%8F%9C%E8%B0%B1/"}],"author":null}],"categories":[{"name":"technology","slug":"technology","permalink":"https://yaheii.github.io/categories/technology/"},{"name":"编程基础","slug":"technology/编程基础","permalink":"https://yaheii.github.io/categories/technology/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"},{"name":"音视频","slug":"technology/音视频","permalink":"https://yaheii.github.io/categories/technology/%E9%9F%B3%E8%A7%86%E9%A2%91/"},{"name":"SLAM","slug":"technology/SLAM","permalink":"https://yaheii.github.io/categories/technology/SLAM/"},{"name":"AI","slug":"technology/AI","permalink":"https://yaheii.github.io/categories/technology/AI/"},{"name":"life","slug":"life","permalink":"https://yaheii.github.io/categories/life/"},{"name":"运动","slug":"life/运动","permalink":"https://yaheii.github.io/categories/life/%E8%BF%90%E5%8A%A8/"},{"name":"菜谱","slug":"life/菜谱","permalink":"https://yaheii.github.io/categories/life/%E8%8F%9C%E8%B0%B1/"}],"tags":[{"name":"回调函数","slug":"回调函数","permalink":"https://yaheii.github.io/tags/%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0/"},{"name":"RTSP服务器","slug":"RTSP服务器","permalink":"https://yaheii.github.io/tags/RTSP%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"高性能","slug":"高性能","permalink":"https://yaheii.github.io/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"},{"name":"网络编程，Socket","slug":"网络编程，Socket","permalink":"https://yaheii.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%EF%BC%8CSocket/"},{"name":"SLAM","slug":"SLAM","permalink":"https://yaheii.github.io/tags/SLAM/"},{"name":"概率","slug":"概率","permalink":"https://yaheii.github.io/tags/%E6%A6%82%E7%8E%87/"},{"name":"RTSP","slug":"RTSP","permalink":"https://yaheii.github.io/tags/RTSP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://yaheii.github.io/tags/Nginx/"},{"name":"HLS","slug":"HLS","permalink":"https://yaheii.github.io/tags/HLS/"},{"name":"RTMP","slug":"RTMP","permalink":"https://yaheii.github.io/tags/RTMP/"},{"name":"ffmpeg结构体分析","slug":"ffmpeg结构体分析","permalink":"https://yaheii.github.io/tags/ffmpeg%E7%BB%93%E6%9E%84%E4%BD%93%E5%88%86%E6%9E%90/"},{"name":"播放器工作流","slug":"播放器工作流","permalink":"https://yaheii.github.io/tags/%E6%92%AD%E6%94%BE%E5%99%A8%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"name":"C\\C++","slug":"C-C","permalink":"https://yaheii.github.io/tags/C-C/"},{"name":"编译流程","slug":"编译流程","permalink":"https://yaheii.github.io/tags/%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B/"},{"name":"库依赖","slug":"库依赖","permalink":"https://yaheii.github.io/tags/%E5%BA%93%E4%BE%9D%E8%B5%96/"},{"name":"C、C++混合编译","slug":"C、C-混合编译","permalink":"https://yaheii.github.io/tags/C%E3%80%81C-%E6%B7%B7%E5%90%88%E7%BC%96%E8%AF%91/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://yaheii.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"分层模型","slug":"分层模型","permalink":"https://yaheii.github.io/tags/%E5%88%86%E5%B1%82%E6%A8%A1%E5%9E%8B/"},{"name":"通信架构","slug":"通信架构","permalink":"https://yaheii.github.io/tags/%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/"},{"name":"通信模式","slug":"通信模式","permalink":"https://yaheii.github.io/tags/%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%BC%8F/"},{"name":"C++","slug":"C","permalink":"https://yaheii.github.io/tags/C/"},{"name":"WebRtc","slug":"WebRtc","permalink":"https://yaheii.github.io/tags/WebRtc/"},{"name":"RTCP","slug":"RTCP","permalink":"https://yaheii.github.io/tags/RTCP/"},{"name":"RTP","slug":"RTP","permalink":"https://yaheii.github.io/tags/RTP/"},{"name":"拥塞控制","slug":"拥塞控制","permalink":"https://yaheii.github.io/tags/%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/"},{"name":"音视频服务质量","slug":"音视频服务质量","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/"},{"name":"FFmpeg","slug":"FFmpeg","permalink":"https://yaheii.github.io/tags/FFmpeg/"},{"name":"编程基础","slug":"编程基础","permalink":"https://yaheii.github.io/tags/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"},{"name":"CMake","slug":"CMake","permalink":"https://yaheii.github.io/tags/CMake/"},{"name":"音视频传输","slug":"音视频传输","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E4%BC%A0%E8%BE%93/"},{"name":"音视频编解码","slug":"音视频编解码","permalink":"https://yaheii.github.io/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81/"},{"name":"MCP","slug":"MCP","permalink":"https://yaheii.github.io/tags/MCP/"},{"name":"AI","slug":"AI","permalink":"https://yaheii.github.io/tags/AI/"},{"name":"STL","slug":"STL","permalink":"https://yaheii.github.io/tags/STL/"},{"name":"MiniAGI","slug":"MiniAGI","permalink":"https://yaheii.github.io/tags/MiniAGI/"},{"name":"Prompt","slug":"Prompt","permalink":"https://yaheii.github.io/tags/Prompt/"},{"name":"agent","slug":"agent","permalink":"https://yaheii.github.io/tags/agent/"},{"name":"标定","slug":"标定","permalink":"https://yaheii.github.io/tags/%E6%A0%87%E5%AE%9A/"},{"name":"台球","slug":"台球","permalink":"https://yaheii.github.io/tags/%E5%8F%B0%E7%90%83/"},{"name":"菜谱","slug":"菜谱","permalink":"https://yaheii.github.io/tags/%E8%8F%9C%E8%B0%B1/"}]}